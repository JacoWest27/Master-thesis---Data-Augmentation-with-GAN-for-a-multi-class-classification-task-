{"cells":[{"cell_type":"markdown","metadata":{"id":"l1Fs0GL_mVgn"},"source":["# ***Progetto CV&DL - Alex Giacomini & Denis Bernovschi***"]},{"cell_type":"markdown","metadata":{"id":"JvzVxkrcd4Mj"},"source":["To Do List: \n","- Provare altre Gan \n","- Inserire altre metriche (https://machinelearningmastery.com/how-to-evaluate-generative-adversarial-networks/)  - ti ho inviato anche il paper su teams\n","\n","Done: \n","- Metriche Inserite (maximum_mean_discrepancy, FID, Inception Score) \n","- Test a colori \n","\n","Prima di riprovare il codice ricontrolla tutti gli iper-parametri della gan (ho modificato il salvataggio delle foto e il numero di epoche) \n","\n","\n","LEGGERE IMMAGINI DA CARTELLA E RICAVARE IL NOME\n","https://stackoverflow.com/questions/68112479/get-file-names-and-file-path-using-pytorch-dataloader"]},{"cell_type":"markdown","metadata":{"id":"ox6IFmrBfpP-"},"source":["## IMPORT"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":6877,"status":"ok","timestamp":1656671119834,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"mwFSXnnkfot7"},"outputs":[],"source":["import sys\n","import subprocess\n","if 'google.colab' in sys.modules:\n","  subprocess.call(\"pip install -U progress\".split())\n","\n","#---- RANDOM ------\n","import random\n","from random import randrange\n","random.seed( 40 )\n","\n","#---- NUMPY -------\n","import pandas as pd\n","import numpy as np\n","import math\n","from pandas.compat._optional import import_optional_dependency\n","\n","#----- OS ---------\n","import os\n","from os import path\n","\n","#---- DRIVE --------\n","from google.colab import drive\n","\n","#----- SCYPY -------\n","import scipy.ndimage\n","\n","#----- MATPLOT LIB -----\n","import matplotlib.pyplot as plt\n","\n","#------- TORCH VISION -------\n","from torchsummary import summary\n","from torchvision import transforms, datasets, utils \n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import models\n","from torchvision.utils import save_image\n","from torch import optim \n","from torch.optim import Adam, SGD, RMSprop \n","import torchvision, torch \n","import torch.nn as nn\n","import torchvision as tv\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","import torch.autograd as autograd\n","from torchvision.transforms.transforms import CenterCrop\n","\n","\n","#----- PIL --------\n","from PIL import Image, ImageOps\n","\n","#----- SKLEARN ----- \n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score,classification_report, f1_score, precision_score, recall_score, confusion_matrix\n","from sklearn.metrics import confusion_matrix\n","import sklearn.metrics as metrics\n","from sklearn.utils.class_weight import compute_sample_weight\n","from sklearn.utils import class_weight\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from sklearn.model_selection import train_test_split\n","import keras.backend as K #------ valutare/testare se serve ancora\n","from sklearn.metrics import balanced_accuracy_score\n","from sklearn.metrics import confusion_matrix\n","\n","\n","#----- IMPORT  PYTORCH ----------\n","!pip install pytorch-lightning -q \n","from pytorch_lightning.callbacks import Callback, ModelCheckpoint, EarlyStopping\n","from pytorch_lightning import Trainer\n","\n","\n","#----- IMPORT CV2 -----------\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","#----- ALTRO ------------\n","#LINK UTILE : https://github.com/chongwar/vgg16-pytorch/blob/master/vgg16_transfer_learning.py\n","#LINK UTILE : https://www.analyticsvidhya.com/blog/2019/10/how-to-master-transfer-learning-using-pytorch/ \n","from functools import reduce\n","\n","#----- SKIIMAGE -----------\n","from skimage import io\n","from skimage.io import imread\n","from skimage.color import rgb2gray, gray2rgb\n","\n","#WGAN-GP APPROACH - imports\n","#https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py\n","import argparse\n","\n","#---- PROGRESS BAR -------\n","from tqdm import tqdm\n","\n","\n","# per attuare il D.A. secondo il link di Riccardo\n","#!pip install https://github.com/ufoym/imbalanced-dataset-sampler/archive/master.zip --quiet --ignore-installed #--- qui va in errore, serviva per creare il batch rappresentativo\n","#from torchsampler import ImbalancedDatasetSampler\n","\n","\n","\n","device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"50n4oVKemd9n"},"source":["##WGAN-GP - APPOGGIO CODICE E PROVA"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1656671120129,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"BFWWEnqlmi--","outputId":"eb6b5665-995d-428b-d2d3-887b9aa0d273"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n#STO LAVORANDO QUA SE CI GUARDI - QUINDI NON TOCCARE NIENTE - CIT.ALEX\\n\\nos.makedirs(\"images\", exist_ok=True)\\n\\n\\'\\'\\'\\nparser = argparse.ArgumentParser()\\nparser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\\nparser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\\nparser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\\nparser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\\nparser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\\nparser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\\nparser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\\nparser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\\nparser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\\nparser.add_argument(\"--n_critic\", type=int, default=5, help=\"number of training steps for discriminator per iter\")\\nparser.add_argument(\"--clip_value\", type=float, default=0.01, help=\"lower and upper clip value for disc. weights\")\\nparser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\\n\\nopt = parser.parse_args()\\n\\'\\'\\'\\n\\nopt = [[\"n_epochs\", \"batch_size\", \"lr\", \"b1\", \"b2\", \"n_cpu\", \"latent_dim\", \"img_size\", \"channels\", \"n_critic\", \"clip_value\", \"sample_interval\"],\\n       [10,            64,      0.0002, 0.5,0.999,    8,        100,          28,          1,          5,         0.01,           400]]\\n#print(opt[0])\\n#print(opt[1])\\n\\n\\n\\nimg_shape = (opt[1][8], opt[1][7],  opt[1][7])\\n\\ncuda = True if torch.cuda.is_available() else False\\n\\n\\nclass Generator(nn.Module):\\n    def __init__(self):\\n        super(Generator, self).__init__()\\n\\n        def block(in_feat, out_feat, normalize=True):\\n            layers = [nn.Linear(in_feat, out_feat)]\\n            if normalize:\\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\\n            return layers\\n\\n        self.model = nn.Sequential(\\n            *block( opt[1][6], 128, normalize=False),\\n            *block(128, 256),\\n            *block(256, 512),\\n            *block(512, 1024),\\n            nn.Linear(1024, int(np.prod(img_shape))),\\n            nn.Tanh()\\n        )\\n\\n    def forward(self, z):\\n        img = self.model(z)\\n        img = img.view(img.shape[0], *img_shape)\\n        return img\\n\\n\\nclass Discriminator(nn.Module):\\n    def __init__(self):\\n        super(Discriminator, self).__init__()\\n\\n        self.model = nn.Sequential(\\n            nn.Linear(int(np.prod(img_shape)), 512),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Linear(512, 256),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Linear(256, 1),\\n        )\\n\\n    def forward(self, img):\\n        img_flat = img.view(img.shape[0], -1)\\n        validity = self.model(img_flat)\\n        return validity\\n\\n\\n# Loss weight for gradient penalty\\nlambda_gp = 10\\n\\n# Initialize generator and discriminator\\ngenerator = Generator()\\ndiscriminator = Discriminator()\\n\\nif cuda:\\n    generator.cuda()\\n    discriminator.cuda()\\n\\n#generator.to(device)\\n#discriminator.to(device)\\n\\n#VALUTA SE CONFIGURARE ANCHE IL SALVATAGGIO DELLE IMMAGINI DIRETTAMENTE SU DRIVE PER QUELLE CHE CREA IL \\'GENERATOR\\'\\n#PROBABILMENTE SERVIRÀ PERCHE A NOI SERVONO LE IMMAGINI COME DATA AUGMENTATION, QUINDI O LE SALVIAMO PRIMA E POI CI FACCIAMO IL LOAD, OPPURE PENSA COME COMBINARE INSIEME QUESTA FASE CON QUELLA DI ALLENAMENTO DELLA RETE VERA E PROPRIA\\n\\n# Configure data loader\\nos.makedirs(\"../../data/mnist\", exist_ok=True)\\ndataloader = torch.utils.data.DataLoader(\\n    datasets.MNIST(\\n        \"../../data/mnist\",\\n        train=True,\\n        download=True,\\n        transform=transforms.Compose(\\n            [transforms.Resize( opt[1][7]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\\n        ),\\n    ),\\n    batch_size= opt[1][1],\\n    shuffle=True,\\n)\\n\\n\\n\\n\\n\\n\\n# Optimizers\\noptimizer_G = torch.optim.Adam(generator.parameters(), lr= opt[1][2], betas=( opt[1][3],  opt[1][4]))\\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr= opt[1][2], betas=( opt[1][3],  opt[1][4]))\\n\\nTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\\n\\n\\ndef compute_gradient_penalty(D, real_samples, fake_samples):\\n    Calculates the gradient penalty loss for WGAN GP\\n    # Random weight term for interpolation between real and fake samples\\n    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\\n    # Get random interpolation between real and fake samples\\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\\n    d_interpolates = D(interpolates)\\n    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\\n    # Get gradient w.r.t. interpolates\\n    gradients = autograd.grad(\\n        outputs=d_interpolates,\\n        inputs=interpolates,\\n        grad_outputs=fake,\\n        create_graph=True,\\n        retain_graph=True,\\n        only_inputs=True,\\n    )[0]\\n    gradients = gradients.view(gradients.size(0), -1)\\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\\n    return gradient_penalty\\n\\n\\n# ----------\\n#  Training\\n# ----------\\n\\nbatches_done = 0\\nfor epoch in range( opt[1][0]):\\n    for i, (imgs, _) in enumerate(dataloader):\\n\\n        # Configure input\\n        real_imgs = Variable(imgs.type(Tensor))\\n\\n        # ---------------------\\n        #  Train Discriminator\\n        # ---------------------\\n\\n        optimizer_D.zero_grad()\\n\\n\\n        # Sample noise as generator input\\n        #-----FORSE QUESTO NON CI SERVE PERCHÈ NELLE NOSTRE IMMAGINI NON C\\'È RUMORE? OPPURE SERVE AL GENERATORE PER DISTORCERE L\\'IMMAGINE VERA?------\\n        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0],  opt[1][6]))))\\n\\n        # Generate a batch of images\\n        fake_imgs = generator(z)\\n\\n        # Real images\\n        real_validity = discriminator(real_imgs)\\n        # Fake images\\n        fake_validity = discriminator(fake_imgs)\\n        # Gradient penalty\\n        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\\n        # Adversarial loss\\n        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\\n\\n        d_loss.backward()\\n        optimizer_D.step()\\n\\n        optimizer_G.zero_grad()\\n\\n        # Train the generator every n_critic steps\\n        if i %  opt[1][9] == 0:\\n\\n            # -----------------\\n            #  Train Generator\\n            # -----------------\\n\\n            # Generate a batch of images\\n            fake_imgs = generator(z)\\n            # Loss measures generator\\'s ability to fool the discriminator\\n            # Train on fake images\\n            fake_validity = discriminator(fake_imgs)\\n            g_loss = -torch.mean(fake_validity)\\n\\n            g_loss.backward()\\n            optimizer_G.step()\\n\\n            print(\\n                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\\n                % (epoch,  opt[1][0], i, len(dataloader), d_loss.item(), g_loss.item())\\n            )\\n\\n            if batches_done %  opt[1][11] == 0:\\n                save_image(fake_imgs.data[:36], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\\n\\n            batches_done +=  opt[1][9]\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":43}],"source":["\"\"\"\n","#STO LAVORANDO QUA SE CI GUARDI - QUINDI NON TOCCARE NIENTE - CIT.ALEX\n","\n","os.makedirs(\"images\", exist_ok=True)\n","\n","'''\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n","parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n","parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n","parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n","parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n","parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n","parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n","parser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\n","parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n","parser.add_argument(\"--n_critic\", type=int, default=5, help=\"number of training steps for discriminator per iter\")\n","parser.add_argument(\"--clip_value\", type=float, default=0.01, help=\"lower and upper clip value for disc. weights\")\n","parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\n","\n","opt = parser.parse_args()\n","'''\n","\n","opt = [[\"n_epochs\", \"batch_size\", \"lr\", \"b1\", \"b2\", \"n_cpu\", \"latent_dim\", \"img_size\", \"channels\", \"n_critic\", \"clip_value\", \"sample_interval\"],\n","       [10,            64,      0.0002, 0.5,0.999,    8,        100,          28,          1,          5,         0.01,           400]]\n","#print(opt[0])\n","#print(opt[1])\n","\n","\n","\n","img_shape = (opt[1][8], opt[1][7],  opt[1][7])\n","\n","cuda = True if torch.cuda.is_available() else False\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        def block(in_feat, out_feat, normalize=True):\n","            layers = [nn.Linear(in_feat, out_feat)]\n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *block( opt[1][6], 128, normalize=False),\n","            *block(128, 256),\n","            *block(256, 512),\n","            *block(512, 1024),\n","            nn.Linear(1024, int(np.prod(img_shape))),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        img = self.model(z)\n","        img = img.view(img.shape[0], *img_shape)\n","        return img\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(int(np.prod(img_shape)), 512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(256, 1),\n","        )\n","\n","    def forward(self, img):\n","        img_flat = img.view(img.shape[0], -1)\n","        validity = self.model(img_flat)\n","        return validity\n","\n","\n","# Loss weight for gradient penalty\n","lambda_gp = 10\n","\n","# Initialize generator and discriminator\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","if cuda:\n","    generator.cuda()\n","    discriminator.cuda()\n","\n","#generator.to(device)\n","#discriminator.to(device)\n","\n","#VALUTA SE CONFIGURARE ANCHE IL SALVATAGGIO DELLE IMMAGINI DIRETTAMENTE SU DRIVE PER QUELLE CHE CREA IL 'GENERATOR'\n","#PROBABILMENTE SERVIRÀ PERCHE A NOI SERVONO LE IMMAGINI COME DATA AUGMENTATION, QUINDI O LE SALVIAMO PRIMA E POI CI FACCIAMO IL LOAD, OPPURE PENSA COME COMBINARE INSIEME QUESTA FASE CON QUELLA DI ALLENAMENTO DELLA RETE VERA E PROPRIA\n","\n","# Configure data loader\n","os.makedirs(\"../../data/mnist\", exist_ok=True)\n","dataloader = torch.utils.data.DataLoader(\n","    datasets.MNIST(\n","        \"../../data/mnist\",\n","        train=True,\n","        download=True,\n","        transform=transforms.Compose(\n","            [transforms.Resize( opt[1][7]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n","        ),\n","    ),\n","    batch_size= opt[1][1],\n","    shuffle=True,\n",")\n","\n","\n","\n","\n","\n","\n","# Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr= opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr= opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","\n","def compute_gradient_penalty(D, real_samples, fake_samples):\n","    \"\"\"\"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\"\"\"\n","    # Random weight term for interpolation between real and fake samples\n","    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n","    # Get random interpolation between real and fake samples\n","    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n","    d_interpolates = D(interpolates)\n","    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n","    # Get gradient w.r.t. interpolates\n","    gradients = autograd.grad(\n","        outputs=d_interpolates,\n","        inputs=interpolates,\n","        grad_outputs=fake,\n","        create_graph=True,\n","        retain_graph=True,\n","        only_inputs=True,\n","    )[0]\n","    gradients = gradients.view(gradients.size(0), -1)\n","    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","    return gradient_penalty\n","\n","\n","# ----------\n","#  Training\n","# ----------\n","\n","batches_done = 0\n","for epoch in range( opt[1][0]):\n","    for i, (imgs, _) in enumerate(dataloader):\n","\n","        # Configure input\n","        real_imgs = Variable(imgs.type(Tensor))\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        optimizer_D.zero_grad()\n","\n","\n","        # Sample noise as generator input\n","        #-----FORSE QUESTO NON CI SERVE PERCHÈ NELLE NOSTRE IMMAGINI NON C'È RUMORE? OPPURE SERVE AL GENERATORE PER DISTORCERE L'IMMAGINE VERA?------\n","        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0],  opt[1][6]))))\n","\n","        # Generate a batch of images\n","        fake_imgs = generator(z)\n","\n","        # Real images\n","        real_validity = discriminator(real_imgs)\n","        # Fake images\n","        fake_validity = discriminator(fake_imgs)\n","        # Gradient penalty\n","        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n","        # Adversarial loss\n","        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n","\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        optimizer_G.zero_grad()\n","\n","        # Train the generator every n_critic steps\n","        if i %  opt[1][9] == 0:\n","\n","            # -----------------\n","            #  Train Generator\n","            # -----------------\n","\n","            # Generate a batch of images\n","            fake_imgs = generator(z)\n","            # Loss measures generator's ability to fool the discriminator\n","            # Train on fake images\n","            fake_validity = discriminator(fake_imgs)\n","            g_loss = -torch.mean(fake_validity)\n","\n","            g_loss.backward()\n","            optimizer_G.step()\n","\n","            print(\n","                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n","                % (epoch,  opt[1][0], i, len(dataloader), d_loss.item(), g_loss.item())\n","            )\n","\n","            if batches_done %  opt[1][11] == 0:\n","                save_image(fake_imgs.data[:36], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n","\n","            batches_done +=  opt[1][9]\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"VgAYZM1NDIRj"},"source":["## Custom TO CATEGORICAL"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1656671120130,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"6ZLOAK3RDLMR"},"outputs":[],"source":["#from tensorflow.keras.utils import to_categorical \n","# LINK UTILE : https://pytorch.org/docs/stable/distributions.html\n","def custom_to_categorical(y, num_classes):\n","    \"\"\" 1-hot encodes a tensor \"\"\"\n","    return np.eye(num_classes, dtype='uint8')[y]"]},{"cell_type":"markdown","metadata":{"id":"zUvw3mwvfCMA"},"source":["## DRIVE "]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4367,"status":"ok","timestamp":1656671124483,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"kkWUwK_IfBtx","outputId":"5bc74d63-8df3-4116-f309-82cf1711ea80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive', force_remount=True)\n","path_drive = '/content/drive/My Drive/'\n","path_progettoDL = path_drive+'ProgettoDL/'"]},{"cell_type":"markdown","metadata":{"id":"YEzVkjhHbISE"},"source":["## Parametri Immagini "]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":63,"status":"ok","timestamp":1656671124485,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"fSyxD4LWbG3J"},"outputs":[],"source":["'''DEFINE VARIABLES AND PARAMETERS TO COLLECT THE INFORMATIONS FROM GOOGLE DRIVE'''\n","'''define a path for the collection of informations (CSV file) for the creation of the dataframe'''\n","os.chdir(path_progettoDL) \n","\n","'''to have always the same sequence of randomized values (numbers)'''\n","random_state = 3  \n","\n","'''some useful parameters and variables'''\n","cnn = \"vgg16\" \n","\n","'''series of production & quality classes of the wood rifle butt'''\n","#classi = ['1','2','3','4']    \n","classi = ['1','2-','2','2+','3-','3','3+','4-','4','4+']          \n","serie = [2,4,8,10,6,9,3,11,12,13,14,15,7] \n","\n","'''size of the images & their paths (location) '''\n","#immg_rows = 270 \n","#immg_cols = 470\n","#immgs = '{}_{}'.format(parte,tipo)\n","#path_imgs = os.path.join(path_drive+'{}'.format(immgs))\n","\n","#CSV loading (reading annotations/attributes/informations)\n","csv = pd.read_csv(('/content/drive/MyDrive/ProgettoDL/20201102_ExportDB.txt'), sep=\";\")\n","\n","#CROP o NO CROP \n","type_img = 'CROP' \n","if type_img == 'CROP':\n","  path_images = '/content/drive/MyDrive/CALCIO_CROP_BASE/'\n","else:\n","  path_images = '/content/drive/MyDrive/CALCIO_NOPRE/'\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xuS19wSXbdE8"},"source":["## SPLIT DATA"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":60,"status":"ok","timestamp":1656671124487,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"t0JSykxbbeCU"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","import custom_split_data \n","from custom_split_data import split_data"]},{"cell_type":"markdown","metadata":{"id":"G4OSrOanfe7I"},"source":["## CUSTOM DATASET + TRANSFORMS D.A. "]},{"cell_type":"markdown","metadata":{"id":"Wnckk5UdrJbn"},"source":["#### TRANSFORMS"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":60,"status":"ok","timestamp":1656671124489,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"YBukxX0brIpY"},"outputs":[],"source":["\n","#ESEMPI DI TRASFROMAZIONI (PER D.A.)\n","#https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n","\n","#LINK DI TRASFORMAZIONI\n","#https://pytorch.org/vision/stable/transforms.html\n","\n","\n","_transform_1 = transforms.Compose([\n","  transforms.ToPILImage(),\n","  #transforms.CenterCrop(0.75 * 64),\n","  transforms.Resize((270, 470)),            \n","  #T.RandomResizedCrop(image_size),\n","  transforms.RandomHorizontalFlip(p=1),\n","  transforms.ToTensor()])#,\n","  #transforms.Normalize([0.5], [0.5])])  #normalizza correttamente in quanto partendo da [0,1] arriva a [-1,1], guarda primo link])\n","\n","_transform_3 = transforms.Compose([\n","  transforms.ToPILImage(),\n","  #transforms.CenterCrop(0.75 * 64),\n","  transforms.Resize((270, 470)),  \n","  transforms.ColorJitter(brightness=[.7,1.3]),  \n","  transforms.RandomPosterize(bits=2, p=1),  #ALTERNATIVA1, bit da mantenere per ogni canale RGB, (0-8 range permesso).  -- ALTERNATIVE DA TESTARE\n","  #transforms.RandomRotation(degrees=(-20, 20), expand=True, PIL.Image.NEAREST ),   #ALTERNATIVA2       \n","  #T.RandomResizedCrop(image_size),\n","  #transforms.RandomHorizontalFlip(),\n","  transforms.ToTensor()])#,\n","  #transforms.Normalize([0.5], [0.5])])  #normalizza correttamente in quanto partendo da [0,1] arriva a [-1,1], guarda primo link])\n","\n","_transform_2 = transforms.Compose([\n","  transforms.ToPILImage(),\n","  #transforms.CenterCrop(0.75 * 64),\n","  transforms.Resize((270, 470)),            \n","  #T.RandomResizedCrop(image_size),\n","  transforms.RandomVerticalFlip(p=1),\n","  transforms.ToTensor()])#,\n","  #transforms.Normalize([0.5], [0.5])])  #normalizza correttamente in quanto partendo da [0,1] arriva a [-1,1], guarda primo link])\n","\n","\n","# trasform generale per fare l'allenamento (senza GAN)\n","_transform_ = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((270, 470)),                                            \n","        transforms.ToTensor()])#,\n","        #transforms.Normalize([0.5], [0.5])])  #normalizza correttamente in quanto partendo da [0,1] arriva a [-1,1], guarda primo link\n","\n","\n","_transform_4 = transforms.Compose([                   #PER ESTRARRE patch di dimensione DIMEZZATA e farla in scala di grigi\n","        transforms.ToPILImage(),\n","        transforms.Resize((270, 470)), \n","        transforms.CenterCrop((135,235)), \n","        transforms.Grayscale(num_output_channels=1),  #VALUTA SE DIMINUIRE IL CROP A MENO PIXELS O ANCHE PER FORME RETTANGOLARI/QUADRATE\n","        transforms.ToTensor()])\n","\n","_transform_GAN = transforms.Compose([                   #normalizzazione per GAN\n","        transforms.ToPILImage(),\n","        transforms.Resize((270, 470)), \n","        #transforms.CenterCrop((256,256)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.5], [0.5])])              #VALORI DI NORMALIZZAZIONE IN QUESTO MODO PORTANO I VALORI FINALI A DIVENTARE IN UN RANGE TRA [-1 E 1]\n","\n","\n","'''TRANSFORM PER TRAIN DOVE OGNUNO SI ATTIVA CON PROBABILITA' DEL 50% COSì DA OTTENERE PIU' COMBINAZIONI POSSIBILI'''\n","_transform_train = transforms.Compose(   #usato per ricavare i nomi delle nuove immagini generate con la GAN\n","    [transforms.ToPILImage(),\n","     transforms.Resize((270, 470)), \n","     transforms.RandomHorizontalFlip(p=0.5),\n","     transforms.RandomVerticalFlip(p=0.5),\n","     transforms.ColorJitter(brightness=[.7,1.3]),  \n","     transforms.RandomPosterize(bits=2, p=0.4),                                           \n","     transforms.ToTensor(),\n","     transforms.Normalize([0.5], [0.5])]\n","     )"]},{"cell_type":"markdown","metadata":{"id":"sr7QNs-GZ12S"},"source":["NORMALIZE ------\n","* [Mezza spiegazione](https://discuss.pytorch.org/t/understanding-transform-normalize/21730)\n","* [LINK REPO](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize)\n","\n","https://deeplizard.com/learn/video/lu7TCu7HeYc\n","\n"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60,"status":"ok","timestamp":1656671124491,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"LKDjjzqtZLNp","outputId":"fabe6620-7c60-45c7-85da-1b78cc934543"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor before Normalize:\n"," tensor([1., 2., 3., 4., 5.])\n","Mean, Std and Var before Normalize:\n"," tensor(3.) tensor(1.5811) tensor(2.5000)\n","Tensor after Normalize:\n"," tensor([-1.2649, -0.6325,  0.0000,  0.6325,  1.2649])\n","Mean, std and Var after normalize:\n"," tensor(0.) tensor(1.) tensor(1.)\n"]}],"source":["# Python program to normalize a tensor to\n","# 0 mean and 1 variance\n","# Step 1: Importing torch\n","import torch\n","  \n","# Step 2: creating a torch tensor\n","t = torch.tensor([1.,2.,3.,4.,5.])\n","print(\"Tensor before Normalize:\\n\", t)\n","  \n","# Step 3: Computing the mean, std and variance\n","mean, std, var = torch.mean(t), torch.std(t), torch.var(t)\n","print(\"Mean, Std and Var before Normalize:\\n\", \n","      mean, std, var)\n","  \n","# Step 4: Normalizing the tensor\n","t  = (t-mean)/std\n","\n","\n","print(\"Tensor after Normalize:\\n\", t)\n","  \n","# Step 5: Again compute the mean, std and variance\n","# after Normalize\n","mean, std, var = torch.mean(t), torch.std(t), torch.var(t)\n","print(\"Mean, std and Var after normalize:\\n\", \n","      mean, std, var)"]},{"cell_type":"markdown","metadata":{"id":"z6DNhY9LrOZF"},"source":["#### CUSTOM DATASET "]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1656671124492,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"ScS3q5MmWMJg"},"outputs":[],"source":["class CustomDataset(Dataset):\n","  def __init__ (self, dataframe, transform_0=None, transform_1 = None, transform_2 = None, transform_3 = None, weight=None, mode = None):\n","    self.transform_0 = transform_0\n","    self.transform_1 = transform_1\n","    self.transform_2 = transform_2\n","    self.transform_3 = transform_3\n","    self.mode = mode\n","    self.weight = weight\n","    self.dataframe = dataframe\n","  def __len__(self):\n","    return len(self.dataframe)\n","  \n","  #ho dovuto aggiungerlo perché l'imbalance dataset sampler lo chiedeva \n","  def get_labels(self):\n","    print(self.dataframe['class'])\n","    return self.dataframe['class']\n","\n","  def __getitem__(self, index):\n","    path = self.dataframe.iloc[index, 2]\n","    img_path = os.path.join(path_images+path)\n","    image = io.imread(img_path)\n","    y_label_class = torch.tensor(int(self.dataframe.iloc[index, 3]))              \n","    y_label_series = torch.tensor(int(self.dataframe.iloc[index, 1]))\n","\n","    #da usare solo se aumentiamo il set offline, ma conviene senza troppe modifiche di creare un transform e mettere quei stessi metodi come probabilità a 0.5 così si possono anche combinare\n","    if self.mode == 'train':                            #questi 3 if servono solo per Augmentation Online se si usano le tecniche base di Computer Visioni di questi transform\n","      if self.dataframe.iloc[index, 4] == 'T0': \n","        #print('transform T1')\n","        image = self.transform_0(image)\n","      if self.dataframe.iloc[index, 4] == 'T1': \n","        #print('transform T1')\n","        image = self.transform_1(image)\n","      if self.dataframe.iloc[index, 4] == 'T2':\n","        #print('transform T2')\n","        image = self.transform_2(image)\n","      if self.dataframe.iloc[index, 4] == 'T3':\n","        #print('transform T3')\n","        image = self.transform_3(image)\n","    else:\n","      image = self.transform_0(image)\n","\n","      \n","    \n","    #plt.imshow(image.numpy()[0], cmap='gray')\n","    \n","    return (image, y_label_class, y_label_series)"]},{"cell_type":"markdown","metadata":{"id":"28i0TbSpzt_Z"},"source":["## NETWORK "]},{"cell_type":"markdown","metadata":{"id":"Ti_N5HuUFggs"},"source":["### Model Custom "]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46,"status":"ok","timestamp":1656671124493,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"8XLdW4ScmkmR","outputId":"ab18f707-033e-4a3f-c2f3-59b96d84ec6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["RETE CUSTOM CREATA\n"]}],"source":["'''\n","import torch.nn.functional as F\n","class VGG16(nn.Module):\n","    def __init__(self):\n","        super(VGG16, self).__init__()\n","        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n","    \n","        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n","    \n","        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n","        self.conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n","    \n","        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n","        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n","        self.conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n","    \n","        self.conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n","        self.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n","        self.conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n","    \n","        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.flatten1 = nn.Flatten()\n","        self.dropOut = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(57344, 4096)\n","        self.fc2 = nn.Linear(4096, 4096)\n","        self.BatchNorm = nn.BatchNorm1d(4096)\n","        self.fc3 = nn.Linear(4096, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1_1(x))\n","        x = F.relu(self.conv1_2(x))\n","        x = self.maxpool(x)\n","        x = F.relu(self.conv2_1(x))\n","        x = F.relu(self.conv2_2(x))\n","        x = self.maxpool(x)\n","        x = F.relu(self.conv3_1(x))\n","        x = F.relu(self.conv3_2(x))\n","        x = F.relu(self.conv3_3(x))\n","        x = self.maxpool(x)\n","        x = F.relu(self.conv4_1(x))\n","        x = F.relu(self.conv4_2(x))\n","        x = F.relu(self.conv4_3(x))\n","        x = self.maxpool(x)\n","        x = F.relu(self.conv5_1(x))\n","        x = F.relu(self.conv5_2(x))\n","        x = F.relu(self.conv5_3(x))\n","        x = self.maxpool(x)\n","        \n","        #x = x.reshape(x.shape[0], -1)\n","        x = self.flatten1(x)\n","        x = F.relu(self.dropOut(x))  \n","        x = F.relu(self.fc1(x))\n","        x = F.dropout2d(x, 0.5) #dropout was included to combat overfitting\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.BatchNorm(x))\n","        x = F.relu(self.fc3(x))\n","        return x\n","\n","modelX = VGG16() #Definizione del modello Custom \n","\n","'''\n","#Il blocco sotto serve a freezare i pesi relativi ai layer convoluzionali \n","#del modello custom - non preallenato\n","'''\n","\n","modelX.conv1_1.weight.requires_grad = False\n","modelX.conv1_2.weight.requires_grad = False\n","\n","modelX.conv2_1.weight.requires_grad = False\n","modelX.conv2_2.weight.requires_grad = False\n","\n","modelX.conv3_1.weight.requires_grad = False\n","modelX.conv3_2.weight.requires_grad = False\n","modelX.conv3_3.weight.requires_grad = False\n","\n","modelX.conv4_1.weight.requires_grad = False\n","modelX.conv4_2.weight.requires_grad = False\n","modelX.conv4_3.weight.requires_grad = False\n","\n","modelX.conv5_1.weight.requires_grad = False\n","modelX.conv5_2.weight.requires_grad = False\n","modelX = modelX.to(device) #modelX = Model Custom Definito da NOI \n","\n","summary(modelX,input_size=(3, 270,470)) #modelX = Model Custom Definito da NOI \n","\n","model = modelX\n","'''\n","print(\"RETE CUSTOM CREATA\")"]},{"cell_type":"markdown","metadata":{"id":"o3iWx_QMIVQp"},"source":["### Model Pre-Trained - per classificazione"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":10777,"status":"ok","timestamp":1656671135239,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"q0Dc7ahEzpWW","outputId":"0938c49a-2f17-49fa-823f-56bad027562d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): ReLU(inplace=True)\n","  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (3): ReLU(inplace=True)\n","  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (6): ReLU(inplace=True)\n","  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (8): ReLU(inplace=True)\n","  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (11): ReLU(inplace=True)\n","  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (13): ReLU(inplace=True)\n","  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (15): ReLU(inplace=True)\n","  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (18): ReLU(inplace=True)\n","  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (20): ReLU(inplace=True)\n","  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (22): ReLU(inplace=True)\n","  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (25): ReLU(inplace=True)\n","  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (27): ReLU(inplace=True)\n","  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (29): ReLU(inplace=True)\n","  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 270, 470]           1,792\n","              ReLU-2         [-1, 64, 270, 470]               0\n","            Conv2d-3         [-1, 64, 270, 470]          36,928\n","              ReLU-4         [-1, 64, 270, 470]               0\n","         MaxPool2d-5         [-1, 64, 135, 235]               0\n","            Conv2d-6        [-1, 128, 135, 235]          73,856\n","              ReLU-7        [-1, 128, 135, 235]               0\n","            Conv2d-8        [-1, 128, 135, 235]         147,584\n","              ReLU-9        [-1, 128, 135, 235]               0\n","        MaxPool2d-10         [-1, 128, 67, 117]               0\n","           Conv2d-11         [-1, 256, 67, 117]         295,168\n","             ReLU-12         [-1, 256, 67, 117]               0\n","           Conv2d-13         [-1, 256, 67, 117]         590,080\n","             ReLU-14         [-1, 256, 67, 117]               0\n","           Conv2d-15         [-1, 256, 67, 117]         590,080\n","             ReLU-16         [-1, 256, 67, 117]               0\n","        MaxPool2d-17          [-1, 256, 33, 58]               0\n","           Conv2d-18          [-1, 512, 33, 58]       1,180,160\n","             ReLU-19          [-1, 512, 33, 58]               0\n","           Conv2d-20          [-1, 512, 33, 58]       2,359,808\n","             ReLU-21          [-1, 512, 33, 58]               0\n","           Conv2d-22          [-1, 512, 33, 58]       2,359,808\n","             ReLU-23          [-1, 512, 33, 58]               0\n","        MaxPool2d-24          [-1, 512, 16, 29]               0\n","           Conv2d-25          [-1, 512, 16, 29]       2,359,808\n","             ReLU-26          [-1, 512, 16, 29]               0\n","           Conv2d-27          [-1, 512, 16, 29]       2,359,808\n","             ReLU-28          [-1, 512, 16, 29]               0\n","           Conv2d-29          [-1, 512, 16, 29]       2,359,808\n","             ReLU-30          [-1, 512, 16, 29]               0\n","        MaxPool2d-31           [-1, 512, 8, 14]               0\n","         Identity-32           [-1, 512, 8, 14]               0\n","          Flatten-33                [-1, 57344]               0\n","          Dropout-34                [-1, 57344]               0\n","           Linear-35                 [-1, 4096]     234,885,120\n","             ReLU-36                 [-1, 4096]               0\n","           Linear-37                 [-1, 4096]      16,781,312\n","             ReLU-38                 [-1, 4096]               0\n","      BatchNorm1d-39                 [-1, 4096]           8,192\n","           Linear-40                   [-1, 10]          40,970\n","          Softmax-41                   [-1, 10]               0\n","================================================================\n","Total params: 266,430,282\n","Trainable params: 251,715,594\n","Non-trainable params: 14,714,688\n","----------------------------------------------------------------\n","Input size (MB): 1.45\n","Forward/backward pass size (MB): 549.98\n","Params size (MB): 1016.35\n","Estimated Total Size (MB): 1567.78\n","----------------------------------------------------------------\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/waleedka/hiddenlayer.git@master\n","  Cloning https://github.com/waleedka/hiddenlayer.git (to revision master) to /tmp/pip-req-build-leer84nv\n","  Running command git clone -q https://github.com/waleedka/hiddenlayer.git /tmp/pip-req-build-leer84nv\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return forward_call(*input, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_opset9.py:1672: UserWarning: Dropout is a training op and should not be exported in inference mode. For inference, make sure to call eval() on the model and to export it with param training=False.\n","  warnings.warn(\"Dropout is a training op and should not be exported in inference mode. \"\n"]},{"output_type":"execute_result","data":{"text/plain":["<hiddenlayer.graph.Graph at 0x7f0968c52bd0>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"1805pt\" height=\"116pt\"\n viewBox=\"0.00 0.00 1805.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(72 80)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-72,36 -72,-80 1733,-80 1733,36 -72,36\"/>\n<!-- /outputs/42 -->\n<g id=\"node1\" class=\"node\">\n<title>/outputs/42</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"190,-40 120,-40 120,-4 190,-4 190,-40\"/>\n<text text-anchor=\"start\" x=\"128\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">MaxPool2x2</text>\n</g>\n<!-- 14617908681454553813 -->\n<g id=\"node12\" class=\"node\">\n<title>14617908681454553813</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"310,-44 226,-44 226,0 310,0 310,-44\"/>\n<text text-anchor=\"start\" x=\"234\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Conv3x3 &gt; Relu</text>\n<text text-anchor=\"start\" x=\"295\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x2</text>\n</g>\n<!-- /outputs/42&#45;&gt;14617908681454553813 -->\n<g id=\"edge6\" class=\"edge\">\n<title>/outputs/42&#45;&gt;14617908681454553813</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M190.1345,-22C198.237,-22 207.0271,-22 215.6853,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.7397,-25.5001 225.7397,-22 215.7396,-18.5001 215.7397,-25.5001\"/>\n</g>\n<!-- /outputs/47 -->\n<g id=\"node2\" class=\"node\">\n<title>/outputs/47</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"416,-40 346,-40 346,-4 416,-4 416,-40\"/>\n<text text-anchor=\"start\" x=\"354\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">MaxPool2x2</text>\n</g>\n<!-- 12558322270756888333 -->\n<g id=\"node14\" class=\"node\">\n<title>12558322270756888333</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"536,-44 452,-44 452,0 536,0 536,-44\"/>\n<text text-anchor=\"start\" x=\"460\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Conv3x3 &gt; Relu</text>\n<text text-anchor=\"start\" x=\"521\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x3</text>\n</g>\n<!-- /outputs/47&#45;&gt;12558322270756888333 -->\n<g id=\"edge10\" class=\"edge\">\n<title>/outputs/47&#45;&gt;12558322270756888333</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M416.1345,-22C424.237,-22 433.0271,-22 441.6853,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"441.7397,-25.5001 451.7397,-22 441.7396,-18.5001 441.7397,-25.5001\"/>\n</g>\n<!-- /outputs/54 -->\n<g id=\"node3\" class=\"node\">\n<title>/outputs/54</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"642,-40 572,-40 572,-4 642,-4 642,-40\"/>\n<text text-anchor=\"start\" x=\"580\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">MaxPool2x2</text>\n</g>\n<!-- 9654823641575477822 -->\n<g id=\"node15\" class=\"node\">\n<title>9654823641575477822</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"762,-44 678,-44 678,0 762,0 762,-44\"/>\n<text text-anchor=\"start\" x=\"686\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Conv3x3 &gt; Relu</text>\n<text text-anchor=\"start\" x=\"747\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x3</text>\n</g>\n<!-- /outputs/54&#45;&gt;9654823641575477822 -->\n<g id=\"edge12\" class=\"edge\">\n<title>/outputs/54&#45;&gt;9654823641575477822</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M642.1345,-22C650.237,-22 659.0271,-22 667.6853,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"667.7397,-25.5001 677.7397,-22 667.7396,-18.5001 667.7397,-25.5001\"/>\n</g>\n<!-- /outputs/61 -->\n<g id=\"node4\" class=\"node\">\n<title>/outputs/61</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"868,-40 798,-40 798,-4 868,-4 868,-40\"/>\n<text text-anchor=\"start\" x=\"806\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">MaxPool2x2</text>\n</g>\n<!-- 18000895836176557862 -->\n<g id=\"node16\" class=\"node\">\n<title>18000895836176557862</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"988,-44 904,-44 904,0 988,0 988,-44\"/>\n<text text-anchor=\"start\" x=\"912\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Conv3x3 &gt; Relu</text>\n<text text-anchor=\"start\" x=\"973\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x3</text>\n</g>\n<!-- /outputs/61&#45;&gt;18000895836176557862 -->\n<g id=\"edge14\" class=\"edge\">\n<title>/outputs/61&#45;&gt;18000895836176557862</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M868.1345,-22C876.237,-22 885.0271,-22 893.6853,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"893.7397,-25.5001 903.7397,-22 893.7396,-18.5001 893.7397,-25.5001\"/>\n</g>\n<!-- /outputs/68 -->\n<g id=\"node5\" class=\"node\">\n<title>/outputs/68</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1094,-40 1024,-40 1024,-4 1094,-4 1094,-40\"/>\n<text text-anchor=\"start\" x=\"1032\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">MaxPool2x2</text>\n</g>\n<!-- 15677865886447711823 -->\n<g id=\"node10\" class=\"node\">\n<title>15677865886447711823</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1184,-44 1130,-44 1130,0 1184,0 1184,-44\"/>\n<text text-anchor=\"start\" x=\"1143\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Flatten</text>\n<text text-anchor=\"start\" x=\"1165\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x2</text>\n</g>\n<!-- /outputs/68&#45;&gt;15677865886447711823 -->\n<g id=\"edge3\" class=\"edge\">\n<title>/outputs/68&#45;&gt;15677865886447711823</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1094.1034,-22C1102.3675,-22 1111.2064,-22 1119.5686,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1119.8266,-25.5001 1129.8266,-22 1119.8266,-18.5001 1119.8266,-25.5001\"/>\n</g>\n<!-- /outputs/71/72 -->\n<g id=\"node6\" class=\"node\">\n<title>/outputs/71/72</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1274,-40 1220,-40 1220,-4 1274,-4 1274,-40\"/>\n<text text-anchor=\"start\" x=\"1230\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Dropout</text>\n</g>\n<!-- 14412741954763885589 -->\n<g id=\"node13\" class=\"node\">\n<title>14412741954763885589</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1382,-44 1310,-44 1310,0 1382,0 1382,-44\"/>\n<text text-anchor=\"start\" x=\"1318\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Linear &gt; Relu</text>\n<text text-anchor=\"start\" x=\"1367\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x2</text>\n</g>\n<!-- /outputs/71/72&#45;&gt;14412741954763885589 -->\n<g id=\"edge8\" class=\"edge\">\n<title>/outputs/71/72&#45;&gt;14412741954763885589</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1274.0522,-22C1281.9527,-22 1290.8212,-22 1299.5618,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1299.6982,-25.5001 1309.6982,-22 1299.6982,-18.5001 1299.6982,-25.5001\"/>\n</g>\n<!-- /outputs/77/78/79/80/81 -->\n<g id=\"node7\" class=\"node\">\n<title>/outputs/77/78/79/80/81</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1481,-40 1418,-40 1418,-4 1481,-4 1481,-40\"/>\n<text text-anchor=\"start\" x=\"1426.5\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">BatchNorm</text>\n</g>\n<!-- /outputs/82 -->\n<g id=\"node8\" class=\"node\">\n<title>/outputs/82</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1571,-40 1517,-40 1517,-4 1571,-4 1571,-40\"/>\n<text text-anchor=\"start\" x=\"1531\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Linear</text>\n</g>\n<!-- /outputs/77/78/79/80/81&#45;&gt;/outputs/82 -->\n<g id=\"edge1\" class=\"edge\">\n<title>/outputs/77/78/79/80/81&#45;&gt;/outputs/82</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1481.23,-22C1489.464,-22 1498.4053,-22 1506.889,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1506.932,-25.5001 1516.932,-22 1506.9319,-18.5001 1506.932,-25.5001\"/>\n</g>\n<!-- /outputs/83 -->\n<g id=\"node9\" class=\"node\">\n<title>/outputs/83</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"1661,-40 1607,-40 1607,-4 1661,-4 1661,-40\"/>\n<text text-anchor=\"start\" x=\"1617\" y=\"-19\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Softmax</text>\n</g>\n<!-- /outputs/82&#45;&gt;/outputs/83 -->\n<g id=\"edge2\" class=\"edge\">\n<title>/outputs/82&#45;&gt;/outputs/83</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1571.003,-22C1579.0277,-22 1587.9665,-22 1596.5309,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1596.7051,-25.5001 1606.705,-22 1596.705,-18.5001 1596.7051,-25.5001\"/>\n</g>\n<!-- 15677865886447711823&#45;&gt;/outputs/71/72 -->\n<g id=\"edge4\" class=\"edge\">\n<title>15677865886447711823&#45;&gt;/outputs/71/72</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1184.003,-22C1192.0277,-22 1200.9665,-22 1209.5309,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1209.7051,-25.5001 1219.705,-22 1209.705,-18.5001 1209.7051,-25.5001\"/>\n</g>\n<!-- 980930239154923773 -->\n<g id=\"node11\" class=\"node\">\n<title>980930239154923773</title>\n<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"84,-44 0,-44 0,0 84,0 84,-44\"/>\n<text text-anchor=\"start\" x=\"8\" y=\"-28\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Conv3x3 &gt; Relu</text>\n<text text-anchor=\"start\" x=\"69\" y=\"-7\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">x2</text>\n</g>\n<!-- 980930239154923773&#45;&gt;/outputs/42 -->\n<g id=\"edge5\" class=\"edge\">\n<title>980930239154923773&#45;&gt;/outputs/42</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M84.077,-22C92.4638,-22 101.305,-22 109.7918,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.9138,-25.5001 119.9138,-22 109.9138,-18.5001 109.9138,-25.5001\"/>\n</g>\n<!-- 14617908681454553813&#45;&gt;/outputs/47 -->\n<g id=\"edge7\" class=\"edge\">\n<title>14617908681454553813&#45;&gt;/outputs/47</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M310.077,-22C318.4638,-22 327.305,-22 335.7918,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"335.9138,-25.5001 345.9138,-22 335.9138,-18.5001 335.9138,-25.5001\"/>\n</g>\n<!-- 14412741954763885589&#45;&gt;/outputs/77/78/79/80/81 -->\n<g id=\"edge9\" class=\"edge\">\n<title>14412741954763885589&#45;&gt;/outputs/77/78/79/80/81</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1382.1993,-22C1390.4879,-22 1399.3633,-22 1407.8603,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1407.9723,-25.5001 1417.9722,-22 1407.9722,-18.5001 1407.9723,-25.5001\"/>\n</g>\n<!-- 12558322270756888333&#45;&gt;/outputs/54 -->\n<g id=\"edge11\" class=\"edge\">\n<title>12558322270756888333&#45;&gt;/outputs/54</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M536.077,-22C544.4638,-22 553.305,-22 561.7918,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"561.9138,-25.5001 571.9138,-22 561.9138,-18.5001 561.9138,-25.5001\"/>\n</g>\n<!-- 9654823641575477822&#45;&gt;/outputs/61 -->\n<g id=\"edge13\" class=\"edge\">\n<title>9654823641575477822&#45;&gt;/outputs/61</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M762.077,-22C770.4638,-22 779.305,-22 787.7918,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"787.9138,-25.5001 797.9138,-22 787.9138,-18.5001 787.9138,-25.5001\"/>\n</g>\n<!-- 18000895836176557862&#45;&gt;/outputs/68 -->\n<g id=\"edge15\" class=\"edge\">\n<title>18000895836176557862&#45;&gt;/outputs/68</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M988.077,-22C996.4638,-22 1005.305,-22 1013.7918,-22\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1013.9138,-25.5001 1023.9138,-22 1013.9138,-18.5001 1013.9138,-25.5001\"/>\n</g>\n</g>\n</svg>\n"},"metadata":{},"execution_count":52}],"source":["model = models.vgg16(pretrained=True)\n","#model.classifier = nn.Sequential(*[model.classifier[i] for i in range(5)])\n","print(model.features) # stampa tutto il modello a valle del classificatore \n","#print(model)\n","#print(model.classifier) # stampa tutto il classificatore \n","model.avgpool = nn.Identity() #sostituisco l'avg pool con un layer identità \n","#print(model.avgpool) #stampo quel layer per controllo \n","\n","model.classifier = nn.Sequential(\n","        nn.Flatten(),\n","        nn.Dropout(0.5, inplace=False),\n","        nn.Linear(in_features=57344, out_features=4096,bias=True),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(in_features=4096, out_features=4096,bias=True), \n","        nn.ReLU(inplace=True),\n","        nn.BatchNorm1d(4096, affine=True),\n","        nn.Linear(4096, 10),\n","        nn.Softmax()\n",")\n","#print(model.classifier)\n","#model.classifier[6] = nn.Linear(in_features=4096, out_features=10, bias=True)\n","\n","#Freeze Weights Convolution Layer \n","for name, layer in model.named_modules():\n","  #print(name)\n","  #conv layer \n","  if name == 'features.0':\n","    layer.weight.requires_grad = False \n","  if name == 'features.2': \n","    layer.weight.requires_grad = False \n","  if name == 'features.5':\n","    layer.weight.requires_grad = False \n","  if name == 'features.7': \n","    layer.weight.requires_grad = False \n","  if name == 'features.10':\n","    layer.weight.requires_grad = False \n","  if name == 'features.12': \n","    layer.weight.requires_grad = False \n","  if name == 'features.14':\n","    layer.weight.requires_grad = False \n","  if name == 'features.17': \n","    layer.weight.requires_grad = False \n","  if name == 'features.19': \n","    layer.weight.requires_grad = False\n","  if name == 'features.21':\n","    layer.weight.requires_grad = False \n","  if name == 'features.24': \n","    layer.weight.requires_grad = False \n","  if name == 'features.26':\n","    layer.weight.requires_grad = False \n","  if name == 'features.28':\n","    layer.weight.requires_grad = False \n","\n","model = model.to(device)\n","summary(model,input_size=(3, 270,470))\n","\n","#Plot Model \n","%pip install -U git+https://github.com/waleedka/hiddenlayer.git@master\n","import hiddenlayer as hl\n","hl.build_graph(model, torch.zeros([3, 3, 270, 470]))\n"]},{"cell_type":"markdown","metadata":{"id":"2XV_nhqtTWxr"},"source":["## Metrica Balance Accuracy "]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1656671135240,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"P-rg-uNKANyC"},"outputs":[],"source":["\n","'''\n","Funzione per Balance Accuracy \n","-- Link Utile : https://medium.com/@mostafa.m.ayoub/customize-your-keras-metrics-44ac2e2980bd --\n","-- https://medium.com/analytics-vidhya/custom-metrics-for-keras-tensorflow-ae7036654e05 --- \n","-- https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score --- \n","'''\n","\n","'''\n","---- Questo caso funzione solo nel caso di classificazioni binarie ---- \n","def monitor_balance_accuracy ():\n","\tdef bal_acc(y_true, y_pred):\n","\t\ttp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","\t\tfn = K.sum(K.round(K.clip(y_true, 0, 1)))\n","\t\t#sensitivity = tp / (fn + K.epsilon()) #--- primo test : versione trovata sul web, ma non tornano le formule\n","\t\tsensitivity = tp / (fn + tp + K.epsilon()) #--OK\n","\n","\t\ttn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n","\t\tfp = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n","\t\t#specificity = tn / (fp + K.epsilon()) #--- primo test : versione trovata sul web, ma non tornano le formule \n","\t\tspecificity = tn / (fp + tn + K.epsilon()) #--OK \n","\t\n","\t\tBalanced_Accuracy = (sensitivity+specificity)/2 #--OK\n","\t\treturn Balanced_Accuracy \n","\treturn bal_acc\n","'''\n","\t\t\n","'''\n","Funzione per Balance Accuracy \n","'''\n","def monitor_balance_accuracy ():\n","\tdef bal_acc(y_true, y_pred):\n","\t\ty_true = y_true.numpy().argmax(axis=1) #Returns the indices of the maximum values along an axis.\n","\t\ty_pred = y_pred.numpy().argmax(axis=1) #Returns the indices of the maximum values along an axis.\n","\t\tBalanced_Accuracy = balanced_accuracy_score(y_true, y_pred)\n","\t\tBalanced_Accuracy = torch.tensor(Balanced_Accuracy) ##-- va trasformata in tensore (torch.tensor)\n","\t\treturn (Balanced_Accuracy) #--- capire se si può togliere il K. \n","\treturn bal_acc\n","\n","def _bal_acc_(y_true, y_pred):\n","\ty_true = y_true.detach().numpy()\n","\ty_pred = y_pred.detach().numpy()\n","#\tprint(y_pred)\n","#\tprint(len(y_pred))\n","#\tprint(type(y_pred))\n","#\tprint(y_true)\n","#\tprint(len(y_true))\n","#\tprint(type(y_true))\n","#\tprint('categorical')\n","\ty_true = custom_to_categorical(y_true,10)\n","\ty_true = y_true.argmax(axis=1) #Returns the indices of the maximum values along an axis.\n","\ty_pred = y_pred.argmax(axis=1) #Returns the indices of the maximum values along an axis.\n","\tBalanced_Accuracy = balanced_accuracy_score(y_true, y_pred)\n","\tBalanced_Accuracy = torch.tensor(Balanced_Accuracy) ##-- va trasformata in tensore (torch.tensor)\n","\treturn (Balanced_Accuracy) \n"]},{"cell_type":"markdown","metadata":{"id":"VX2SB5uYzzyB"},"source":["## Early Stopping Class"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1656671135241,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"gvAnOHC2W134"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","import custom_early_stopping \n","from custom_early_stopping import _EarlyStopping"]},{"cell_type":"markdown","metadata":{"id":"VQsYqmoQVJIu"},"source":["## PREPROCESSING & DATA FRAME "]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1656671135242,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"5Y0wGA0Ji5Jv"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","import check_for_leakage_function\n","from check_for_leakage_function import check_for_leakage"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6973,"status":"ok","timestamp":1656671142195,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"kCvpWRyju3cE","outputId":"bac60fde-b5a8-4ff0-aea9-d4181479dd11"},"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------------\n","DATAFRAME COMPLETO INIZIALE\n","result\n","        ID  series            filename class\n","0        3       2  20201 319 5323.png    3+\n","1        4       2  20201 3110125 .png    3+\n","2        5       2  20201 31101327.png    3+\n","3        6       2  20201 3110161 .png    3+\n","4        7       2  20201 3110177 .png    3+\n","...    ...     ...                 ...   ...\n","1059  2023       7  20201031090549.png    3+\n","1060  2024       7  20201031090855.png    3+\n","1061  2025       7  20201031091127.png    3+\n","1062  2026       7  20201031091720.png    3+\n","1063  2027       7  20201031091941.png    3+\n","\n","[2128 rows x 4 columns]\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","Number of Null values in column 'quality_classes' : 2\n","- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n","mostro quegli elementi che hanno valore nullo\n","       ID  series            filename  class\n","963  1927       3  20200825181909.png    NaN\n","963  1927       3  20200825181932.png    NaN\n","- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n","Rimuovo gli elementi nulli e verifico stampando nuovamente i valori nulli:\n","elementi nulli rimasti: 0\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","elimino i file che non sono presenti in Google Drive anche se ci sono nel CSV\n","CHECK FILE NON PRESENTI NELLA CARTELLA\n","File Non Esiste !!!\n","File : 20202 13101023.png eliminato\n","File Non Esiste !!!\n","File : 20200825180901.png eliminato\n","File Non Esiste !!!\n","File : 20200825181058.png eliminato\n","File Non Esiste !!!\n","File : 20202 13101011.png eliminato\n","File Non Esiste !!!\n","File : 20200825180918.png eliminato\n","File Eliminati : 5 \n","CHECK FILE CON NaN\n","Empty DataFrame\n","Columns: [ID, series, filename, class]\n","Index: []\n","Empty DataFrame\n","Columns: [ID, series, filename, class]\n","Index: []\n","Empty DataFrame\n","Columns: [ID, series, filename, class]\n","Index: []\n","Empty DataFrame\n","Columns: [ID, series, filename, class]\n","Index: []\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","SPLIT DATA\n","train_balance_df\n","        ID series            filename  class\n","0      398      4  20203 3 7 0 38.png    7.0\n","1      398      4  20203 3 7 0 19.png    8.0\n","2     1944     12  20201010071452.png    6.0\n","3     1944     12  20201010071339.png    6.0\n","4      463      4  20203 108 147 .png    8.0\n","...    ...    ...                 ...    ...\n","1269   521      4  20203 10110 49.png    7.0\n","1270   265      2  20202 14123442.png    5.0\n","1271   265      2  20202 14123432.png    5.0\n","1272  1942     12  20201010070727.png    5.0\n","1273  1942     12  20201010070550.png    5.0\n","\n","[1274 rows x 4 columns]\n","test_balance_df\n","       ID series            filename  class\n","0     305      3  20203 2 7 1944.png    0.0\n","1     305      3  20203 2 7 1932.png    0.0\n","2     331      3  20203 2 8 8 22.png    0.0\n","3     331      3  20203 2 8 8 1 .png    4.0\n","4     271      3  20202 2716421 .png    0.0\n","..    ...    ...                 ...    ...\n","419   261      2  20202 14122743.png    5.0\n","420  1936      3  20200929122824.png    6.0\n","421  1936      3  20200929122632.png    6.0\n","422  1917      6  20200609213900.png    5.0\n","423  1917      6  20200609213844.png    5.0\n","\n","[424 rows x 4 columns]\n","val_balance_df\n","       ID series            filename  class\n","0    1683      6  20200525173531.png    4.0\n","1    1683      6  20200525173519.png    4.0\n","2     175      2  20202 138 560 .png    4.0\n","3     175      2  20202 138 5546.png    4.0\n","4     292      3  20203 2 7 4 51.png    0.0\n","..    ...    ...                 ...    ...\n","417  1729      7  20200526173659.png    2.0\n","418  1858     11  20200603131724.png    9.0\n","419  1858     11  20200603131710.png    7.0\n","420  1825      4  20200529181313.png    9.0\n","421  1825      4  20200529181301.png    9.0\n","\n","[422 rows x 4 columns]\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","conta del numero di immagini per speicfica classe in set Train\n","1274\n","1:95\n","2-:83\n","2:132\n","2+:110\n","3-:111\n","3:179\n","3+:206\n","4-:125\n","4:171\n","4+:62\n","conta del numero di immagini per speicfica classe in set Validation\n","422\n","1:33\n","2-:26\n","2:42\n","2+:39\n","3-:33\n","3:64\n","3+:68\n","4-:40\n","4:56\n","4+:21\n","conta del numero di immagini per speicfica classe in set Test\n","424\n","1:37\n","2-:39\n","2:38\n","2+:28\n","3-:35\n","3:64\n","3+:69\n","4-:43\n","4:48\n","4+:23\n","Weight train_balance_df\n","{0: 1.34, 1: 1.53, 2: 0.97, 3: 1.16, 4: 1.15, 5: 0.71, 6: 0.62, 7: 1.02, 8: 0.75, 9: 2.05}\n","Weight val_balance_df\n","{0: 1.28, 1: 1.62, 2: 1.0, 3: 1.08, 4: 1.28, 5: 0.66, 6: 0.62, 7: 1.06, 8: 0.75, 9: 2.01}\n","Weight test_balance_df\n","{0: 1.15, 1: 1.09, 2: 1.12, 3: 1.51, 4: 1.21, 5: 0.66, 6: 0.61, 7: 0.99, 8: 0.88, 9: 1.84}\n","------------------------------------------------------------------------------------------------------------------------------------------------------------\n","test case 1 - train VS validation\n","Stessi ID in set usati?: False\n","-------------------------------------\n","test case 2 - train VS test\n","Stessi ID in set usati ?: False\n","-------------------------------------\n","test case 3 - validation VS test\n","Stessi ID in set usati?: False\n"]}],"source":["'''PREPROCESSING PHASE OF THE DATAFRAME (CREATIONS OF THE SUBSETS TRAIN/VALIDATION/TEST, CALCULATE WEIGHTS OF ELEMENTS OF THE SUBSETS, VERIFY THAT SAME IDs ARE IN THE SAME SUBSET)'''\n","os.chdir(path_progettoDL)\n","path = os.getcwd()\n","\n","'''reading informations from the CSV'''\n","col_list_sx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATOSX\", \"CLASSE_CALCIOSX\"]\n","dataframe_sx = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_sx, sep=\";\")\n","\n","col_list_dx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATODX\", \"CLASSE_CALCIODX\"]\n","dataframe_dx = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_dx, sep=\";\")\n","\n","'''rename the dataframe columns'''\n","dataframe_sx.columns = ['ID','series', 'filename', 'class']\n","dataframe_dx.columns = ['ID','series', 'filename', 'class']\n","\n","frames = [dataframe_sx, dataframe_dx] \n","result = pd.concat(frames) #concatenate the two dataframes\n","\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"DATAFRAME COMPLETO INIZIALE\")\n","print(\"result\")\n","print(result)\n","\n","#print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","#print(\"STAMPO ELEMENTO/I CON INDICE 1 (elemento tutto a sinistra)\")\n","#print(result.loc[[1]])\n","#print(type(result.loc[[1]]))    #STAMPO IL TIPO DELL'ELEMENTO \n","\n","#print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","#print(\"LUNGHEZZA DATAFRAME COMPLESSIVO : {} \".format(result[result.columns[0]].count()))\n","\n","\n","'''mapping the values used for the classification into integer values'''\n","#version with 10 classes\n","result[\"class\"] = result[\"class\"].map({'1': int(0), '2-': int(1), '2': int(2), '2+': int(3), '3-': int(4), '3': int(5), '3+': int(6), '4-': int(7), '4': int(8), '4+': int(9)})\n","result[\"series\"] = result[\"series\"].map({2: int(0), 4: int(1), 8: int(2), 10: int(3), 6: int(4), 9: int(5), 3: int(6), 11: int(7), 12: int(8), 13: int(9), 14: int(10), 15: int(11), 7: int(12)}) \n","\n","\n","'''identification of NULL values that would bring the execution on failing and eliminate those values'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"Number of Null values in column 'quality_classes' : \"+format(result['class'].isnull().sum()))\n","print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n","#print(result.loc[result['class'] == '0'])\n","print(\"mostro quegli elementi che hanno valore nullo\")\n","print(result[result['class'].isnull()])\n","print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n","\n","'''Remove Null elements to avoid failures during executions (data in not useful!)'''\n","print(\"Rimuovo gli elementi nulli e verifico stampando nuovamente i valori nulli:\")\n","result['class'] = pd.to_numeric(result['class'], errors='coerce')\n","result = result.dropna(subset=['class'])    #rimuovo le righe con elementi nulli\n","\n","print(\"elementi nulli rimasti: \"+format(result['class'].isnull().sum()))     #stampo per verifica se ci sono elementi nulli\n","\n","\n","'''verify if images exist in the Google Drive folder, when not present it is eliminated from the dataset aswell'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"elimino i file che non sono presenti in Google Drive anche se ci sono nel CSV\")\n","print('CHECK FILE NON PRESENTI NELLA CARTELLA')\n","\n","os.chdir(path_images)\n","i = 0; \n","for index, row in result.iterrows():\n","    filename = row['filename']\n","    if os.path.exists(path_images+filename) == False:\n","      print('File Non Esiste !!!')\n","    \n","    if(os.path.exists(filename) == False):\n","      result = result.drop(result[(result['filename'] == filename)].index)\n","      print('File : {} eliminato'.format(filename))\n","      i = i + 1             \n","print('File Eliminati : {} '.format(i))\n","\n","print('CHECK FILE CON NaN')\n","print(result[result['class'].isnull()])\n","print(result[result['series'].isnull()])\n","print(result[result['filename'].isnull()])\n","print(result[result['ID'].isnull()])\n","result = result[result['class'].notna()]\n","result = result[result['series'].notna()]\n","result = result[result['filename'].notna()]\n","result = result[result['ID'].notna()]\n","\n","\n","\n","#----PER FARE LE PROVE RIDUCO LA DIMENSIONE DI RESULTS#\n","#result = result[:,100]\n","\n","'''creation of masked images (grayscale images) and save them in Google Drive'''\n","'''than create a second dataframe with these new images'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","'''\n","mask_filenames = []\n","IDs = []\n","classes = []\n","for index, row in result.iterrows():\n","    filename = row['filename']\n","    mask_filenames.append(str(\"mask_\"+filename))\n","    IDs.append(row['ID'])\n","    classes.append(row['class'])\n","\n","print(\"DATAFRAME CON MASCHERE\")\n","result2 = result.copy()\n","result2['mask_filename'] = mask_filenames\n","result2.drop('filename', axis='columns', inplace=True)   #rimuovo colonna con path immagini normali\n","\n","column_names = [\"ID\",\"series\", \"mask_filename\", \"class\"]\n","result2 = result2.reindex(columns=column_names)\n","\n","print(\"result2\")\n","print(result2)\n","#stampa della conta delle serie dei calci presenti nel dataframe\n","'''\n","\n","'''performing the splitting of the dataframe into sub-sets'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"SPLIT DATA\")\n","\n","train_balance_df, test_balance_df, val_balance_df  = split_data(result, 0.2, 0.2, 3)  #CUSTOM SPLIT CON ID IN STESSO SET DI DATI\n","#train_mask, test_mask, validation_mask  = split_data(result2, 0.2, 0.2, 3)           #split per test con immagini con maschere\n","\n","print(\"train_balance_df\")\n","print(train_balance_df)\n","print(\"test_balance_df\")\n","print(test_balance_df)\n","print(\"val_balance_df\")\n","print(val_balance_df)\n","\n","\n","#------------------------version with 4 classes (togliere se si lavora con 10 classi)-----------------------------------\n","#train_balance_df[\"class\"] = train_balance_df[\"class\"].map({0: int(0), 1: int(1), 2: int(1), 3: int(1), 4: int(2), 5: int(2), 6: int(2), 7: int(3), 8: int(3), 9: int(3)})\n","#val_balance_df[\"class\"] = val_balance_df[\"class\"].map({0: int(0), 1: int(1), 2: int(1), 3: int(1), 4: int(2), 5: int(2), 6: int(2), 7: int(3), 8: int(3), 9: int(3)})\n","#test_balance_df[\"class\"] = test_balance_df[\"class\"].map({0: int(0), 1: int(1), 2: int(1), 3: int(1), 4: int(2), 5: int(2), 6: int(2), 7: int(3), 8: int(3), 9: int(3)})\n","\n","\"\"\"\n","NOTA: versione dei metodi di tensorflow, che non divide però mantenendo stessi ID in stessi Sub-set\n","train_balance_df, test_balance_df = train_test_split(result, test_size=0.4, stratify=result['class'], random_state=2)\n","test_balance_df, val_balance_df = train_test_split(test_balance_df, test_size=0.5, stratify=test_balance_df['class'],random_state=2)\n","\"\"\"\n","\n","'''verify distibution of classes in the sub-sets and calculate weights of the classes in each sub-set'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","vals, counts = np.unique(train_balance_df['class'], return_counts=True)\n","print(\"conta del numero di immagini per speicfica classe in set Train\")\n","print(len(train_balance_df))\n","for i in range(0,len(classi)):\n","    print('{}:{}'.format(classi[i], counts[i]))\n","\n","vals2, counts2 = np.unique(val_balance_df['class'], return_counts=True)\n","print(\"conta del numero di immagini per speicfica classe in set Validation\")\n","print(len(val_balance_df))\n","for i in range(0,len(classi)):\n","    print('{}:{}'.format(classi[i], counts2[i]))\n","\n","vals3, counts3 = np.unique(test_balance_df['class'], return_counts=True)\n","print(\"conta del numero di immagini per speicfica classe in set Test\")\n","print(len(test_balance_df))\n","for i in range(0,len(classi)):\n","    print('{}:{}'.format(classi[i], counts3[i]))    \n","\n","\n","class_weights_train = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(train_balance_df['class']),y = train_balance_df['class'])\n","weight_train = {i : round(class_weights_train[i], 2) for i in range(len(classi))} \n","print('Weight train_balance_df')\n","print(weight_train)\n","\n","\n","class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(val_balance_df['class']),y = val_balance_df['class'])\n","weight = {i : round(class_weights[i], 2) for i in range(len(classi))} \n","print('Weight val_balance_df')\n","print(weight)\n","\n","\n","class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(test_balance_df['class']),y = test_balance_df['class'])\n","weight = {i : round(class_weights[i], 2) for i in range(len(classi))} \n","print('Weight test_balance_df')\n","print(weight)\n","\n","\n","'''verify that same IDs are in the same sub-sets'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","\n","#--------verifico che stessi ID siano in stesso set--------\n","print(\"test case 1 - train VS validation\")\n","print(f\"Stessi ID in set usati?: {check_for_leakage(train_balance_df, val_balance_df, 'ID')}\")\n","print(\"-------------------------------------\")\n","print(\"test case 2 - train VS test\")\n","print(f\"Stessi ID in set usati ?: {check_for_leakage(train_balance_df, test_balance_df, 'ID')}\")\n","print(\"-------------------------------------\")\n","print(\"test case 3 - validation VS test\")\n","print(f\"Stessi ID in set usati?: {check_for_leakage(val_balance_df, test_balance_df, 'ID')}\")\n"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126,"status":"ok","timestamp":1656671142197,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"QPihVVu9fXJb","outputId":"89bc381b-1edb-4f99-867d-fd9cf4ccae22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Verifica Classi Qualità per ogni Serie\n","SOMMA IMG totali: 2120\n"]}],"source":["'''Verifica Classi Qualità per ogni Serie'''\n","print(\"Verifica Classi Qualità per ogni Serie\")\n","result_x_ = result.groupby(['series','class']).size()\n","#print(result_x_)\n","result_class = result.groupby(['class']).size()\n","#print(result_class)\n","result_series = result.groupby(['series']).size()\n","#print(result_series)  \n","print('SOMMA IMG totali: {}'.format(np.sum(result_class)))\n"]},{"cell_type":"markdown","metadata":{"id":"26Za2K_yFljo"},"source":["## Loss Function"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":118,"status":"ok","timestamp":1656671142199,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"TZ05ClCuFnkj"},"outputs":[],"source":["from keras import backend as K\n","class weighted_categorical_crossentropy(object):\n","    \"\"\"\n","    A weighted version of keras.objectives.categorical_crossentropy\n","    \n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","    \n","    Usage:\n","        loss = weighted_categorical_crossentropy(weights).loss\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","    \n","    def __init__(self,weights):\n","        self.weights = K.variable(weights)\n","        \n","    def loss(self,y_true, y_pred): \n","        # scale preds so that the class probas of each sample sum to 1\n","        y_pred /= K.sum(y_pred)\n","        # clip\n","        y_pred = K.clip(y_pred, K.epsilon(), 1)\n","        # calc\n","        \n","        loss = y_true*K.log(y_pred)*self.weights\n","        loss =-K.sum(loss,-1)\n","        return loss"]},{"cell_type":"markdown","metadata":{"id":"Ks3Fly2oUf3g"},"source":["## HYPERPARAMETERS"]},{"cell_type":"markdown","metadata":{"id":"DXwAzoJE_OIP"},"source":["\n","The **optimization algorithm** (or optimizer) is the main approach used today for training a machine learning model to minimize its error rate. There are *two metrics* to determine the efficacy of an optimizer: **speed of convergence** (the process of reaching a global optimum for gradient descent); and **generalization** (the model’s performance on new data)\n","\n","***SGD*** : Stochastic Gradient Descent \n","\n","Parameters \n","- Learning : learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. ... In setting a learning rate, there is a trade-off between the rate of convergence and overshooting\n","- Momentum : Momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n","- Decay :  We then set our decay to be the learning rate divided by the total number of epochs we are training the network for (a common rule of thumb) ... lr = (lr_iniziale - (1.0/(1-decay*iterations)))\n","- Nesterov: Nesterov which is set to false by default. Nesterov momentum is a different version of the momentum method which has stronger theoretical converge guarantees for convex functions.\n","\n","[1° LINK](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)\n","\n","[2° LINK](https://www.pyimagesearch.com/2019/07/22/keras-learning-rate-schedules-and-decay/)\n","\n","\n","![1*VQkTnjr2VJOz0R2m4hDucQ.jpeg](data:image/jpeg;base64,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)\n","\n","***ADAM*** :  Adaptive Moment Estimation\n","\n","\n","\n","\n","FONTI : Paper for ICLR 2019"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":118,"status":"ok","timestamp":1656671142201,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"Bbjwj4QwW6yx","outputId":"8ac5039b-9a26-4263-9a32-613c6bed2046"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nlosses = {\\n\\t\"class_output\": \\'categorical_crossentropy\\'\\n}\\nlossWeights = { \"class_output\": 1}\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":59}],"source":["#Definizione hyperparameters                                                                                                                        \n","optimizer_ = optim.SGD(model.parameters(), weight_decay=1e-5, lr=0.001, momentum=0.8)\n","epochs = 30 #100  \n","bs = 64\n","\n","'''\n","losses = {\n","\t\"class_output\": 'categorical_crossentropy'\n","}\n","lossWeights = { \"class_output\": 1}\n","'''\n","#hyper parametri test "]},{"cell_type":"markdown","metadata":{"id":"Utp1VUbSWy_0"},"source":["## CREAZIONE DATASET di TRAIN, VAL, TEST + TRANSFORM + creazione patch "]},{"cell_type":"markdown","metadata":{"id":"DBHMPMTPX5V5"},"source":["#### D.A. offline"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2087,"status":"ok","timestamp":1656671345074,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"gkPcoLlpXkyg","outputId":"b4f1cd58-c673-48ff-c481-565431cfd405"},"outputs":[{"output_type":"stream","name":"stdout","text":["        ID series            filename  class\n","0      398      4  20203 3 7 0 38.png    7.0\n","1      398      4  20203 3 7 0 19.png    8.0\n","2     1944     12  20201010071452.png    6.0\n","3     1944     12  20201010071339.png    6.0\n","4      463      4  20203 108 147 .png    8.0\n","...    ...    ...                 ...    ...\n","1269   521      4  20203 10110 49.png    7.0\n","1270   265      2  20202 14123442.png    5.0\n","1271   265      2  20202 14123432.png    5.0\n","1272  1942     12  20201010070727.png    5.0\n","1273  1942     12  20201010070550.png    5.0\n","\n","[1274 rows x 4 columns]\n","n° immagini per ciascuna classe  [ 95.  83. 132. 110. 111. 179. 206. 125. 171.  62.]\n","La classe maggioritaria contiene n:  206.0 immagini\n","La classe minoritaria contiene n:  62.0 immagini\n","classe minima :  9\n","classe massima :  6\n","n° immagini da creare per ciascuna classe  [111. 123.  74.  96.  95.  27.   0.  81.  35. 144.]\n","devo creare immagini della classe :  0\n","len actual df 95\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/series.py:1056: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  cacher_needs_updating = self._check_is_chained_assignment_possible()\n"]},{"output_type":"stream","name":"stdout","text":["devo creare immagini della classe :  1\n","len actual df 83\n","devo creare immagini della classe :  2\n","len actual df 132\n","devo creare immagini della classe :  3\n","len actual df 110\n","devo creare immagini della classe :  4\n","len actual df 111\n","devo creare immagini della classe :  5\n","len actual df 179\n","non devo creare immagini della classe :  6\n","devo creare immagini della classe :  7\n","len actual df 125\n","devo creare immagini della classe :  8\n","len actual df 171\n","devo creare immagini della classe :  9\n","len actual df 62\n","class_weight train_balance_df_copy_off [1. 1. 1. ... 1. 1. 1.]\n","n° immagini per ciascuna classe  [206. 206. 206. 206. 206. 206. 206. 206. 206. 206.]\n"]}],"source":["\n","print(train_balance_df)\n","train_balance_df_copy_off = train_balance_df\n","n_img_for_class = np.zeros(10)\n","for i in range(10):\n","  n_img_for_class[i] = len(train_balance_df[train_balance_df['class']==i])\n","print(f'n° immagini per ciascuna classe ', n_img_for_class) #-- il n° di img per ciascuna classe \n","\n","classe_maggioritaria = np.max(n_img_for_class)\n","classe_minoritaria = np.min(n_img_for_class)\n","print(f'La classe maggioritaria contiene n: ',classe_maggioritaria,'immagini')\n","print(f'La classe minoritaria contiene n: ',classe_minoritaria,'immagini')\n","index_min = np.argmin(n_img_for_class)\n","index_max = np.argmax(n_img_for_class)\n","print(f'classe minima : ',index_min)\n","print(f'classe massima : ',index_max)\n","\n","\n","# calcolo n° immagini da creare per ciascuna classe di qualità - non lo usiamo direttamente\n","n_img_for_class_to_create = np.zeros(10)\n","for i in range(10):\n","  n_img_for_class_to_create[i] = classe_maggioritaria - n_img_for_class[i] \n","print(f'n° immagini da creare per ciascuna classe ', n_img_for_class_to_create) #-- il n° di img per ciascuna classe \n","\n","\n","#Data Augmentation Offline - Parto dalla prima classe di qualità e genero immagini volta per volta fino ad arrivare al numero della maggioritaria\n","for k in range(10): \n","  #print(k)\n","  actual_df = train_balance_df[train_balance_df['class']==k]\n","  #print(actual_df)\n","  if n_img_for_class_to_create[k] > 0:\n","    print(f'devo creare immagini della classe : ',k)\n","    n_img_in_actual_df = len(actual_df)\n","    print(f'len actual df', n_img_in_actual_df)\n","    count = n_img_in_actual_df\n","    #for row in range(int(n_img_in_actual_df)):\n","    for i in range(int(abs(classe_maggioritaria/classe_minoritaria))): \n","      for row in range(int(n_img_in_actual_df)):\n","        if (count < classe_maggioritaria):\n","          count = count + 1\n","          row_selected = actual_df.iloc[row]\n","          #--- D.A. offline  \n","          filename = row_selected['filename']\n","          \n","          img_path = os.path.join(path_images+filename)\n","          \n","\n","          if i==0: \n","            if os.path.exists(path_images+'T1_'+filename) == False:\n","              image = io.imread(img_path)\n","              image = _transform_1(image)\n","              save_image(image, path_images+'T1_'+filename)\n","            row_selected['filename'] = 'T1_'+filename\n","            #print(row_selected['filename'])\n","            train_balance_df_copy_off = train_balance_df_copy_off.append(row_selected, ignore_index = True)   \n","          if i==1: \n","            if os.path.exists(path_images+'T2_'+filename) == False:\n","              image = io.imread(img_path)\n","              image = _transform_2(image)\n","              save_image(image, path_images+'T2_'+filename)\n","            row_selected['filename'] = 'T2_'+filename\n","            #print(row_selected['filename'])\n","            train_balance_df_copy_off = train_balance_df_copy_off.append(row_selected, ignore_index = True)\n","          if i==2: \n","            if os.path.exists(path_images+'T3_'+filename) == False:\n","              image = io.imread(img_path)\n","              image = _transform_3(image)\n","              save_image(image, path_images+'T3_'+filename)\n","            row_selected['filename'] = 'T3_'+filename\n","            #print(row_selected['filename'])\n","            train_balance_df_copy_off = train_balance_df_copy_off.append(row_selected, ignore_index = True)    \n","        else: \n","          break\n","  else:\n","    print(f'non devo creare immagini della classe : ',k)\n","# check di verifica \n","#print(f'class_weight train_balance_df',compute_sample_weight(class_weight='balanced', y=train_balance_df['class']))\n","print(f'class_weight train_balance_df_copy_off',compute_sample_weight(class_weight='balanced', y=train_balance_df_copy_off['class']))\n","n_img_for_class_after = np.zeros(10)\n","for i in range(10):\n","  n_img_for_class_after[i] = len(train_balance_df_copy_off[train_balance_df_copy_off['class']==i])\n","print(f'n° immagini per ciascuna classe ', n_img_for_class_after) #-- il n° di img per ciascuna classe \n","\n","\n","train_c_dataset = CustomDataset(train_balance_df_copy_off, transform_0=_transform_)\n","val_c_dataset = CustomDataset(val_balance_df, transform_0=_transform_)\n","test_c_dataset = CustomDataset(test_balance_df,transform_0=_transform_ )\n"]},{"cell_type":"markdown","metadata":{"id":"ZnbL-T3PAGWI"},"source":["#### D.A. offline - GAN images load from drive"]},{"cell_type":"markdown","metadata":{"id":"M08_ckvtTEzl"},"source":["RICERCA DELLE IMMAGINI E SALVATAGGIO DEI NOMI SU UN CSV"]},{"cell_type":"markdown","metadata":{"id":"HeBvfBU5GLKO"},"source":["DATA AUGMENTATION ON THE FLY - AUMENTARE LE IMMAGINI INVECE DI MODIFICARLE E BASTA\n","https://blog.paperspace.com/data-augmentation-for-object-detection-building-input-pipelines/ - NON SONOSICURO CHE MOLTIPLICHI LE IMMAGINI\n","\n","https://discuss.pytorch.org/t/understand-data-augmentation-in-pytorch/139720\n","\n","https://discuss.pytorch.org/t/increase-number-of-images/132075/6\n","\n","\n","\n","----------------------------------------------------------------------------------------------------------------------------\n","\n","\n","DATA AUGMENTATION OFFLINE - AUGMENTING IMAGES\n","https://www.analyticsvidhya.com/blog/2019/12/image-augmentation-deep-learning-pytorch/\n","\n","----------------------------------------------------------------------------------------------------------------------------\n","\n","SUGGERIMENTI SU NORMALIZZAZIONE E COME FARLA, PER AIUTARE A VELOCIZZARE IL TRAINING!\n","https://glassboxmedicine.com/2022/01/21/building-custom-image-data-sets-in-pytorch-tutorial-with-code/\n","\n","----------------------------------------------------------------------------------------------------------------------------\n","\n","DATA AUGMENTATION ON THE FLY - METODI PER MODIFICARE LE IMMAGINI (TRANSFORMS)\n","https://github.com/gatsby2016/Augmentation-PyTorch-Transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":533,"status":"aborted","timestamp":1656671142644,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"0UBAoRewI5X7"},"outputs":[],"source":["\"\"\"\n","'''PARTE DOVE SI USA IL METODO ImageFolder PER LEGGERE TUTTE LE IMMAGINI IN UNA CARTELLA SENZA SAPERNE IL NOME E SALVARLE IN CSV E ALLEGARLE AL DATASET '''\n","\n","# Define directory containing images-\n","data_dir = '/content/drive/MyDrive/CALCIO_CROP_BASE/'\n","data_dir_2 = '/content/drive/MyDrive/CALCIO_CROP_BASE/Data_Aug_GAN'\n","\n","# Define datasets-\n","train_data = datasets.ImageFolder(data_dir + 'Data_Aug_GAN', \n","                                  transform = transform_train)\n","\n","print(train_data.imgs[785][0])  #QUESTO è IL NOME DEL FILE\n","print(train_data.imgs[785][1])  #QUESTO è LA CLASSE DI QUALITA' PESCATA DAL NOME DELLA CARTELLA\n","\n","\n","import os\n","#https://www.studytonight.com/python-howtos/how-to-get-the-last-part-of-the-path-in-python\n","path = os.path.basename(os.path.normpath(train_data.imgs[0][0]))  #ESTRARRE L'ULTIMA PARTE DI UN PERCORSO IN STRINGA, QUINDI IL NOME DEL FILE\n","print(path)\n","\n","print(f\"number of train images = {len(train_data)}\")\n","print(f\"number of training classes = {len(train_data.classes)}\")\n","\n","\n","i = 0\n","os.chdir(path_progettoDL)\n","path = os.getcwd()\n","\n","\n","#CREO L'INTESTAZIONE DEL FILE\n","with open(path + '/GAN_DB.txt', 'a') as f:\n","    f.write('ID;COD_COMPONENTE;IMG;CLASSE_CALCIO')\n","\n","#memorizzo ogni riga come il precedente file txt per le immagini vere\n","for i in range(len(train_data)):\n","  with open(path + '/GAN_DB.txt', 'a') as f:\n","    if train_data.imgs[i][1] > 5:\n","      f.write('\\n{};10;Data_Aug_GAN/images_{}/{};{}'.format(i+2028,train_data.imgs[i][1]+1, os.path.basename(os.path.normpath(train_data.imgs[i][0])),train_data.imgs[i][1]+1))  #mi serve anche questo caso perchè la classe 6 non esiste, e lui scorrendo poi dalla classe 7 la considera come la 6\n","    else:\n","      f.write('\\n{};10;Data_Aug_GAN/images_{}/{};{}'.format(i+2028,train_data.imgs[i][1], os.path.basename(os.path.normpath(train_data.imgs[i][0])),train_data.imgs[i][1]))\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":535,"status":"aborted","timestamp":1656671142646,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"5FviH-aLlMPj"},"outputs":[],"source":["#READ CSV WITH THE INFOs & CREATE THE FINAL BALANCED DATASET TO BE USED\n","os.chdir(path_progettoDL)\n","path = os.getcwd()\n","\n","'''reading informations from the CSV'''\n","col_list = [\"ID\", \"COD_COMPONENTE\", \"IMG\", \"CLASSE_CALCIO\"]\n","\n","dataframe_GAN = pd.read_csv(os.path.join(path + '/GAN_DB.txt'), usecols=col_list, sep=\";\")\n","dataframe_GAN.columns = ['ID','series', 'filename', 'class']\n","\n","\n","print(train_balance_df)\n","print(dataframe_GAN)\n","frames = [train_balance_df, dataframe_GAN] \n","result_2 = pd.concat(frames) #concatenate the two dataframes\n","\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"DATAFRAME COMPLETO INIZIALE\")\n","print(\"result\")\n","print(result_2)\n","\n","os.chdir(path_images)\n","i = 0; \n","for index, row in result_2.iterrows():\n","    filename = row['filename']\n","    if os.path.exists(path_images+filename) == False:\n","      print('File Non Esiste !!!')\n","    \n","    if(os.path.exists(filename) == False):\n","      result_2 = result_2.drop(result_2[(result_2['filename'] == filename)].index)\n","      print('File : {} eliminato'.format(filename))\n","      i = i + 1             \n","print('File Eliminati : {} '.format(i))\n","\n","train_balance_df_GAN = result_2\n","\n","'''verify distibution of classes in the sub-sets and calculate weights of the classes in each sub-set'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","vals, counts = np.unique(train_balance_df_GAN['class'], return_counts=True)\n","print(\"conta del numero di immagini per speicfica classe in set Train\")\n","print(len(train_balance_df_GAN))\n","for i in range(0,len(classi)):\n","    print('{}:{}'.format(classi[i], counts[i]))"]},{"cell_type":"markdown","metadata":{"id":"KhieXzJbuGDr"},"source":["#### D.A.  Online"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":535,"status":"aborted","timestamp":1656671142647,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"HU9hZQ12uFFc"},"outputs":[],"source":["'''\n","\n","#CREO SOLO IL DATASET OFFLINE, CON I NOMI DEI FILE CHE DEVE GENERARE, POI IL Custom_Dataset effettua le transform direttamente quando gli servono e quindi fa augmentation online\n","\n","#NOTA IMPORTANTE SUL AUGMENTATION ONLINE! : praticamente non fa vero augmentation raddoppiando le immagini, ma applica solo le trasformazioni usate quando si attivano nel custom dataset e quelle vengono usate come immagini di train, quindi bisognerebbe cercare un modo per aumentarle\n","# https://stackoverflow.com/questions/51677788/data-augmentation-in-pytorch\n","\n","\n","\n","#print train balance df \n","#print(train_balance_df)\n","\n","#nuovo df con le trasformazioni (DA CREARE)\n","data = train_balance_df\n","train_balance_df_copy = pd.DataFrame(data, columns=['ID', 'series', 'filename', 'class', 'transform'])\n","train_balance_df_copy['transform'] = 'T0'\n","#print DF copy (BEFORE)\n","#print(f'train_balance_df_copy\\n', train_balance_df_copy)\n","\n","# N° IMG per ciascuna classe\n","n_img_for_class = np.zeros(10)\n","for i in range(10):\n","  n_img_for_class[i] = len(train_balance_df[train_balance_df['class']==i])\n","print(f'n° immagini per ciascuna classe ', n_img_for_class) #-- il n° di img per ciascuna classe \n","\n","# Classe maggioritaria e minoritaria \n","classe_maggioritaria = np.max(n_img_for_class)\n","classe_minoritaria = np.min(n_img_for_class)\n","print(f'La classe maggioritaria contiene n: ',classe_maggioritaria,'immagini')\n","print(f'La classe minoritaria contiene n: ',classe_minoritaria,'immagini')\n","# indice classe maggioritaria e minoritaria \n","index_min = np.argmin(n_img_for_class)\n","index_max = np.argmax(n_img_for_class)\n","print(f'classe minima : ',index_min)\n","print(f'classe massima : ',index_max)\n","\n","# calcolo n° immagini da creare per ciascuna classe di qualità - non lo usiamo direttamente\n","n_img_for_class_to_create = np.zeros(10)\n","for i in range(10):\n","  n_img_for_class_to_create[i] = classe_maggioritaria - n_img_for_class[i] \n","print(f'n° immagini da creare per ciascuna classe ', n_img_for_class_to_create) #-- il n° di img per ciascuna classe \n","\n","\n","#Data Augmentation Online - genero nuovo dataset con colonna di trasformazioni da usare\n","for k in range(10): \n","  #print(k)\n","  actual_df = train_balance_df[train_balance_df['class']==k]\n","  #print(actual_df)\n","  if n_img_for_class_to_create[k] > 0:\n","    print(f'devo creare immagini della classe : ',k)\n","    n_img_in_actual_df = len(actual_df)\n","    print(f'len actual df', n_img_in_actual_df)\n","    count = n_img_in_actual_df\n","    #for row in range(int(n_img_in_actual_df)):\n","    for i in range(int(abs(classe_maggioritaria/classe_minoritaria))): \n","      for row in range(int(n_img_in_actual_df)):\n","        if (count < classe_maggioritaria):\n","          count = count + 1\n","          row_selected = actual_df.iloc[row]\n","          #parametri nuove img\n","          filename_ = row_selected['filename']\n","          class_ = row_selected['class']\n","          series_ = row_selected['series']\n","          ID_ = row_selected['ID']\n","          #creo un nuovo DF con l'id di transforms \n","          if i==0: \n","            row = {'ID': ID_, 'series': series_, 'filename': filename_, 'class': class_ , 'transform':'T1'}\n","            #print(row)\n","            train_balance_df_copy = train_balance_df_copy.append(row, ignore_index = True)   \n","          if i==1: \n","            row = {'ID': ID_, 'series': series_, 'filename': filename_, 'class': class_ , 'transform':'T2'}\n","            #print(row)\n","            train_balance_df_copy = train_balance_df_copy.append(row, ignore_index = True)   \n","          if i==2: \n","            row = {'ID': ID_, 'series': series_, 'filename': filename_, 'class': class_ , 'transform':'T3'}\n","            #print(row)\n","            train_balance_df_copy = train_balance_df_copy.append(row, ignore_index = True)    \n","        else: \n","          break\n","  else:\n","    print(f'non devo creare immagini della classe : ',k)\n","\n","#check di verifica \n","#print(f'class_weight train_balance_df',compute_sample_weight(class_weight='balanced', y=train_balance_df['class']))\n","#print(f'class_weight train_balance_df_copy',compute_sample_weight(class_weight='balanced', y=train_balance_df_copy['class']))\n","\n","n_img_for_class_after = np.zeros(10)\n","for i in range(10):\n","  n_img_for_class_after[i] = len(train_balance_df_copy[train_balance_df_copy['class']==i])\n","print(f'n° immagini per ciascuna classe ', n_img_for_class_after) #-- il n° di img per ciascuna classe \n","\n","\n","##print DF copy (AFTER)\n","#print(f'train_balance_df_copy\\n', train_balance_df_copy)\n","\n","train_c_dataset = CustomDataset(train_balance_df_copy, transform_0=_transform_, transform_1 = _transform_1, transform_2 = _transform_2, transform_3 = _transform_3, mode = 'train' )\n","val_c_dataset = CustomDataset(val_balance_df, transform_0=_transform_, transform_1 = None, transform_2 = None, transform_3 = None, mode = None)\n","test_c_dataset = CustomDataset(test_balance_df,transform_0=_transform_, transform_1 = None, transform_2 = None, transform_3 = None,  mode = None)\n","\n","'''"]},{"cell_type":"code","source":["print(train_balance_df_copy)"],"metadata":{"id":"UewBx4BM9fmQ","executionInfo":{"status":"aborted","timestamp":1656671142648,"user_tz":-120,"elapsed":536,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lGEjbyRmMQNm"},"source":["###PRE-PROCESSING FOR GAN USAGE"]},{"cell_type":"markdown","metadata":{"id":"8nObcOneuOOT"},"source":["#### SETS EXTRACTION FOR DATA AUGEMNTATION WITH GANs\n","\n","NOTA = per rimuovere Data Augmentation, commentare tutto il blocco e utilizzare direttamente il set 'train_balance_df'"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":536,"status":"aborted","timestamp":1656671142649,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"y2kXjyROQRan"},"outputs":[],"source":["#print(train_balance_df)\n","#print(len(train_balance_df))\n","\n","\n","#Estraggo solo le immagini di un solo lato perchè altrimenti geometrie sono opposte - SE USIAMO LE PATCH NON ABBIAMO BISOGNO DI SPEZZARE IL DATASET PERCHÈ LA GEOMETRIA NON C'È PIÙ\n","#-----PER IMMAGINI INTERE\n","#train_balance_df_lato_1 = train_balance_df[::2]\n","\n","#-----PER IMMAGINI PATCH PRENDO TUTTO IL DATASET CON CLASSI DI QUALITÀ\n","train_balance_df_lato_1 = train_balance_df\n","'''\n","print(len(train_balance_df_lato_1))\n","print(train_balance_df_lato_1) #Prendo solo quelle da un lato, quindi o solo le pari o solo le dispari\n","'''\n","\n","df_class_0 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 0]\n","print(df_class_0)\n","print(len(df_class_0))\n","\n","print(\"---------------------------------------------------\")\n","df_class_9 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 9]\n","print(df_class_9)\n","print(len(df_class_9))\n","\n","print(\"---------------------------------------------------\")\n","df_class_8 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 8]\n","print(df_class_8)\n","print(len(df_class_8))\n","\n","print(\"---------------------------------------------------\")\n","df_class_7 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 7]\n","print(df_class_7)\n","print(len(df_class_7))\n","\n","print(\"---------------------------------------------------\")\n","df_class_6 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 6]\n","print(df_class_6)\n","print(len(df_class_6))\n","\n","print(\"---------------------------------------------------\")\n","df_class_5 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 5]\n","print(df_class_5)\n","print(len(df_class_5))\n","\n","print(\"---------------------------------------------------\")\n","df_class_4 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 4]\n","print(df_class_4)\n","print(len(df_class_4))\n","\n","print(\"---------------------------------------------------\")\n","df_class_3 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 3]\n","print(df_class_3)\n","print(len(df_class_3))\n","\n","print(\"---------------------------------------------------\")\n","df_class_2 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 2]\n","print(df_class_2)\n","print(len(df_class_2))\n","\n","print(\"---------------------------------------------------\")\n","df_class_1 = train_balance_df_lato_1.loc[train_balance_df_lato_1['class'] == 1]\n","print(df_class_1)\n","print(len(df_class_1))\n"]},{"cell_type":"markdown","metadata":{"id":"jn-ni_HDvT8-"},"source":["##### Custom Dataset 2\n","\n","(da utilizzare solo per salvare le patch delle immagini in ciascuna cartella, eventualmente dopo aver applicato delle trasformazioni ma che al momento non si fanno)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":537,"status":"aborted","timestamp":1656671142651,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"xoafbS7QvTPJ"},"outputs":[],"source":["#Custom dataset to SAVE PATCHES\n","class CustomDataset2(Dataset):\n","  def __init__ (self, dataframe, transform_0=None, transform_1 = None, transform_2 = None, transform_3 = None, weight=None, mode = None):\n","    self.transform_0 = transform_0\n","    self.transform_1 = transform_1\n","    self.transform_2 = transform_2\n","    self.transform_3 = transform_3\n","    self.mode = mode\n","    self.weight = weight\n","    self.dataframe = dataframe\n","  def __len__(self):\n","    return len(self.dataframe)\n","  \n","  #ho dovuto aggiungerlo perché l'imbalance dataset sampler lo chiedeva - per il SAMPLER di molte versioni di codice fà\n","  def get_labels(self):\n","    print(self.dataframe['class'])\n","    return self.dataframe['class']\n","\n","  def __getitem__(self, index):\n","    path = self.dataframe.iloc[index, 2]\n","    img_path = os.path.join(path_images+path)\n","    image = io.imread(img_path)\n","    y_label_class = torch.tensor(int(self.dataframe.iloc[index, 3]))              \n","    y_label_series = torch.tensor(int(self.dataframe.iloc[index, 1]))\n","\n","    if self.mode == 'train':\n","      if self.dataframe.iloc[index, 4] == 'T1': \n","        #print('transform T1')\n","        image = self.transform_1(image)\n","      if self.dataframe.iloc[index, 4] == 'T2':\n","        #print('transform T2')\n","        image = self.transform_2(image)\n","      if self.dataframe.iloc[index, 4] == 'T3':\n","        #print('transform T3')\n","        image = self.transform_3(image)\n","\n","    if self.transform_0: \n","      #print('transform generale')\n","      image = self.transform_0(image)\n","      #save_image(image, \"patches/class{0}.0/patch_{1}\".format(y_label_class,path), normalize=True, cmap='gray')    #sbloccare quando serve di salvare le patch\n","\n","      #ALTERNATIVA DI SALVATAGGIO IMMAGINE\n","      #torch.save(image, 'data_drive_path{}'.format(idx))\n","    \n","    return (image, y_label_class, y_label_series)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":537,"status":"aborted","timestamp":1656671142652,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"_uElwqndtHlX"},"outputs":[],"source":["'''ricavare il dataset di immagini per una specifica classe di qualità'''\n","\n","#SAVE PATCHES EXTRACTED FROM IMAGES\n","#cartelle in cui salvare i patches : /drive/CALCIO_CROP_BASE/patches/...\n","os.makedirs(\"patches/class0.0\", exist_ok=True)\n","os.makedirs(\"patches/class9.0\", exist_ok=True)\n","os.makedirs(\"patches/class8.0\", exist_ok=True)\n","os.makedirs(\"patches/class7.0\", exist_ok=True)\n","os.makedirs(\"patches/class6.0\", exist_ok=True)\n","os.makedirs(\"patches/class5.0\", exist_ok=True)\n","os.makedirs(\"patches/class4.0\", exist_ok=True)\n","os.makedirs(\"patches/class3.0\", exist_ok=True)\n","os.makedirs(\"patches/class2.0\", exist_ok=True)\n","os.makedirs(\"patches/class1.0\", exist_ok=True)\n","\n","#------------------------------------------------------MODIFICARE SEMPRE DA QUA SOTTO---------------------------------------------------\n","#---------------------------------------------------------------------------------------------------------------------------------------\n","\n","#IN QUESTA RIGA SOTTO UTILIZZARE COME PRIMO PARAMETRO IL DATASET CHE SI VUOLE USARE PER ESTRARRE IL PATCH\n","train_c_dataset_patches = CustomDataset2(df_class_9, transform_0=_transform_GAN)                                                  #MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","trainloader_patches = torch.utils.data.DataLoader(dataset = train_c_dataset_patches, batch_size=bs, shuffle=True)#,num_workers=2\n","\n","#ITERO GLI ELEMENTI DEL DATALOADER PER RICAVARE I PATCH\n","for i, (data, targets, targets2) in tqdm(enumerate(trainloader_patches)):\n","  print(i)\n","\n","\n","#CREO IL NUOVO DATASET DA USARE COI PATCH PER LA GAN, SENZA MODIFICARE IL PRECEDENTE\n","mask_filenames = []\n","IDs = []\n","classes = []\n","for index, row in df_class_9.iterrows():                                                                                          #MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","    filename = row['filename']\n","    mask_filenames.append(str(\"patches/class{0}/patch_{1}\".format(row['class'],filename)))\n","\n","    IDs.append(row['ID'])\n","    classes.append(row['class'])\n","\n","print(\"DATAFRAME CON PATCHES\")\n","df_class_new = df_class_9.copy()                                                                                                  #MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","df_class_new.drop('filename', axis='columns', inplace=True)   \n","df_class_new['filename'] = mask_filenames\n","\n","column_names = [\"ID\",\"series\", \"filename\", \"class\"]\n","df_class_new = df_class_new.reindex(columns=column_names)\n","print(df_class_new)\n","\n","\n","#ATTIVA SOLO SE I RISULTATI CON TROPPE IMMAGINI PORTANO A GENERAZIONI NON BUONE, COSì GLI SI DANNO MENO IMMAGINI O SVALVOLA\n","#df_class_new = df_class_new[40:71]\n","df_class_new = df_class_new.sample(n = 62, random_state = 4)                                                                      #MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","print(df_class_new)\n","print(len(df_class_new))\n"]},{"cell_type":"markdown","metadata":{"id":"jJ_HZL4CX0sf"},"source":["### Esempio di Posterize & Normalize"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":537,"status":"aborted","timestamp":1656671142653,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"Vh5tn-BwedHc"},"outputs":[],"source":["img_path = os.path.join('/content/drive/MyDrive/CALCIO_CROP_BASE/20200506124834.png') \n","image = io.imread(img_path)\n","io.imshow(image)\n","plt.show()\n","\n","img_path2 = os.path.join('/content/drive/MyDrive/CALCIO_CROP_BASE/T3_20200506124834.png') \n","image2 = io.imread(img_path2)\n","io.imshow(image2)\n","plt.show()\n","\n","img_path3 = os.path.join('/content/drive/MyDrive/CALCIO_CROP_BASE/T2_20200506124834.png') \n","image3 = io.imread(img_path3)\n","io.imshow(image3)\n","plt.show()\n","\n","\n","img_path4 = os.path.join('/content/drive/MyDrive/CALCIO_CROP_BASE/T1_20200506124834.png') \n","image4 = io.imread(img_path4)\n","io.imshow(image4)\n","plt.show()\n","#image = _transform_(image)\n","#plt.imshow(image.numpy()[0], cmap='rgb')\n","\n","\n","#esempio NORMALIZE\n","image5 = io.imread(img_path2)\n","image5 = _transform_(image5)\n","image6 = io.imread(img_path2)\n","image6 = _transform_(image6)\n","\n","print(image5)\n","print(image6)\n","print(image6.shape)\n","print(torch.max(image5[0]))\n","print(torch.max(image6[0]))\n","print(torch.max(image5[1]))\n","print(torch.max(image6[1]))\n","print(torch.max(image5[2]))\n","print(torch.max(image6[2]))\n","print(image5[0])"]},{"cell_type":"markdown","metadata":{"id":"9rWuwr7bXGLI"},"source":["## SAMPLER (usato attualmente)"]},{"cell_type":"markdown","metadata":{"id":"8ohcNPBHMrq-"},"source":["#### Classe BalancedBatchSampler \n","\n","SERVE A CREARE IL BATCH RAPPRESENTATIVO"]},{"cell_type":"code","execution_count":65,"metadata":{"executionInfo":{"elapsed":311,"status":"ok","timestamp":1656671394704,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"PGtHdrBHwQbO"},"outputs":[],"source":["import torch\n","is_torchvision_installed = True\n","try:\n","    import torchvision\n","except:\n","    is_torchvision_installed = False\n","import torch.utils.data\n","import random\n","\n","class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n","    def __init__(self, dataset, labels=None):\n","        self.labels = labels\n","        self.dataset = dict()\n","        self.balanced_max = 0\n","\n","        #prova denis \n","        self.number_of_samples = 0\n","\n","        # Save all the indices for all the classes\n","        for idx in range(0, len(dataset)):\n","            label = self._get_label(dataset, idx)\n","            if label not in self.dataset:\n","                self.dataset[label] = list()\n","            self.dataset[label].append(idx)\n","            self.balanced_max = len(self.dataset[label]) \\\n","                if len(self.dataset[label]) > self.balanced_max else self.balanced_max\n","        \n","        # Oversample the classes with fewer elements than the max\n","        for label in self.dataset:\n","            while len(self.dataset[label]) < self.balanced_max:\n","                self.dataset[label].append(random.choice(self.dataset[label]))\n","        self.keys = list(self.dataset.keys())\n","        self.currentkey = 0\n","        self.indices = [-1]*len(self.keys)\n","        self.number_of_samples = self.balanced_max * len(self.dataset)\n","        \n","    def __iter__(self):\n","        while self.indices[self.currentkey] < self.balanced_max - 1:\n","            self.indices[self.currentkey] += 1\n","            yield self.dataset[self.keys[self.currentkey]][self.indices[self.currentkey]]\n","            self.currentkey = (self.currentkey + 1) % len(self.keys)\n","        self.indices = [-1]*len(self.keys)\n","    \n","    def _get_label(self, dataset, idx, labels = None):\n","        if self.labels is not None:\n","            return self.labels[idx].item()\n","        else:\n","            # Trying guessing\n","            dataset_type = type(dataset)\n","            if is_torchvision_installed and dataset_type is torchvision.datasets.MNIST:\n","                return dataset.train_labels[idx].item()\n","            elif is_torchvision_installed and dataset_type is torchvision.datasets.ImageFolder:\n","                return dataset.imgs[idx][1]\n","            else:\n","                raise Exception(\"You should pass the tensor of labels to the constructor as second argument\")\n","\n","    def __len__(self):\n","        return self.balanced_max*len(self.keys)\n"]},{"cell_type":"markdown","metadata":{"id":"51c5KglTMxCe"},"source":["#### WeightedRandomSampler (TEST) \n","\n","You are correct regarding the transformation. The transformation will be applied on the fly on your minority class data. You are also correct regarding the WeightedRandomSampler, if you are keeping the default replacement=True argument.\n","\n","https://discuss.pytorch.org/t/how-to-augment-the-minority-class-only-in-an-unbalanced-dataset/13797/2"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1656671395025,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"27g-SJndM0UD","outputId":"3e5f1deb-2a8a-4ef6-fa0f-c333e1a1178f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nclass_sample_counts = train_balance_df.groupby(by=\"series\").count()\\nclass_sample_counts = class_sample_counts[\\'ID\\'].to_numpy()\\nprint(class_sample_counts)\\nclass_weights = torch.tensor(compute_sample_weight(class_weight=\\'balanced\\', y=train_balance_df[\\'series\\'])) #--- use of compute_sample_weight from sklearn \\nprint(class_weights)\\n\\nsampler_ = torch.utils.data.sampler.WeightedRandomSampler(\\n    weights=class_weights,\\n    num_samples=len(class_weights),\\n    replacement=True)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":66}],"source":["'''\n","class_sample_counts = train_balance_df.groupby(by=\"series\").count()\n","class_sample_counts = class_sample_counts['ID'].to_numpy()\n","print(class_sample_counts)\n","class_weights = torch.tensor(compute_sample_weight(class_weight='balanced', y=train_balance_df['series'])) #--- use of compute_sample_weight from sklearn \n","print(class_weights)\n","\n","sampler_ = torch.utils.data.sampler.WeightedRandomSampler(\n","    weights=class_weights,\n","    num_samples=len(class_weights),\n","    replacement=True)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"oTOxIN2g2Jcv"},"source":["#### Imbalanced Dataset Sampler \n","(LINK: https://github.com/ufoym/imbalanced-dataset-sampler) "]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3697,"status":"ok","timestamp":1656671398712,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"YiTVi6842Bi7","outputId":"afbdef46-42de-4436-dfe0-9abb76f3604e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchsampler in /usr/local/lib/python3.7/dist-packages (0.1.2)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from torchsampler) (1.11.0+cu113)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from torchsampler) (4.11.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsampler) (1.3.5)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.7/dist-packages (from torchsampler) (0.12.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->torchsampler) (4.1.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->torchsampler) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->torchsampler) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->torchsampler) (1.21.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->torchsampler) (3.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsampler) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsampler) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsampler) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5->torchsampler) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5->torchsampler) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5->torchsampler) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5->torchsampler) (3.0.4)\n"]}],"source":["#!pip install https://github.com/ufoym/imbalanced-dataset-sampler/archive/master.zip --quiet --ignore-installed #--- qui va in errore, serviva per creare il batch rappresentativo\n","!pip install torchsampler\n","from torchsampler import ImbalancedDatasetSampler\n","\n","#sampler_imbalance = ImbalancedDatasetSampler(train_c_dataset)"]},{"cell_type":"markdown","metadata":{"id":"-gwtHXxECiTK"},"source":["## Creazione DataLoader TRAIN, VAL, TEST"]},{"cell_type":"code","execution_count":68,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1656671398714,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"ty1I-hVWryqq"},"outputs":[],"source":["#QUA SOTTO SI TENGONO DA PARTE TUTTI I SET ESTRATTI VOLTA PER VOLTA SE DOVESSERO RISERVIRE PER TESTS\n","\n","#---------NO DATA AUGMENTATION\n","#train_c_dataset = CustomDataset(train_balance_df, transform_0=_transform_)\n"," \n","\n","#---------PER GAN\n","#train_c_dataset = CustomDataset(df_class_new, transform_0=_transform_GAN)  \n","#val_c_dataset = CustomDataset(val_balance_df, transform_0=_transform_)\n","#test_c_dataset = CustomDataset(test_balance_df,transform_0=_transform_ )\n","\n","#---------PER CLASSIFICATORE CON IMMAGINI AUGMENTATION GAN OFFLINE\n","#train_c_dataset = CustomDataset(train_balance_df_GAN, transform_0=_transform_train) \n","#val_c_dataset = CustomDataset(val_balance_df, transform_0=_transform_)\n","#test_c_dataset = CustomDataset(test_balance_df,transform_0=_transform_ )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":539,"status":"aborted","timestamp":1656671142659,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"c7g-y1mkd_YM"},"outputs":[],"source":["# creating tensor from targets_df \n","torch_tensor = torch.tensor(train_balance_df_GAN['class'].values)\n","\n","# printing out result\n","print(torch_tensor)\n","print(torch_tensor.shape)\n","\n","#HO RICAVATO LE LABELS DEGLI ELEMENTI IN TENSOREDA PASSARE AL BALANCED BATCH SAMPLER"]},{"cell_type":"code","execution_count":69,"metadata":{"executionInfo":{"elapsed":388,"status":"ok","timestamp":1656671401072,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"Un908UAc2bmH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"42d309e7-702a-4554-bd37-152c5e257090"},"outputs":[{"output_type":"stream","name":"stdout","text":["0       7.0\n","1       8.0\n","2       6.0\n","3       6.0\n","4       8.0\n","       ... \n","2055    9.0\n","2056    9.0\n","2057    9.0\n","2058    9.0\n","2059    9.0\n","Name: class, Length: 2060, dtype: float64\n"]}],"source":["\n","#QUESTA PARTE POTREMO DOVERE USARLA SE IMPLEMENTEREMO UN SAMPLER CHE ABBIAMO SOPRA PER PESCARE DAL DATASET IMMAGINI IN MODO SPECIFICO\n","\n","\n","\n","'''#BALANCED DATA SAMPLER\n","trainloader = torch.utils.data.DataLoader(\n","    train_c_dataset,\n","    sampler=BalancedBatchSampler(train_c_dataset, torch_tensor),\n","    batch_size=bs\n",")\n","'''\n","\n","#IMBALANCED DATA SAMPLER\n","trainloader = torch.utils.data.DataLoader(\n","    train_c_dataset,\n","    sampler=ImbalancedDatasetSampler(train_c_dataset),\n","    batch_size=bs\n",")\n","\n","\n","\n","#trainloader senza BALANCED Batch Sampler / iMBALANCED\n","#trainloader = torch.utils.data.DataLoader(dataset = train_c_dataset, batch_size=64, shuffle=True)#,num_workers=2\n","\n","valloader = torch.utils.data.DataLoader(dataset = val_c_dataset, batch_size=bs, shuffle=True)#,num_workers=2\n","testloader = torch.utils.data.DataLoader(dataset = test_c_dataset, batch_size=bs, shuffle=True)#,num_workers=2"]},{"cell_type":"markdown","metadata":{"id":"oow7OM0obIF4"},"source":["##GAN EVALUATION METRICS - IMAGE QUALITY "]},{"cell_type":"markdown","metadata":{"id":"DcsBnqNhd10V"},"source":["Quello che si potrbbe fare per avere un valore di riferimento da controllare con le metriche è calcolare la metrica stessa solo su immagini reali così da avere un valore di qualità di riferimento, così come quando applicheremo sotto le metriche per la diversità, da avere anche un rifeirmento sul quanto vengano univoche e quindi di valore .\n","\n","\n","PYTORCH LIBRARY GAN METRICS (FID & IS)\n","https://pypi.org/project/pytorch-gan-metrics/"]},{"cell_type":"markdown","metadata":{"id":"zbbX2PVYy2q5"},"source":["#### FID \n","[GITHUB](https://github.com/hukkelas/pytorch-frechet-inception-distance)\n","\n","- Il valore deve essere il PIÙ BASSO possibile\n","- è un miglioramento dell'IS quindi valuta se sotituire IS con un'altra metrica"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":205,"status":"aborted","timestamp":1656671142664,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"kbW-8G0fxDh6"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","from custom_FID import load_images, calculate_fid\n"]},{"cell_type":"markdown","metadata":{"id":"BAc0bXoskKkK"},"source":["#### IS - Inception Score \n","[GITHUB](https://github.com/sbarratt/inception-score-pytorch/blob/master/inception_score.py)\n","\n","- il valore deve essere PIÙ ALTO possibile\n","- serve il CLASSIFICATORE per valutare quante immagini riconosce bene, solo sul set di immagini create\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":206,"status":"aborted","timestamp":1656671142666,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"1N3IGC9DycxK"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","import custom_IS \n","from custom_IS import inception_score"]},{"cell_type":"markdown","metadata":{"id":"lXgnkNAZmc0z"},"source":["####MMD - Maximum Mean Discrepancy \n","\n","maximum_mean_discrepancy\n","[LINK](https://www.kaggle.com/code/onurtunali/maximum-mean-discrepancy/notebook)\n","\n","- il valore deve essere PIÙ BASSO possibile, perchè indica similarità dalle imaggini RAW, ma troppo basso vorrebbe anche dire che potrebbe generarle troppo simili alle originali"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":206,"status":"aborted","timestamp":1656671142667,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"u8_qXyVJzFj2"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","#import custom_MMD\n","#from custom_MMD import MMD\n","from custom_MMD import custom_MMD as MMD"]},{"cell_type":"markdown","metadata":{"id":"pMvexlvYe5xD"},"source":["##GAN EVALUATION METRICS - IMAGE DIVERSITY\n","\n","(se fosse la metrica migliore con quelle generate significherebbe che la GAN genera immagini significative, e quindi limiterebbe anche overfitting in partenza)"]},{"cell_type":"markdown","metadata":{"id":"MG_zlz7PfzZC"},"source":["####NDB -\n","\n","- Per valautare la diversità delle immagini e se c'è MODE COLLAPSE\n","\n","- per avere un risultato buono, l'andamento deve abbassarsi durante l'allenamento e comunque tendere a 0 il pià possibile"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":208,"status":"aborted","timestamp":1656671142669,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"kcusRj12f5K_"},"outputs":[],"source":["'''IMPORTS NEEDED FOR THE NDB IMPLEMENTATION'''\n","\n","#https://github.com/yhlleo/GAN-Metrics/blob/master/scores/ndb_jsd.py - VERSIONE CHE NON MI CONVINCE \n","\n","#https://colab.research.google.com/drive/1fGrFl5UzYc3upShr25Hv8VfqyzhZOPTM?usp=sharing\n","\n","#https://wandb.ai/authors/DCGAN-ndb-test/reports/Draft--VmlldzoxNDcyMjc - INFO\n","\n","from __future__ import print_function\n","#%matplotlib inline\n","import argparse\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","import time\n","import gc\n","\n","# Set random seed for reproducibility\n","manualSeed = 999\n","#manualSeed = random.randint(1, 10000) # use if you want new results\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","np.random.seed(999)\n","\n","#Install + import Weights and Biases\n","!pip install --upgrade wandb\n","import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":208,"status":"aborted","timestamp":1656671142670,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"B_ICp_tG3ymk"},"outputs":[],"source":["'''CLASS DEFINITION NDB'''\n","\n","import os\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from scipy.stats import norm\n","from matplotlib import pyplot as plt\n","import pickle as pkl\n","\n","\n","'''The NDB score is one way to measure the effects of mode collapse quantitatively.\n","\n","We show that logging the score during training can be a good way to detect mode collapse if we don't see a substantial decrease in the score over time. \n","The key takeaway is that it's useful to have another quantitative metric to measure such an important issue such as mode collapse instead of purely relying on visual methods or IS/FID scores\n","'''\n","\n","\n","class NDB:\n","    def __init__(self, training_data=None, number_of_bins=100, significance_level=0.05, z_threshold=None,\n","                 whitening=False, max_dims=None, cache_folder=None):\n","        \"\"\"\n","        NDB Evaluation Class\n","        :param training_data: Optional - the training samples - array of m x d floats (m samples of dimension d)\n","        :param number_of_bins: Number of bins (clusters) default=100\n","        :param significance_level: The statistical significance level for the two-sample test\n","        :param z_threshold: Allow defining a threshold in terms of difference/SE for defining a bin as statistically different\n","        :param whitening: Perform data whitening - subtract mean and divide by per-dimension std\n","        :param max_dims: Max dimensions to use in K-means. By default derived automatically from d\n","        :param bins_file: Optional - file to write / read-from the clusters (to avoid re-calculation)\n","        \"\"\"\n","        \n","        #Added\n","        self.original_bin_centers = None\n","        self.count = None\n","        self.bin_order = None\n","        \n","        #Original\n","        self.number_of_bins = number_of_bins\n","        self.significance_level = significance_level\n","        self.z_threshold = z_threshold\n","        self.whitening = whitening\n","        self.ndb_eps = 1e-6\n","        self.training_mean = 0.0\n","        self.training_std = 1.0\n","        self.max_dims = max_dims\n","        self.cache_folder = cache_folder\n","        self.bin_centers = None\n","        self.bin_proportions = None\n","        self.ref_sample_size = None\n","        self.used_d_indices = None\n","        self.results_file = None\n","        self.test_name = 'ndb_{}_bins_{}'.format(self.number_of_bins, 'whiten' if self.whitening else 'orig')\n","        self.cached_results = {}\n","        if self.cache_folder:\n","            self.results_file = os.path.join(cache_folder, self.test_name+'_results.pkl')\n","            if os.path.isfile(self.results_file):\n","                # print('Loading previous results from', self.results_file, ':')\n","                self.cached_results = pkl.load(open(self.results_file, 'rb'))\n","                # print(self.cached_results.keys())\n","        if training_data is not None or cache_folder is not None:\n","                bins_file = None\n","                if cache_folder:\n","                    os.makedirs(cache_folder, exist_ok=True)\n","                    bins_file = os.path.join(cache_folder, self.test_name+'.pkl')\n","                self.construct_bins(training_data, bins_file)\n","\n","    def construct_bins(self, training_samples, bins_file):\n","        \"\"\"\n","        Performs K-means clustering of the training samples\n","        :param training_samples: An array of m x d floats (m samples of dimension d)\n","        \"\"\"\n","\n","        if self.__read_from_bins_file(bins_file):\n","            return\n","        n, d = training_samples.shape\n","        k = self.number_of_bins\n","        if self.whitening:\n","            self.training_mean = np.mean(training_samples, axis=0)\n","            self.training_std = np.std(training_samples, axis=0) + self.ndb_eps\n","\n","        if self.max_dims is None and d > 1000:\n","            # To ran faster, perform binning on sampled data dimension (i.e. don't use all channels of all pixels)\n","            self.max_dims = d//6\n","\n","        whitened_samples = (training_samples-self.training_mean)/self.training_std\n","        d_used = d if self.max_dims is None else min(d, self.max_dims)\n","        self.used_d_indices = np.random.choice(d, d_used, replace=False)\n","\n","        #print('Performing K-Means clustering of {} samples in dimension {} / {} to {} clusters ...'.format(n, d_used, d, k))\n","        #print('Can take a couple of minutes...')\n","        if n//k > 1000:\n","            print('Training data size should be ~500 times the number of bins (for reasonable speed and accuracy)')\n","\n","        clusters = KMeans(n_clusters=k, max_iter=100).fit(whitened_samples[:, self.used_d_indices])\n","        #clusters = KMeans(n_clusters=k, max_iter=100, n_jobs=-1).fit(whitened_samples[:, self.used_d_indices])\n","\n","\n","        bin_centers = np.zeros([k, d])\n","        for i in range(k):\n","            bin_centers[i, :] = np.mean(whitened_samples[clusters.labels_ == i, :], axis=0)\n","        \n","        self.original_bin_centers = bin_centers\n","        #print(\"Bin centers: \", bin_centers.shape)\n","        # Organize bins by size (largest bin -> smallest bin)\n","        label_vals, label_counts = np.unique(clusters.labels_, return_counts=True)\n","        self.count = list(zip(label_vals, label_counts))\n","        self.count.sort(key=lambda tup: tup[1], reverse=True)\n","        bin_order = np.argsort(-label_counts)\n","        self.bin_order = bin_order\n","        self.bin_proportions = label_counts[bin_order] / np.sum(label_counts)\n","        self.bin_centers = bin_centers[bin_order, :]\n","        self.ref_sample_size = n\n","        self.__write_to_bins_file(bins_file)\n","        print('Done.')\n","\n","    def evaluate(self, query_samples, model_label=None):\n","        \"\"\"\n","        Assign each sample to the nearest bin center (in L2). Pre-whiten if required. and calculate the NDB\n","        (Number of statistically Different Bins) and JS divergence scores.\n","        :param query_samples: An array of m x d floats (m samples of dimension d)\n","        :param model_label: optional label string for the evaluated model, allows plotting results of multiple models\n","        :return: results dictionary containing NDB and JS scores and array of labels (assigned bin for each query sample)\n","        \"\"\"\n","        n = query_samples.shape[0]\n","        query_bin_proportions, query_bin_assignments = self.__calculate_bin_proportions(query_samples)\n","        # print(query_bin_proportions)\n","        different_bins = NDB.two_proportions_z_test(self.bin_proportions, self.ref_sample_size, query_bin_proportions,\n","                                                    n, significance_level=self.significance_level,\n","                                                    z_threshold=self.z_threshold)\n","        ndb = np.count_nonzero(different_bins)\n","        js = NDB.jensen_shannon_divergence(self.bin_proportions, query_bin_proportions)\n","        results = {'NDB': ndb,\n","                   'JS': js,\n","                   'Proportions': query_bin_proportions,\n","                   'N': n,\n","                   'Bin-Assignment': query_bin_assignments,\n","                   'Different-Bins': different_bins}\n","\n","        if model_label:\n","            #print('Results for {} samples from {}: '.format(n, model_label), end='')\n","            self.cached_results[model_label] = results\n","            if self.results_file:\n","                # print('Storing result to', self.results_file)\n","                pkl.dump(self.cached_results, open(self.results_file, 'wb'))\n","\n","        #print('NDB =', ndb, 'NDB/K =', ndb/self.number_of_bins, ', JS =', js)\n","        return results\n","\n","    def print_results(self):\n","        print('NSB results (K={}{}):'.format(self.number_of_bins, ', data whitening' if self.whitening else ''))\n","        for model in sorted(list(self.cached_results.keys())):\n","            res = self.cached_results[model]\n","            print('%s: NDB = %d, NDB/K = %.3f, JS = %.4f' % (model, res['NDB'], res['NDB']/self.number_of_bins, res['JS']))\n","\n","    def plot_results(self, models_to_plot=None):\n","        \"\"\"\n","        Plot the binning proportions of different methods\n","        :param models_to_plot: optional list of model labels to plot\n","        \"\"\"\n","        K = self.number_of_bins\n","        w = 1.0 / (len(self.cached_results)+1)\n","        assert K == self.bin_proportions.size\n","        assert self.cached_results\n","\n","        # Used for plotting only\n","        def calc_se(p1, n1, p2, n2):\n","            p = (p1 * n1 + p2 * n2) / (n1 + n2)\n","            return np.sqrt(p * (1 - p) * (1/n1 + 1/n2))\n","\n","        if not models_to_plot:\n","            models_to_plot = sorted(list(self.cached_results.keys()))\n","\n","        # Visualize the standard errors using the train proportions and size and query sample size\n","        train_se = calc_se(self.bin_proportions, self.ref_sample_size,\n","                           self.bin_proportions, self.cached_results[models_to_plot[0]]['N'])\n","        plt.bar(np.arange(0, K)+0.5, height=train_se*2.0, bottom=self.bin_proportions-train_se,\n","                width=1.0, label='Train$\\pm$SE', color='gray')\n","\n","        ymax = 0.0\n","        for i, model in enumerate(models_to_plot):\n","            results = self.cached_results[model]\n","            label = '%s (%i : %.4f)' % (model, results['NDB'], results['JS'])\n","            ymax = max(ymax, np.max(results['Proportions']))\n","            if K <= 70:\n","                plt.bar(np.arange(0, K)+(i+1.0)*w, results['Proportions'], width=w, label=label)\n","            else:\n","                plt.plot(np.arange(0, K)+0.5, results['Proportions'], '--*', label=label)\n","        plt.legend(loc='best')\n","        plt.ylim((0.0, min(ymax, np.max(self.bin_proportions)*4.0)))\n","        plt.grid(True)\n","        plt.title('Binning Proportions Evaluation Results for {} bins (NDB : JS)'.format(K))\n","        plt.show()\n","\n","    def __calculate_bin_proportions(self, samples):\n","        if self.bin_centers is None:\n","            print('First run construct_bins on samples from the reference training data')\n","        assert samples.shape[1] == self.bin_centers.shape[1]\n","        n, d = samples.shape\n","        k = self.bin_centers.shape[0]\n","        D = np.zeros([n, k], dtype=samples.dtype)\n","\n","        #print('Calculating bin assignments for {} samples...'.format(n))\n","        whitened_samples = (samples-self.training_mean)/self.training_std\n","        for i in range(k):\n","            print('.', end='', flush=True)\n","            D[:, i] = np.linalg.norm(whitened_samples[:, self.used_d_indices] - self.bin_centers[i, self.used_d_indices],\n","                                     ord=2, axis=1)\n","        print()\n","        labels = np.argmin(D, axis=1)\n","        probs = np.zeros([k])\n","        label_vals, label_counts = np.unique(labels, return_counts=True)\n","        probs[label_vals] = label_counts / n\n","        return probs, labels\n","\n","    def __read_from_bins_file(self, bins_file):\n","        if bins_file and os.path.isfile(bins_file):\n","            print('Loading binning results from', bins_file)\n","            bins_data = pkl.load(open(bins_file,'rb'))\n","            self.bin_proportions = bins_data['proportions']\n","            self.bin_centers = bins_data['centers']\n","            self.ref_sample_size = bins_data['n']\n","            self.training_mean = bins_data['mean']\n","            self.training_std = bins_data['std']\n","            self.used_d_indices = bins_data['d_indices']\n","            return True\n","        return False\n","\n","    def __write_to_bins_file(self, bins_file):\n","        if bins_file:\n","            print('Caching binning results to', bins_file)\n","            bins_data = {'proportions': self.bin_proportions,\n","                         'centers': self.bin_centers,\n","                         'n': self.ref_sample_size,\n","                         'mean': self.training_mean,\n","                         'std': self.training_std,\n","                         'd_indices': self.used_d_indices}\n","            pkl.dump(bins_data, open(bins_file, 'wb'))\n","\n","    @staticmethod\n","    def two_proportions_z_test(p1, n1, p2, n2, significance_level, z_threshold=None):\n","        # Per http://stattrek.com/hypothesis-test/difference-in-proportions.aspx\n","        # See also http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/binotest.htm\n","        p = (p1 * n1 + p2 * n2) / (n1 + n2)\n","        se = np.sqrt(p * (1 - p) * (1/n1 + 1/n2))\n","        z = (p1 - p2) / se\n","        # Allow defining a threshold in terms as Z (difference relative to the SE) rather than in p-values.\n","        if z_threshold is not None:\n","            return abs(z) > z_threshold\n","        p_values = 2.0 * norm.cdf(-1.0 * np.abs(z))    # Two-tailed test\n","        return p_values < significance_level\n","\n","    @staticmethod\n","    def jensen_shannon_divergence(p, q):\n","        \"\"\"\n","        Calculates the symmetric Jensen–Shannon divergence between the two PDFs\n","        \"\"\"\n","        m = (p + q) * 0.5\n","        return 0.5 * (NDB.kl_divergence(p, m) + NDB.kl_divergence(q, m))\n","\n","    @staticmethod\n","    def kl_divergence(p, q):\n","        \"\"\"\n","        The Kullback–Leibler divergence.\n","        Defined only if q != 0 whenever p != 0.\n","        \"\"\"\n","        assert np.all(np.isfinite(p))\n","        assert np.all(np.isfinite(q))\n","        assert not np.any(np.logical_and(p != 0, q == 0))\n","\n","        p_pos = (p > 0)\n","        return np.sum(p[p_pos] * np.log(p[p_pos] / q[p_pos]))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":210,"status":"aborted","timestamp":1656671142672,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"qV6UxgXT6AXI"},"outputs":[],"source":["'''TEST DI NDB SU MATRICI A CASO'''\n","\n","'''\n","def test_ndb():  \n","    dim=100\n","    k=50\n","    n_train = k*100\n","    n_test = k*10\n","\n","    train_samples = np.random.uniform(size=[n_train, dim])\n","    ndb = NDB(training_data=train_samples, number_of_bins=k, whitening=True)\n","\n","    test_samples = np.random.uniform(high=1.0, size=[n_test, dim])\n","    results = ndb.evaluate(test_samples, model_label='Test')\n","    print(results['Bin-Assignment'])\n","    print\n","    print(ndb.bin_order)\n","    print()\n","    print(ndb.count)\n","    print()\n","    unique, counts = np.unique(results[\"Bin-Assignment\"], return_counts=True)\n","    zipped = list(zip(unique, counts))\n","    zipped.sort(key=lambda tup: tup[1], reverse=True)\n","    print(zipped)\n","\n","    \n","test_ndb()\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":211,"status":"aborted","timestamp":1656671142674,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"JZ65bxNmdQ7a"},"outputs":[],"source":["'''DEFINIZIONE PARAMETRI NDB'''\n","\n","#wandb.init(entity=\"authors\", project=\"GAN-ndb-test\")\n","\n","#NBD Scoring\n","\n","numTrainBatches = 1 #156  \n","\n","numTestBatches = 1 #39\n","\n","#number of bins for NBD (number of clusters)\n","k = 5\n","\n","#number of color channels\n","nc = 3 "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":213,"status":"aborted","timestamp":1656671142675,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"SoadtFIpZmDe"},"outputs":[],"source":["'''DEFINIZIONE METODO PER CREARE UN SET DI BATCHES DI IMMAGINI COL 'GENERATOR'  '''\n","\n","def generated_fakes(numTestBatches, netG, image_size1, image_size2, batch_size, lat_space_t=None):\n","\n","    generated_batches = []\n","\n","    '''questo parametro serve per fare le elaborazioni con Optuna, quindi se non si esegue riportiamo lat_space a dimensione fissa'''\n","    if lat_space_t == None:\n","        lat_space_t=100\n","\n","    for i in range(numTestBatches):\n","        '''- MEMORIZZO IL SET DI IMMAGINI GENERATE PER USARLO NELLA METRICA 'NDB' -'''\n","        z = Variable(Tensor(np.random.normal(0, 1, (data.shape[0], lat_space_t)))) \n","        gen_imgs = netG(z)\n","        fake_imgs = gen_imgs.detach().cpu().numpy()\n","        generated_batches.append(fake_imgs)\n","        '''------------------------------------------------------------------------'''\n","\n","    generated_batches = np.array(generated_batches)\n","    # (1, 62, 3, 256, 256)\n","\n","\n","    #Display a sample\n","    #plt.imshow(np.transpose(generated_batches[0][0], (1,2,0)))\n","    \n","    gen_combined = generated_batches.reshape(numTestBatches*batch_size, 3, image_size1, image_size2)     #62 perchè il batch non è completo\n","    # (62, 3, 256, 256)\n","  \n","    del generated_batches\n","    gc.collect()\n","\n","\n","\n","    # gen_combined = generated_batches[0]\n","    # for i in range(1,len(generated_batches)):\n","    #     gen_combined = np.concatenate((gen_combined, generated_batches[i]))\n","    \n","    return gen_combined\n","\n","\n","def real_samples(numTrainBatches, dataloader, nc, image_size1, image_size2, batch_size):\n","    #Get real samples (to reduce training time take 40% of original data - 80000 samples)\n","\n","    '''--------------------parametri per calcolare NDB score-----------------'''\n","    batches_done = 0\n","    real_batches = [] #to save real images to be used with NDB METRIC\n","    generated_batches = [] #to save real images to be used with NDB METRIC\n","    iters = 0\n","    img_list = []\n","    metrics = []\n","    ndb_scores = []\n","    real_batches = []\n","\n","\n","    for i, (data, targets, targets2) in tqdm(enumerate(trainloader)):\n","\n","        if(i >= numTrainBatches):\n","            break\n","\n","        '''- MEMORIZZO IL SET DI IMMAGINI REALI PER USARLO NELLA METRICA 'NDB' -'''\n","        real = data.numpy()\n","        real_batches.append(real)\n","\n","    '''RIADATTAMENTO DELLE IMMAGINI REALI & GENERATE PER METRICA 'NDB' '''\n","    real_batches = np.array(real_batches)\n","\n","    print(len(real_batches))\n","    #Display a sample\n","    #plt.imshow(np.transpose(real_batches[0][0], (1,2,0)))\n","\n","    real_combined = real_batches.reshape(numTrainBatches*batch_size, nc, image_size1, image_size2)                #QUANDO NON TORNERANNO LE MISURE AL POSTO DI 62 CI VA LA DIMENSIONE DEL BATCH\n","    del real_batches\n","    gc.collect()\n","    \n","    return real_combined\n"]},{"cell_type":"markdown","metadata":{"id":"zghXE8FCfueE"},"source":["####SSIM \n","((SSIM) index provides a measure of the similarity by comparing two images based on luminance similarity, contrast similarity and structural similarity information)\n","\n","- il valore deve essere PIÙ BASSO POSSIBILE (indicherebbe più diversità), ma allo stesso tempo, se valore è troppo basso potrebbe voler dire che le immagini non sono di qualità accettabile (rumorose)\n","\n","\n","- modeling any image distortion as a combination of three factors: (loss of correlation, loss of luminance, loss of contrast distortions)\n","\n","- Serve di passargli come argomento l'insieme di immagini reali e un altro insieme di immagini generate (tendere ad avere insiemi di stessa dimensione)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":213,"status":"aborted","timestamp":1656671142677,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"kK5WvQNFOK2m"},"outputs":[],"source":["'''link utili alternativi'''\n","\n","#https://medium.com/srm-mic/all-about-structural-similarity-index-ssim-theory-code-in-pytorch-6551b455541e\n","\n","#https://cvnote.ddlee.cc/2019/09/12/psnr-ssim-python\n","\n","#https://www.programcreek.com/python/?CodeExample=compute+ssim - NON MI CONVINCE\n","\n","\n","\n","\n","#https://torchmetrics.readthedocs.io/en/stable/image/structural_similarity.html\n","from torchmetrics import StructuralSimilarityIndexMeasure     \n","import torch\n","\n","ssim = StructuralSimilarityIndexMeasure()\n"]},{"cell_type":"markdown","metadata":{"id":"hUVliMM4zT4F"},"source":["#### LPIPS (NON RIESCE AD INSTALLARE QUASI MAI LA LIBRERIA DA USARE UNA VOLTA HA FUNZIONATO)\n","\n","- A low LPIPS score means that image patches are perceptual similar"]},{"cell_type":"markdown","metadata":{"id":"DAANUrOJ4HiS"},"source":["##### Ho utilizzato il GITHUB : [GITHUB](https://github.com/richzhang/PerceptualSimilarity#a-basic-usage)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":215,"status":"aborted","timestamp":1656671142679,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"qID_Z-_h1qDQ"},"outputs":[],"source":["#------ BASTA ESEGUIRLO SOLO UNA VOLTA PER SALVARLO, MA è GIA' STATO FATTO \n","'''\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","os.mkdir('GIT-LPIP') #https://github.com/richzhang/PerceptualSimilarity#a-basic-usage\n","!git clone https://github.com/richzhang/PerceptualSimilarity.git GIT-LPIP\n","'''\n","print('trovi tutto nella cartella /content/drive/MyDrive/Colab Notebooks/algoritmi_custom')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":216,"status":"aborted","timestamp":1656671142681,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"F4FDAciJ2gUg"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom/GIT-LPIP')\n","!pip install -r requirements.txt --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":217,"status":"aborted","timestamp":1656671142682,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"ZpRRVwBr3Eov"},"outputs":[],"source":["!python lpips_2imgs.py -p0 /content/drive/MyDrive/CALCIO_CROP_BASE/images/990.png -p1 /content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0/patch_20200506123315.png #--use_gpu"]},{"cell_type":"markdown","metadata":{"id":"TaiR3ZXv4QpX"},"source":["##### Ho utilizzato il GITHUB di [TORCH METRICS ](https://github.com/PyTorchLightning/metrics.git)\n","\n","Continua a dare lo stesso errore \n","\n","```\n","LPIPS metric requires that lpips is installed. Either install as `pip install torchmetrics[image]` or `pip install lpips`.\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":217,"status":"aborted","timestamp":1656671142683,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"MeGmfv55OaLX"},"outputs":[],"source":["'''SE A VOLTE NON FUNZIONA, RIPROVA AD ESEGUIRLO, PERCHÈ SI BLOCCA IL 'PIP INSTALL' '''\n","#https://pypi.org/project/lpips/\n","\n","#https://torchmetrics.readthedocs.io/en/stable/image/learned_perceptual_image_patch_similarity.html\n","\n","#import lpips\n","#!pip install --ignore-installed torchmetrics[image]\n","#!pip install lpips==0.1.4\n","\n","#from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n","#lpips = LearnedPerceptualImagePatchSimilarity_(net_type='vgg')\n","\n","\n","'''ESEMPIO CALCOLO LPIPS'''\n","#_ = torch.manual_seed(123)\n","\n","'''\n","\n","img1 = torch.rand(10, 3, 100, 100)\n","img2 = torch.rand(10, 3, 100, 100)\n","#x = lpips(img1, img2)\n","#print(x)\n","\n","loss_fn = lpips.LPIPS(net='vgg')\n","d = loss_fn.forward(img1,img2)\n","print(d)\n","'''\n"]},{"cell_type":"markdown","metadata":{"id":"JCHj6IT9fnoi"},"source":["#### GEOMETRY SCORE\n","\n","- non capisco perché mi da anche questo sklearn.metrics.fowlkes_mallows_score tra i vari risultati \n","- [PAPER](https://arxiv.org/pdf/1802.02664.pdf)\n","- [LINK](https://github.com/KhrulkovV/geometry-score)\n","\n","((NON FUNZIONA))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":219,"status":"aborted","timestamp":1656671142685,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"Iyp_qwzkDBJX"},"outputs":[],"source":["#------ BASTA ESEGUIRLO SOLO UNA VOLTA PER SALVARLO, MA è GIA' STATO FATTO \n","'''\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom')\n","os.mkdir('GEOM-SCORE') #https://github.com/KhrulkovV/geometry-score.git\n","!git clone https://github.com/KhrulkovV/geometry-score.git GEOM-SCORE\n","'''\n","print('trovi tutto nella cartella /content/drive/MyDrive/Colab Notebooks/algoritmi_custom')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":219,"status":"aborted","timestamp":1656671142686,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"7X3puAh2D-Be"},"outputs":[],"source":["!pip install gudhi"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":220,"status":"aborted","timestamp":1656671142688,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"WcCUCJbrC7ZW"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom/GEOM-SCORE')\n","!ls\n","import gs\n","from gs import geom_score\n","#!pip install -r requirements.txt --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":219,"status":"aborted","timestamp":1656671142689,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"NdfwLvvff6xs"},"outputs":[],"source":["x = gs.circle()\n","plt.scatter(x[:, 0], x[:, 1], s=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":218,"status":"aborted","timestamp":1656671142690,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"8fX3IXroEpbH"},"outputs":[],"source":["rltx = gs.rlts(x, n=100, L_0=32, i_max=10, gamma=1.0/8)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":219,"status":"aborted","timestamp":1656671142691,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"meelOooEEub8"},"outputs":[],"source":["mrltx = np.mean(rltx, axis=0)\n","gs.fancy_plot(mrltx[:3])\n","plt.xticks(np.arange(3));"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":219,"status":"aborted","timestamp":1656671142692,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"WnB18x4eE1LP"},"outputs":[],"source":["# Get a second dataset for comparison...\n","\n","y = gs.filled_circle()\n","plt.scatter(y[:, 0], y[:, 1], s=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":219,"status":"aborted","timestamp":1656671142693,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"lDZQdRrUE6Ke"},"outputs":[],"source":["rlty = gs.rlts(y, n=100, L_0=32, i_max=10, gamma=1.0/8)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":220,"status":"aborted","timestamp":1656671142694,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"N5Wbc4NvFCbp"},"outputs":[],"source":["mrlty = np.mean(rlty, axis=0)\n","gs.fancy_plot(mrlty[:3])\n","plt.xticks(np.arange(3));"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":220,"status":"aborted","timestamp":1656671142695,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"vTydWGB-FGHE"},"outputs":[],"source":["# Compute score\n","gs.geom_score(rltx, rlty)"]},{"cell_type":"markdown","metadata":{"id":"H2p8tTB2fv7b"},"source":["#### KLD\n","\n","- non so se può essere utile ma c'è questo link che fa uso di Scipy \n","[link](https://machinelearningmastery.com/divergence-between-probability-distributions/) , l'ho provato ad implementare "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":221,"status":"aborted","timestamp":1656671142696,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"p-1ye3R8fyOW"},"outputs":[],"source":["#https://torchmetrics.readthedocs.io/en/stable/classification/kl_divergence.html ++++\n","\n","#https://machinelearningmastery.com/divergence-between-probability-distributions/\n","\n","#https://stackoverflow.com/questions/49886369/kl-divergence-for-two-probability-distributions-in-pytorch  +++++++\n","\n","#https://pytorch.org/docs/stable/generated/torch.nn.functional.kl_div.html\n","\n","\n","\n","'''LOG PROBABILITIES INFOs - servono delle probabilità per calcolare il KL-DIV, quindi forse prima tocca allenare pure il classificatore, e dare le probabilità come input in formato vettoriale, altrimenti i conti non tornano'''\n","\n","#https://stackoverflow.com/questions/58742766/how-to-get-log-probabilities-in-tensorflow\n","\n","#https://machinelearningmastery.com/divergence-between-probability-distributions/\n","\n","#https://discuss.pytorch.org/t/how-to-extract-probabilities/2720\n","\n","#https://stackoverflow.com/questions/58766519/how-to-get-probability-of-each-image-belonging-to-a-class\n","\n","#https://discuss.pytorch.org/t/kl-divergence-for-multi-label-classification/118884/2\n","\n","\n","\n","\n","import torch.nn.functional as F\n","\n","'''prova calcolo KL-div - alternativa 1''' #DOVREBBE ESSERE L'ALTERNATIVA PIÙ CORRETTA\n","P = torch.Tensor([0.36, 0.48, 0.16])\n","Q = torch.Tensor([0.333, 0.333, 0.333])\n","\n","(P * (P / Q).log()).sum()\n","# tensor(0.0863), 10.2 µs ± 508\n","\n","ris1 = F.kl_div(Q.log(), P, None, None, 'sum')\n","# tensor(0.0863), 14.1 µs ± 408 ns\n","print(ris1)\n","\n","'''prova calcolo KL-div - alternativa 2'''\n","ris2 = F.kl_div(P, Q)\n","print(ris2)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":221,"status":"aborted","timestamp":1656671142697,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"Vehz6degGssF"},"outputs":[],"source":["#https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.rel_entr.html\n","\n","# example of calculating the kl divergence (relative entropy) with scipy\n","from scipy.special import rel_entr\n","# define distributions\n","#p = [0.10, 0.40, 0.50]\n","#q = [0.80, 0.15, 0.05]\n","\n","p = [0.36, 0.48, 0.16]\n","q = [0.333, 0.333, 0.333]\n","\n","# calculate (P || Q)\n","kl_pq = rel_entr(p, q)\n","print('KL(P || Q): %.3f nats' % sum(kl_pq))\n","# calculate (Q || P)\n","kl_qp = rel_entr(q, p)\n","print('KL(Q || P): %.3f nats' % sum(kl_qp))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":222,"status":"aborted","timestamp":1656671142698,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"tdOqyzgPHEIm"},"outputs":[],"source":["from math import log2\n","# calculate the kl divergence\n","def kl_divergence(p, q):\n","\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n","\n","# calculate the js divergence\n","def js_divergence(p, q):\n","\tm = 0.5 * (p + q)\n","\treturn 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":222,"status":"aborted","timestamp":1656671142699,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"qjtGOve_HOG4"},"outputs":[],"source":["print(kl_divergence(q,p))\n","\n","print(kl_divergence(p,q))"]},{"cell_type":"markdown","metadata":{"id":"FZz9rT_PYs3V"},"source":["##QUALITATIVE METRIC - NEAREST NEIGHBOR "]},{"cell_type":"markdown","metadata":{"id":"EAZMofCHONfK"},"source":["####NEAREST NEIGHBOR"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":223,"status":"aborted","timestamp":1656671142700,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"kXTF5-TGZwRA"},"outputs":[],"source":["#Another popular approach for subjectively summarizing generator performance is “Nearest Neighbors.” This involves selecting examples of real images from the domain and locating one or more most similar generated images for comparison.\n","\n","#Distance measures, such as Euclidean distance between the image pixel data, is often used for selecting the most similar generated images\n","\n","\n","'''cerca di implementare un Nearest neighbor e capire come applicarlo al generatore, altrimenti metriche sono sufficienti'''\n"]},{"cell_type":"markdown","metadata":{"id":"bfJ3LnbAZhQx"},"source":["## TESTING MODEL GAN - MULTIPLE GAN ARCHITECTURES"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":223,"status":"aborted","timestamp":1656671142701,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"0EHk5JrKefID"},"outputs":[],"source":["os.chdir(path_images)"]},{"cell_type":"markdown","metadata":{"id":"QB6Ks6vqMLmS"},"source":["####APPOGGIO DI UNA VERSIONE DIVERSA DI WGAN-GP (ATTUALMENTE NON FUNZIONANTE)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":224,"status":"aborted","timestamp":1656671142702,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"f8lBXIoEMKT1"},"outputs":[],"source":["'''\n","#VERSIONE DIVERSA DI GENERATORE E DISCRIMINATORE\n","\n","# qui ci sono anche altri link con diverse architteture \n","# https://github.com/EmilienDupont/wgan-gp/tree/ef82364f2a2ec452a52fbf4a739f95039ae76fe3 \n","\n","class Generator(nn.Module):\n","    def __init__(self, img_size, latent_dim, dim):\n","        super(Generator, self).__init__()\n","\n","        self.dim = dim\n","        self.latent_dim = latent_dim\n","        self.img_size = img_size\n","        self.feature_sizes = (self.img_size[0] / 1, self.img_size[1] / 1)\n","        \n","        self.latent_to_features = nn.Sequential(\n","            nn.Linear(latent_dim, 8 * dim * int(self.feature_sizes[0]) * int(self.feature_sizes[1])),\n","            nn.ReLU()\n","        )\n","\n","        self.features_to_image = nn.Sequential(\n","            nn.ConvTranspose2d(8 * dim, 4 * dim, 4, 2, 1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(4 * dim),\n","            nn.ConvTranspose2d(4 * dim, 2 * dim, 4, 2, 1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(2 * dim),\n","            nn.ConvTranspose2d(2 * dim, dim, 4, 2, 1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(dim),\n","            nn.ConvTranspose2d(dim, self.img_size[2], 4, 2, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input_data):\n","        # Map latent into appropriate size for transposed convolutions\n","        x = self.latent_to_features(input_data)\n","        # Reshape\n","        x = x.view(-1, 8 * self.dim, int(self.feature_sizes[0]), int(self.feature_sizes[1]))\n","        # Return generated image\n","        return self.features_to_image(x)\n","\n","    def sample_latent(self, num_samples):\n","        return torch.randn((num_samples, self.latent_dim))\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, img_size, dim):\n","        \"\"\"\n","        img_size : (int, int, int)\n","            Height and width must be powers of 2.  E.g. (32, 32, 1) or\n","            (64, 128, 3). Last number indicates number of channels, e.g. 1 for\n","            grayscale or 3 for RGB\n","        \"\"\"\n","        super(Discriminator, self).__init__()\n","\n","        self.img_size = img_size\n","\n","        self.image_to_features = nn.Sequential(\n","            nn.Conv2d(self.img_size[2], dim, 4, 2, 1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(dim, 2 * dim, 4, 2, 1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(2 * dim, 4 * dim, 4, 2, 1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(4 * dim, 8 * dim, 4, 2, 1),\n","            nn.Sigmoid()\n","        )\n","\n","        # 4 convolutions of stride 2, i.e. halving of size everytime\n","        # So output size will be 8 * (img_size / 2 ^ 4) * (img_size / 2 ^ 4)\n","        output_size = 8 * dim * int(img_size[0] / 16) * int(img_size[1] / 16)\n","        self.features_to_prob = nn.Sequential(\n","            nn.Linear(output_size, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input_data):\n","        batch_size = input_data.size()[0]\n","        x = self.image_to_features(input_data)\n","        x = x.view(batch_size, -1)\n","        return self.features_to_prob(x)\n","\n","\n","  img_size = (270, 470, 3)\n","  generator = Generator(img_size=img_size, latent_dim=100, dim=16)\n","  discriminator = Discriminator(img_size=img_size, dim=16)\n","\n","'''"]},{"cell_type":"markdown","metadata":{"id":"U-uTUl2ZcETp"},"source":["##### (WGAN-GP) "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":224,"status":"aborted","timestamp":1656671142703,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"SvvQjqlocMXB"},"outputs":[],"source":["'''\n","opt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\n","       [2000,            64,               0.0002,   0.5,      0.999,    8,           100,              135,             1,              4,              0.01,              200]]\n","\n","img_shape = (opt[1][8], opt[1][7],  235)\n","#print(*img_shape)\n","\n","cuda = True if torch.cuda.is_available() else False\n","\n","\n","os.makedirs(\"images\", exist_ok=True)\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        def block(in_feat, out_feat, normalize=True):\n","            layers = [nn.Linear(in_feat, out_feat)]\n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *block( opt[1][6], 128, normalize=False),\n","            *block(128, 256),\n","            *block(256, 512),\n","            *block(512, 1024),\n","            nn.Linear(1024, int(np.prod(img_shape))),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        img = self.model(z)\n","        img = img.view(img.shape[0], *img_shape)\n","        #print(img.shape)\n","        return img\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(int(np.prod(img_shape)), 512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(256, 1),\n","        )\n","\n","    def forward(self, img):\n","        img_flat = img.view(img.shape[0], -1)\n","        validity = self.model(img_flat)\n","        return validity\n","\n","\n","\n","\n","# Loss weight for gradient penalty\n","lambda_gp = 10\n","\n","# Initialize generator and discriminator\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","\n","print(generator)\n","print(discriminator)\n","\n","\n","if cuda:\n","    generator.cuda()\n","    discriminator.cuda()\n","\n","#generator.to(device)\n","#discriminator.to(device)\n","\n","#VALUTA SE CONFIGURARE ANCHE IL SALVATAGGIO DELLE IMMAGINI DIRETTAMENTE SU DRIVE PER QUELLE CHE CREA IL 'GENERATOR'\n","#PROBABILMENTE SERVIRÀ PERCHE A NOI SERVONO LE IMMAGINI COME DATA AUGMENTATION, QUINDI O LE SALVIAMO PRIMA E POI CI FACCIAMO IL LOAD, OPPURE PENSA COME COMBINARE INSIEME QUESTA FASE CON QUELLA DI ALLENAMENTO DELLA RETE VERA E PROPRIA\n","'''\n","# Configure data loader per immagini di dataset MNIST (usato per prova)\n","'''\n","os.makedirs(\"../../data/mnist\", exist_ok=True)\n","dataloader = torch.utils.data.DataLoader(\n","    datasets.MNIST(\n","        \"../../data/mnist\",\n","        train=True,\n","        download=True,\n","        transform=transforms.Compose(\n","            [transforms.Resize( opt[1][7]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n","        ),\n","    ),\n","    batch_size= opt[1][1],\n","    shuffle=True,\n",")\n","'''\n","'''\n","# Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr= opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr= opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":224,"status":"aborted","timestamp":1656671142703,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"GkG8LgfxcRAJ"},"outputs":[],"source":["'''\n","def compute_gradient_penalty(D, real_samples, fake_samples):\n","    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n","    # Random weight term for interpolation between real and fake samples\n","    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n","    # Get random interpolation between real and fake samples\n","    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n","    d_interpolates = D(interpolates)\n","    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n","    # Get gradient w.r.t. interpolates\n","    gradients = autograd.grad(\n","        outputs=d_interpolates,\n","        inputs=interpolates,\n","        grad_outputs=fake,\n","        create_graph=True,\n","        retain_graph=True,\n","        only_inputs=True,\n","    )[0]\n","    gradients = gradients.view(gradients.size(0), -1)\n","    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","    return gradient_penalty\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":224,"status":"aborted","timestamp":1656671142704,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"JoFzPzaicWdo"},"outputs":[],"source":["'''\n","# ----------\n","#  Training\n","# ----------\n","gan_history = {}\n","gan_history['DLOSS'] = []\n","gan_history['GLOSS'] = []\n","gan_history['EPOCH'] = []\n","\n","batches_done = 0\n","for epoch in range( opt[1][0]):\n","    #for i, (imgs, _) in enumerate(trainloader):\n","    for i, (data, targets, targets2) in tqdm(enumerate(trainloader)):#, desc = 'Epoch : {} train batch'.format(epoch+1)):\n","        data = data.to(device=device)\n","        targets = targets.to(device = device) #classes\n","        targets2 = targets2.to(device = device) #series\n","\n","        # Configure input\n","        real_imgs = Variable(data.type(Tensor))\n","        ###print(real_imgs.shape)\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        optimizer_D.zero_grad()\n","        \n","        z = Variable(Tensor(np.random.normal(0, 0, (data.shape[0],  opt[1][6])))) \n","        #z = Variable(Tensor(np.random.normal(0, 0.5, (data.shape[0],  opt[1][6])))) #altro prova per ridurre il rumore \n","\n","        # Generate a batch of images\n","        fake_imgs = generator(z)\n","        ###print(fake_imgs.shape)\n","\n","        # Real images\n","        real_validity = discriminator(real_imgs)\n","        # Fake images\n","        fake_validity = discriminator(fake_imgs)\n","        # Gradient penalty\n","        \n","        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n","        \n","        \n","        # Adversarial loss\n","        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n","\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        optimizer_G.zero_grad()\n","\n","        # Train the generator every n_critic steps\n","        if i %  opt[1][9] == 0:\n","\n","            # -----------------\n","            #  Train Generator\n","            # -----------------\n","\n","            # Generate a batch of images\n","            fake_imgs = generator(z)\n","            # Loss measures generator's ability to fool the discriminator\n","            # Train on fake images\n","            fake_validity = discriminator(fake_imgs)\n","            g_loss = -torch.mean(fake_validity)\n","\n","            g_loss.backward()\n","            optimizer_G.step()\n","\n","            print(\n","                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n","                % (epoch,  opt[1][0], i, len(trainloader), d_loss.item(), g_loss.item())\n","            )\n","\n","            if batches_done %  opt[1][11] == 0:\n","\n","                save_image(fake_imgs.data[:1], \"images/new_%d.png\" % batches_done, nrow=1, normalize=True)\n","                ########################################################################            \n","        \n","                img_to_save =  fake_imgs.data[:1]\n","                print(img_to_save.shape)   \n","\n","                                \n","                img_to_save2 = gray2rgb(img_to_save)                             #ULTIMA PROVA FATTA PER SALVARE A COLORI - bisogna trasformarlo in tensore per poi poterlo salvare !!!\n","                tensor = torch.from_numpy(img_to_save2)\n","                print(tensor.shape)\n","\n","                print(img_to_save2.shape)                                                     \n","                \n","                save_image(img_to_save, \"images/new1_%d.png\" % batches_done, nrow=1, normalize=True)\n","                save_image(tensor, \"images/new2_%d.png\" % batches_done, nrow=1, normalize=True)\n","\n","                #########################################################################\n","\n","            batches_done +=  opt[1][9]\n","\n","    print('------------------------------------------------------------')\n","    print(\n","        \"[Epoch %d/%d][D loss: %f] [G loss: %f]\"\n","        % (epoch,  opt[1][0], d_loss.item(), g_loss.item())\n","    )\n","    print('------------------------------------------------------------')\n","    gan_history['DLOSS'].append(d_loss.item())\n","    gan_history['GLOSS'].append(g_loss.item()) \n","    gan_history['EPOCH'].append(epoch)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"GeSLye5CMa4Y"},"source":["#### (SOFTMAX) - GAN "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":227,"status":"aborted","timestamp":1656671142707,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"JNvHfQ0WSHDS"},"outputs":[],"source":["'''\n","opt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\n","       [2000,            64,               0.0002,   0.5,      0.999,    8,           100,              100,             1,              4,              0.01,              200]]\n","\n","import argparse\n","import os\n","import numpy as np\n","import math\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","os.makedirs(\"images\", exist_ok=True)\n","'''\n","'''\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n","parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n","parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n","parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n","parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n","parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n","parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n","parser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\n","parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n","parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\n","opt = parser.parse_args()\n","print(opt)\n","'''\n","'''\n","\n","img_shape = (opt[1][8], opt[1][7],  opt[1][7])\n","\n","cuda = True if torch.cuda.is_available() else False\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        def block(in_feat, out_feat, normalize=True):\n","            layers = [nn.Linear(in_feat, out_feat)]\n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *block(opt[1][6], 128, normalize=False),\n","            *block(128, 256),\n","            *block(256, 512),\n","            *block(512, 1024),\n","            nn.Linear(1024, int(np.prod(img_shape))),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        img = self.model(z)\n","        img = img.view(img.shape[0], *img_shape)\n","        return img\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(int(np.prod(img_shape)), 512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(256, 1),\n","        )\n","\n","    def forward(self, img):\n","        img_flat = img.view(img.shape[0], -1)\n","        print(img_flat.shape)\n","        validity = self.model(img_flat)\n","\n","        return validity\n","\n","\n","# Loss function\n","adversarial_loss = torch.nn.BCELoss()\n","\n","# Initialize generator and discriminator\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","if cuda:\n","    generator.cuda()\n","    discriminator.cuda()\n","    adversarial_loss.cuda()\n","\n","\n","\n","# Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","\n","def log(x):\n","    return torch.log(x + 1e-8)\n","\n","# ----------\n","#  Training\n","# ----------\n","\n","gan_history = {}\n","gan_history['DLOSS'] = []\n","gan_history['GLOSS'] = []\n","gan_history['EPOCH'] = []\n","\n","for epoch in range( opt[1][0]):\n","    for i, (data, targets, targets2) in tqdm(enumerate(trainloader)):\n","\n","        data = data.to(device=device)\n","        targets = targets.to(device = device) #classes\n","        targets2 = targets2.to(device = device) #series\n","\n","        optimizer_G.zero_grad()\n","        optimizer_D.zero_grad()\n","\n","        batch_size = data.shape[0]\n","\n","        # Adversarial ground truths\n","        g_target = 1 / (batch_size * 2)\n","        d_target = 1 / batch_size\n","\n","        # Configure input\n","        real_imgs = Variable(data.type(Tensor))\n","\n","        # Sample noise as generator input\n","        z = Variable(Tensor(np.random.normal(0, 1, (data.shape[0], opt[1][6]))))\n","        # Generate a batch of images\n","        gen_imgs = generator(z)\n","\n","        d_real = discriminator(real_imgs)\n","        d_fake = discriminator(gen_imgs)\n","\n","        # Partition function\n","        Z = torch.sum(torch.exp(-d_real)) + torch.sum(torch.exp(-d_fake))\n","\n","        # Calculate loss of discriminator and update\n","        d_loss = d_target * torch.sum(d_real) + log(Z)\n","        \n","\n","\n","\n","        # Calculate loss of generator and update\n","        g_loss = g_target * (torch.sum(d_real) + torch.sum(d_fake)) + log(Z)\n","\n","\n","        d_loss.backward(retain_graph=True)\n","        g_loss.backward()\n","\n","        optimizer_G.step()\n","        optimizer_D.step()\n","\n","\n","        #print(\n","        #    \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n","        #    % (epoch, opt[1][0], i, len(trainloader), d_loss.item(), g_loss.item())\n","        #)\n","\n","        batches_done = epoch * len(trainloader) + i\n","        if batches_done % opt[1][11] == 0:\n","            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n","\n","    print('------------------------------------------------------------')\n","    print(\n","      \"[Epoch %d/%d][D loss: %f] [G loss: %f]\"\n","      % (epoch,  opt[1][0], d_loss.item(), g_loss.item())\n","    )\n","    print('------------------------------------------------------------')\n","    \n","    gan_history['DLOSS'].append(d_loss.item())\n","    gan_history['GLOSS'].append(g_loss.item()) \n","    gan_history['EPOCH'].append(epoch)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"lY-1rkp2bUi6"},"source":["#### (DC GAN)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":227,"status":"aborted","timestamp":1656671142709,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"j8rJWH1abXJ0"},"outputs":[],"source":["'''\n","opt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\n","       [2000,            8,               0.0002,   0.5,      0.999,    8,           1,              128*2,             1,              4,              0.01,              100]]\n","\n","gan_history = {}\n","gan_history['DLOSS'] = []\n","gan_history['GLOSS'] = []\n","gan_history['EPOCH'] = []\n","\n","import argparse\n","import os\n","import numpy as np\n","import math\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","os.makedirs(\"images\", exist_ok=True)\n","\n","cuda = True if torch.cuda.is_available() else False\n","\n","def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find(\"BatchNorm2d\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        self.init_size = opt[1][7] // 4\n","        self.l1 = nn.Sequential(nn.Linear(opt[1][6], 128 * self.init_size ** 2))\n","\n","        self.conv_blocks = nn.Sequential(\n","            nn.BatchNorm2d(128),\n","            nn.Upsample(scale_factor=2),\n","            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n","            nn.BatchNorm2d(128, 0.8),\n","            #nn.LeakyReLU(0.2, inplace=True),\n","            nn.ReLU(),\n","            nn.Upsample(scale_factor=2),\n","            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n","            nn.BatchNorm2d(64, 0.8),\n","            #nn.LeakyReLU(0.2, inplace=True),\n","            nn.ReLU(),\n","            nn.Conv2d(64, opt[1][8], 3, stride=1, padding=1),\n","            #nn.Tanh(),\n","        )\n","\n","    def forward(self, z):\n","        out = self.l1(z)\n","        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n","        img = self.conv_blocks(out)\n","        return img\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        def discriminator_block(in_filters, out_filters, bn=True):\n","            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), \n","                     #nn.LeakyReLU(0.2, inplace=True),\n","                     nn.ReLU(),\n","                     nn.Dropout2d(0.25)]\n","            if bn:\n","                block.append(nn.BatchNorm2d(out_filters, 0.8))\n","            return block\n","\n","        self.model = nn.Sequential(\n","            *discriminator_block(opt[1][8], 16, bn=False),\n","            *discriminator_block(16, 32),\n","            *discriminator_block(32, 64),\n","            *discriminator_block(64, 128),\n","        )\n","\n","        # The height and width of downsampled image\n","        ds_size = opt[1][7] // 2 ** 4\n","        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n","\n","    def forward(self, img):\n","        out = self.model(img)\n","        out = out.view(out.shape[0], -1)\n","        validity = self.adv_layer(out)\n","        return validity\n"," \n","\n","\n","\n","\n","# Loss function\n","adversarial_loss = torch.nn.MSELoss() #era BCELoss()\n","\n","\n","\n","generator = Generator() #\n","discriminator = Discriminator()\n","print(generator)\n","print(discriminator)\n","\n","if cuda:\n","    generator.cuda()\n","    discriminator.cuda()\n","    adversarial_loss.cuda()\n","\n","# Initialize weights\n","#generator.apply(weights_init_normal)\n","#discriminator.apply(weights_init_normal)\n","\n","# Optimizers\n","#optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","optimizer_G = optim.SGD(model.parameters(), weight_decay=1e-5, lr=0.001, momentum=0.8)\n","#optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","optimizer_D = optim.SGD(model.parameters(), weight_decay=1e-5, lr=0.001, momentum=0.8)\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","# ----------\n","#  Training\n","# ----------\n","\n","\n","for epoch in range( opt[1][0]):\n","    for i, (data, targets, targets2) in tqdm(enumerate(trainloader)):\n","\n","        data = data.to(device=device)\n","        targets = targets.to(device = device) #classes\n","        targets2 = targets2.to(device = device) #series\n","\n","        # Adversarial ground truths\n","        valid = Variable(Tensor(data.shape[0], 1).fill_(1.0), requires_grad=False)\n","        fake = Variable(Tensor(data.shape[0], 1).fill_(0.0), requires_grad=False)\n","\n","\n","        # Configure input\n","        real_imgs = Variable(data.type(Tensor))\n","\n","        # -----------------\n","        #  Train Generator\n","        # -----------------\n","\n","        optimizer_G.zero_grad()\n","\n","        # Sample noise as generator input\n","        z = Variable(Tensor(np.random.normal(0, 0.5, (data.shape[0], opt[1][6]))))\n","\n","        # Generate a batch of images\n","        gen_imgs = generator(z)\n","\n","        # Loss measures generator's ability to fool the discriminator\n","        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n","\n","        g_loss.backward()\n","        optimizer_G.step()\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        optimizer_D.zero_grad()\n","        # Measure discriminator's ability to classify real from generated samples\n","        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n","        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n","        d_loss = (real_loss + fake_loss) / 2\n","\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","    print(\n","        \"[Epoch %d/%d][D loss: %f] [G loss: %f]\"\n","        % (epoch, opt[1][0], d_loss.item(), g_loss.item())\n","    )\n","\n","    batches_done = epoch * len(trainloader) + i\n","    if batches_done % opt[1][11] == 0:\n","        save_image(gen_imgs.data[:1], \"images/%d.png\" % batches_done, nrow=5, normalize=True)    \n","\n","    gan_history['DLOSS'].append(d_loss.item())\n","    gan_history['GLOSS'].append(g_loss.item()) \n","    gan_history['EPOCH'].append(epoch)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"hA59LYtlDgUr"},"source":["#### (WPGAN - DIV) "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":228,"status":"aborted","timestamp":1656671142711,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"1NcROD0_Dm6I"},"outputs":[],"source":["'''\n","import argparse\n","import os\n","import numpy as np\n","import math\n","import sys\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.autograd as autograd\n","import torch\n","\n","\n","opt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\n","       [2000,            8,               0.0002,   0.5,      0.999,    8,           1,              128*2,             1,              4,              0.01,              100]]\n","\n","gan_history = {}\n","gan_history['DLOSS'] = []\n","gan_history['GLOSS'] = []\n","gan_history['EPOCH'] = []\n","\n","\n","os.makedirs(\"images\", exist_ok=True)\n","\n","img_shape = (opt[1][8], opt[1][7], opt[1][7])\n","\n","cuda = True if torch.cuda.is_available() else False\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        def block(in_feat, out_feat, normalize=True):\n","            layers = [nn.Linear(in_feat, out_feat)]\n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *block(opt[1][6], 128, normalize=False),\n","            *block(128, 256),\n","            *block(256, 512),\n","            *block(512, 1024),\n","            nn.Linear(1024, int(np.prod(img_shape))),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        img = self.model(z)\n","        img = img.view(img.shape[0], *img_shape)\n","        return img\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(int(np.prod(img_shape)), 512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(256, 1),\n","        )\n","\n","    def forward(self, img):\n","        img_flat = img.view(img.shape[0], -1)\n","        validity = self.model(img_flat)\n","        return validity\n","\n","\n","k = 2\n","p = 6\n","\n","\n","# Initialize generator and discriminator\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","if cuda:\n","    generator.cuda()\n","    discriminator.cuda()\n","\n","\n","# Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","# ----------\n","#  Training\n","# ----------\n","\n","batches_done = 0\n","for epoch in range( opt[1][0]):\n","    for i, (data, targets, targets2) in tqdm(enumerate(trainloader)):\n","\n","        data = data.to(device=device)\n","        targets = targets.to(device = device) #classes\n","        targets2 = targets2.to(device = device) #series\n","\n","\n","        # Configure input\n","        real_imgs = Variable(data.type(Tensor), requires_grad=True)\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        optimizer_D.zero_grad()\n","\n","        # Sample noise as generator input\n","        z = Variable(Tensor(np.random.normal(0, 1, (data.shape[0], opt[1][6]))))\n","\n","        # Generate a batch of images\n","        fake_imgs = generator(z)\n","\n","        # Real images\n","        real_validity = discriminator(real_imgs)\n","        # Fake images\n","        fake_validity = discriminator(fake_imgs)\n","\n","        # Compute W-div gradient penalty\n","        real_grad_out = Variable(Tensor(real_imgs.size(0), 1).fill_(1.0), requires_grad=False)\n","        real_grad = autograd.grad(\n","            real_validity, real_imgs, real_grad_out, create_graph=True, retain_graph=True, only_inputs=True\n","        )[0]\n","        real_grad_norm = real_grad.view(real_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n","\n","        fake_grad_out = Variable(Tensor(fake_imgs.size(0), 1).fill_(1.0), requires_grad=False)\n","        fake_grad = autograd.grad(\n","            fake_validity, fake_imgs, fake_grad_out, create_graph=True, retain_graph=True, only_inputs=True\n","        )[0]\n","        fake_grad_norm = fake_grad.view(fake_grad.size(0), -1).pow(2).sum(1) ** (p / 2)\n","\n","        div_gp = torch.mean(real_grad_norm + fake_grad_norm) * k / 2\n","\n","        # Adversarial loss\n","        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + div_gp\n","\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        optimizer_G.zero_grad()\n","\n","        # Train the generator every n_critic steps\n","        if i % opt[1][9] == 0:\n","\n","            # -----------------\n","            #  Train Generator\n","            # -----------------\n","\n","            # Generate a batch of images\n","            fake_imgs = generator(z)\n","            # Loss measures generator's ability to fool the discriminator\n","            # Train on fake images\n","            fake_validity = discriminator(fake_imgs)\n","            g_loss = -torch.mean(fake_validity)\n","\n","            g_loss.backward()\n","            optimizer_G.step()\n","\n","    print(\n","        \"[Epoch %d/%d][D loss: %f] [G loss: %f]\"\n","        % (epoch, opt[1][0], d_loss.item(), g_loss.item())\n","    )\n","\n","    batches_done = epoch * len(trainloader) + i\n","    if batches_done % opt[1][11] == 0:\n","        save_image(fake_imgs.data[:1], \"images/%d.png\" % batches_done, nrow=5, normalize=True)    \n","\n","    gan_history['DLOSS'].append(d_loss.item())\n","    gan_history['GLOSS'].append(g_loss.item()) \n","    gan_history['EPOCH'].append(epoch)\n","\n","'''"]},{"cell_type":"markdown","metadata":{"id":"MEhIQS3lx0jL"},"source":["#### GAN - QUELLO FUNZIONANTE \n","(https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)"]},{"cell_type":"markdown","metadata":{"id":"YWzjz2dEFoHr"},"source":["##### GAN ORIGINALE (SENZA OPTUNA)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":227,"status":"aborted","timestamp":1656671142712,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"},"user_tz":-120},"id":"IzKfqHpqS8X9"},"outputs":[],"source":["\"\"\"\n","'''stampa degli istogrammi delle immagini reali'''\n","\n","cuda = True if torch.cuda.is_available() else False\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","\n","dataloader = trainloader\n","for i, (data, targets, targets2) in tqdm(enumerate(dataloader)):\n","\n","        data = data.to(device=device)\n","        targets = targets.to(device = device) #classes\n","        targets2 = targets2.to(device = device) #series\n","\n","\n","        # Adversarial ground truths\n","        valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\n","        fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\n","\n","        # Configure input\n","        real_imgs = Variable(data.type(Tensor))\n","        for counter in range(len(real_imgs)):\n","          x = torch.histc(real_imgs.data[counter], bins=4)\n","          #print(x[1])\n","          print(torch.histc(real_imgs.data[counter], bins=4))\n","\n","\n","\n","\n","'''------------------------------------------------------------DA FARE-----------------------------------------------------------------------------------------------------------------------------------------------'''\n","  #PROVA A CALCOLARE LA METRICA MMD TRA BATCH INTERO DI REALI E SINGOLA IMMAGINE GENERATA CICLANDO.\n","\n","  #OPPURE CALCOLA L'ISTOGRAMMA PER TUTTE LE IMMAGINI REALI E VEDI I VALORI DI RIFERIMENTO CHE DEVONO AVERE PER FARE LE CONDIZIONI DI SALVATAGGIO DI IMMAGINE SOPRA\n","'''-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5Ea9NVNx2xm","executionInfo":{"status":"aborted","timestamp":1656671142714,"user_tz":-120,"elapsed":227,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["\"\"\"\n","\n","\n","'''ADDESTRAMENTO GAN\n","\n","\n","\n","NOTE PER LE MODIFICHE DEGLI  HYPERPARAMETRI DURANTE LA FASE DI TEST DELLA GAN PER MIGLIOR SETTAGGIO\n","\n","\n","########################\n","MODE COLLAPSE = il generatore produce solo pochi dettagli, quindi DIVERSITÀ BASSA (SE METRICHE DIVERSITÀ AVRANNO VALORE BASSO) = C'È OSCILLAZIONE NELLA LOSS DEL GENERATORE TIPICAMENTE (DISCRIMINATOR RICONOSCE CHE SONO FINTE)\n","\n","- LEARNING RATE: modificare il learning rate solo abbassandolo (se immagini non hanno molti dettagli)\n","\n","- LATENT SPACE : come per il caso precedente, (abbassandolo diminuiamo l'insieme dei possibili output che genera ogni iterazione e quindi non si allena molto in diversità)\n","\n","########################\n","\n","\n","\n","########################\n","CONVERGENCE FAILURE = quando la loss non fa al caso del modello utilizzato di generatore, e che con le immagini che si stanno usando la LOSS GENERATORE OSCILLA/HA VALORI ALTI/CONTINUA A CRESCERE e/o LOSS DISCIMINATORE TENDE A 0\n","\n","- LOSS TYPE: modificarla potrebbe migliorare la generazione \n","\n","- MODIFICARE LA CAPACITÀ DEI LAYERS DEL MODELLO: aumentare la dimensione dei layers potrebbe aumentare la QUALITÀ del risultato\n","########################\n","'''\n","\n","\n","import argparse\n","import os\n","import numpy as np\n","import math\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","\n","\n","\n","os.makedirs(\"images\", exist_ok=True)\n","os.makedirs(\"images_to_check\", exist_ok=True)\n","\n","gan_history = {}\n","gan_history['DLOSS'] = []\n","gan_history['GLOSS'] = []\n","gan_history['MMD'] = []\n","gan_history['SSIM'] = []\n","gan_history['EPOCH'] = []\n","\n","opt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\n","      [2500,              32,                0.0002,   0.5,      0.999,    8,           100,                128*2,             3,              4,              0.01,              20]]\n","\n","'''\n","opt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\n","      [2000,              64,                0.0002,   0.5,      0.999,    8,           100,                128*2,             3,              4,              0.01,              20]]\n","'''\n","\n","\n","#inutile cambiare il batch_size, perchè tanto prende quello dato al DataLoader!!!!!!\n","#print(\"ok {} \".format(config))\n","\n","\n","img_shape = (opt[1][8], 270, 470)\n","\n","\n","cuda = True if torch.cuda.is_available() else False\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","\n","#print(opt[1][7])\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        def block(in_feat, out_feat, normalize=True):\n","            layers = [nn.Linear(in_feat, out_feat)]\n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *block(opt[1][6], 128, normalize=False),\n","            *block(128, 256),\n","            *block(256, 512),\n","            *block(512, 1024),\n","            nn.Linear(1024, int(np.prod([img_shape]))),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        img = self.model(z)\n","        img = img.view(img.size(0), *img_shape)\n","        return img\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(int(np.prod(img_shape)), 512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, img):\n","        img_flat = img.view(img.size(0), -1)\n","        validity = self.model(img_flat)\n","\n","        return validity\n","\n","\n","# Loss function\n","adversarial_loss = torch.nn.BCELoss()\n","\n","# Initialize generator and discriminator\n","generator = Generator()\n","\n","#hl.build_graph(generator, torch.zeros([3, 3, 270, 470]))\n","\n","discriminator = Discriminator()\n","\n","#hl.build_graph(discriminator, torch.zeros([3, 3, 270, 470]))\n","\n","if cuda:\n","    generator.cuda()\n","    discriminator.cuda()\n","    adversarial_loss.cuda()\n","\n","# Configure data loader\n","dataloader = trainloader\n","\n","# ---------Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","#optimizer_G = torch.optim.Adam(generator.parameters(), lr=config['lr_'], betas=(config[\"b1_\"],  config[\"b2_\"]))\n","\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt[1][2], betas=( opt[1][3],  opt[1][4]))\n","#optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=config['lr_'], betas=(config[\"b1_\"],  config[\"b2_\"]))\n","\n","#---------\n","#optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=0.00002) #VA MA NON CREA IMMAGINI MOLTO BUONE\n","\n","#optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=0.00002) #VA MA NON CREA IMMAGINI MOLTO BUONE\n","\n","\n","\n","\n","\n","\n","# ----------\n","#  Training\n","# ----------\n","\n","\n","#--------------------parametri per calcolare NDB score-----------------\n","\n","batches_done = 0\n","contatore_stabilita = 0\n","real_batches = [] #to save real images to be used with NDB METRIC\n","generated_batches = [] #to save real images to be used with NDB METRIC\n","iters = 0\n","img_list = []\n","metrics = []\n","ndb_scores = []\n","real_batches2 = []\n","real_batches_np = 0\n","real_batches2_np = 0\n","real_concatenate = 0\n","\n","#per metrica FID\n","BatchSize = 8 \n","UseMultiprocessing = False \n","\n","\n","'''estrazione batch di immagini reali per NDB'''\n","\n","\n","for i, (data, targets, targets2) in tqdm(enumerate(trainloader)):\n","\n","    #- MEMORIZZO IL SET DI IMMAGINI REALI PER USARLO NELLA METRICA 'NDB' -\n","    real = data.numpy()\n","    if len(trainloader)==1:\n","      real_batches.append(real)\n","      real_concatenate = np.array(real_batches)\n","    else:\n","      if i==0:\n","        real_batches.append(real)\n","        real_batches_np = np.array(real_batches)\n","        print(real_batches_np.shape)\n","        real_batches = []\n","      if i==1:\n","        real_batches.append(real)\n","        real_batches2_np = np.array(real_batches)\n","        print(real_batches2_np.shape)\n","        real_batches = []\n","        real_concatenate = np.concatenate((real_batches_np,real_batches2_np), axis=1)\n","      if i>1:\n","        real_batches.append(real)\n","        real_batches2_np = np.array(real_batches)\n","        print(real_batches2_np.shape)\n","        real_batches = []\n","        real_concatenate = np.concatenate((real_concatenate,real_batches2_np), axis=1)\n","    \n","\n","#RIADATTAMENTO DELLE IMMAGINI REALI & GENERATE PER METRICA 'NDB'\n","#real_batches = np.array(real_batches)\n","real_batches = real_concatenate\n","\n","#print(real_batches)\n","#print(real_batches.shape)\n","#print(len(real_batches))\n","\n","#Display a sample\n","#plt.imshow(np.transpose(real_batches[0][0], (1,2,0)))\n","\n","real_combined = real_batches.reshape(numTrainBatches*len(df_class_new), 3, img_shape[1], img_shape[2])  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\n","#real_combined = real_batches.reshape(numTrainBatches*len(df_class_new), 3, opt[1][7], opt[1][7])  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\n","\n","\n","del real_batches\n","gc.collect()\n","real_batches = []\n","real_batches_np = []\n","\n","'''alternativa'''\n","#real_combined = real_samples(numTrainBatches, trainloader, nc, opt[1][7])\n","'''----------------------------------------------------------------------'''   \n","\n","\n","#il KERNEL_TYPE = \"rbf\" serve per il MMD \n","KERNEL_TYPE = \"rbf\"\n","\n","for epoch in range( opt[1][0]):\n","    for i, (data, targets, targets2) in tqdm(enumerate(dataloader)):\n","\n","        data = data.to(device=device)\n","        targets = targets.to(device = device) #classes\n","        targets2 = targets2.to(device = device) #series\n","\n","\n","        # Adversarial ground truths\n","        valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\n","        fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\n","\n","        # Configure input\n","        real_imgs = Variable(data.type(Tensor))\n","\n","        # -----------------\n","        #  Train Generator\n","        # -----------------\n","\n","        optimizer_G.zero_grad()\n","\n","        # Sample noise as generator input\n","        z = Variable(Tensor(np.random.normal(5, 10, (data.shape[0], opt[1][6]))))         #PRIMA INVECE DI -5,10- C'ERA -0,1- MA ERA MENO EFFICACE NEL COMPLESSO DEL BATCH\n","\n","        # Generate a batch of images\n","        gen_imgs = generator(z)\n","\n","\n","\n","        # Loss measures generator's ability to fool the discriminator\n","        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n","\n","        g_loss.backward()\n","        optimizer_G.step()\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        optimizer_D.zero_grad()\n","\n","        # Measure discriminator's ability to classify real from generated samples\n","        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n","        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n","        d_loss = (real_loss + fake_loss) / 2\n","\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        \n","\n","        print(\n","            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n","            % (epoch, opt[1][0], i+1, len(dataloader), d_loss.item(), g_loss.item())\n","        )\n","\n","        batches_done = epoch * len(dataloader) + i\n","        \n","        #questo per vedere ogni 10 epoche come sono le immagini che genera\n","        #if batches_done % 100 == 0 : \n","          #save_image(gen_imgs.data[:30], \"images_batch/{}_{}.png\".format(epoch, i+1), nrow=6, normalize=True)\n","\n","        if d_loss.item() < 1 and d_loss.item() > 0.3 and epoch>=1750 and g_loss.item() < 1.5:\n","          for counter in range(len(gen_imgs)):\n","            x = torch.histc(gen_imgs.data[counter], bins=4)\n","            #if epoch % 10 == 0:\n","              #print(torch.histc(gen_imgs.data[counter], bins=4))\n","            \n","            if x[0]>110000 and x[0]<210000 and x[1]>70000 and x[1]<120000 and x[2]>40000 and x[2]<85000 and x[3]>20000 and x[3]<100000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 9 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","              save_image(gen_imgs.data[counter], \"images9/_{}_{}_{}.png\".format(epoch, i+1, counter), nrow=1, normalize=True)                    #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","              \n","\n","            '''PARTE DI CALCOLO DELL' MMD'''\n","            '''\n","            real_imgs2 =  real_imgs.reshape(real_imgs.shape[0], real_imgs.shape[1]*real_imgs.shape[2]*real_imgs.shape[3])\n","            #print(real_imgs2.shape)\n","            #print(type(real_imgs2))\n","\n","            gen_imgs2 =  gen_imgs.reshape(gen_imgs.shape[0], gen_imgs.shape[1]*gen_imgs.shape[2]*gen_imgs.shape[3])\n","            #print(gen_imgs2.shape)\n","            #print(type(gen_imgs2))\n","\n","            mmd_average = MMD(real_imgs2, gen_imgs2, KERNEL_TYPE, device) #---- aggiungere device nella funzione all'interno dell'file \n","            '''\n","\n","        #------------------------------------------stampo i batch in cartelle e salvo i modelli nei punti di stabilità\n","        '''\n","        if epoch >2000  and g_loss<1.5:  #epoch>1750 quasi sempre con batch da 64, anche con batch da 96 (32*3)\n","          \n","          if contatore_stabilita >=5 and d_loss>0.30:\n","            torch.save(generator.state_dict(),'/content/drive/MyDrive/CALCIO_CROP_BASE/models/model_generator_4_{}.pth'.format(epoch))                      #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","            #torch.save(discriminator.state_dict(),'/content/drive/MyDrive/CALCIO_CROP_BASE/models/model_discriminator_2__{}.pth'.format(epoch))                          #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICAR\n","            contatore_stabilita = 0\n","            save_image(gen_imgs.data[:30], \"images_batch_4/_{}_{}_{}.png\".format(epoch, i+1, d_loss.item()), nrow=6, normalize=True)\n","\n","          if d_loss > 0.25:\n","            contatore_stabilita = contatore_stabilita + 1\n","          \n","          if contatore_stabilita>0 and d_loss<0.25:\n","            contatore_stabilita = 0\n","          \n","        \n","        if epoch % 25 == 0:\n","          save_image(gen_imgs.data[:30], \"images_batch_2/_{}_{}_{}.png\".format(epoch, i+1, d_loss.item()), nrow=6, normalize=True)                        #---------------------------MODIFICARE SEMPRE IL DATASET CHE SI VUOLE ITERARE DA MODIFICARE IL PERCORSO\n","        '''\n","        #------------------------------------------\n","\n","        #SUCCESSIVAMENTE IMPLEMENTARE UN VETTORE CHE MEMORIZZA I VALORI DI LOSS DEL GENERATOR E MEMORIZZARE MODELLO QUANDO LA VARIANZA è BASSA COSì COME QUANDO è IL SUO VALORE BASSO, SOTTO 1.5 (ALMENO)\n","        #siamo sempre in situazione di mode collapse, ovvero il generator oscilla troppo e non riesce a generare troppa diversità tra le immagini, con qualsiasi dimensione di batch, questo perchè forse ha pochi elementi, ma comunque usando un batch superiore a 64 collassa dopo un pò.\n","        #provare comunque a verificare in futuro usando le immagini Flippate e Brightness ad usare un batch più grande e capire se riese a generare le immagini con meno epoche e quando più o meno collassa se ha già generato cose di sufficiente qualità\n","\n","        '''\n","        #PARTE DI CALCOLO DELL' NDB\n","        # Check how the generator is doing by saving G's output on fixed_noise\n","        if (iters % 10 == 0) or ((epoch == opt[1][0]-1) and (i == len(dataloader)-1)):\n","            with torch.no_grad():\n","                fake = generator(z).detach().cpu()\n","            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n","            \n","\n","            #genero fake batches images per calcolare metrica NDB\n","            gen_combined = generated_fakes(numTestBatches, generator, img_shape[1], img_shape[2], opt[1][6])\n","      \n","            #Calculate NDB\n","            train_size = numTrainBatches*62     #perchè il set ha 62 immagini e non riempe neanche il batch\n","            test_size = numTestBatches*62\n","            dim = 270*470*3\n","            #dim = img_shape[1]*img_shape[2]*3\n","\n","            train_samples = real_combined.reshape(train_size, dim)\n","            test_samples = gen_combined.reshape(test_size, dim)\n","            \n","            ndb = NDB(training_data=train_samples, number_of_bins=k, whitening=True)\n","            results = ndb.evaluate(test_samples)\n","                \n","            ndb_k = float(results[\"NDB\"])/ndb.number_of_bins\n","            #wandb.log({\"ndb_k\" : ndb_k, \"JS\": results[\"JS\"]})   \n","\n","        iters += 1\n","\n","\n","    #memorizzo info su NDB\n","    #specifically track NDB\n","    metrics.append(results)\n","    ndb_scores.append(ndb_k)\n","    \n","    #print(\"NDB_K: \", ndb_k)\n","    '''\n","\n","    #--------------------------------------------------------------------------------------------\n","    #--------------------------------------------------------------------------------------------\n","    \n","\n","    \n","    '''\n","    #PARTE DI CALCOLO DELL' MMD\n","    #---- Maximum Mean Discrepancy (MMD) (non capisco come farlo funzionare anche con le img a colori, se ti da qualche errore, prova prima a rieseguire il blocco subito sopra -----\n","    \n","    real_imgs2 =  real_imgs.reshape(real_imgs.shape[0], real_imgs.shape[1]*real_imgs.shape[2]*real_imgs.shape[3])\n","    #print(real_imgs2.shape)\n","    #print(type(real_imgs2))\n","\n","    gen_imgs2 =  gen_imgs.reshape(gen_imgs.shape[0], gen_imgs.shape[1]*gen_imgs.shape[2]*gen_imgs.shape[3])\n","    #print(gen_imgs2.shape)\n","    #print(type(gen_imgs2))\n","\n","    \n","    #mmd_average = MMD(real_imgs2, gen_imgs2, KERNEL_TYPE, device) #---- aggiungere device nella funzione all'interno dell'file \n","    #if epoch > 1000 and epoch % 10 == 0 and mmd_average<2:\n","    #  print('MMD AVERAGE : {}'.format(mmd_average))\n","    '''\n","\n","\n","    '''\n","    #CALCOLO SSIM DA MONITORARE\n","    if epoch > 1000 and epoch % 10 == 0:\n","      #real_combined_ssim = torch.from_numpy(real_combined)\n","      real = data.numpy()\n","      real_batches.append(real)\n","      real_batches_np = np.array(real_batches)\n","      #print(real_batches_np.shape)\n","      real_combined_ssim = real_batches_np.reshape(len(data), 3, img_shape[1], img_shape[2])  #IL PRIMO PARAMETRO SARÀ DA CAMBIARE FORSE QUANDO AVREMO PIÙ IMMAGINI SE NON TORNA IL CONTO\n","      real_combined_ssim = torch.from_numpy(real_combined_ssim)\n","      #print(real_combined_ssim.shape)\n","\n","      #gen_combined_ssim = torch.from_numpy(gen_combined)\n","      #print(gen_combined_ssim.shape)\n","    \n","      #calcolo metrica\n","      ris_ssim = ssim(real_combined_ssim,gen_imgs)\n","      real_batches = []\n","\n","      #print('MMD AVERAGE : {}'.format(mmd_average))\n","      print(\"ssim :{}\".format(ris_ssim))\n","      gan_history['SSIM'].append(ris_ssim.item())\n","      '''\n","    \n","    \n","\n","    \n","\n","\n","    '''memorizzo info su addestramento GAN'''\n","    gan_history['DLOSS'].append(d_loss.item())\n","    gan_history['GLOSS'].append(g_loss.item()) \n","    gan_history['EPOCH'].append(epoch)\n","    #gan_history['MMD'].append(mmd_average.item())\n","    \n","\n","\n","        \n","\n","\n","\n","'''As such, the practice of systematically generating images and saving models during training can and should continue to be used to allow post-hoc model selection'''\n","\n","'''CONVIENE SALVARE I MODELLI GENERATORE E DISCRIMINATORE, PERCHÈ POTREBBE AVERE I SUOI MASSIMI DI RISULTATI ANCHE IN MEZZO ALL'ALLENAMENTO'''\n","\n","'''\n","classe 0 - ok - if  x[0]>40000 and x[0]<180000 and x[1]>60000 and x[1]<130000 and x[2]>55000 and x[2]<100000 and x[3]>10000 and x[3]<2000000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 0 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 1 - ok - if x[0]>80000 and x[0]<200000 and x[1]>50000 and x[1]<150000 and x[2]>50000 and x[2]<100000 and x[3]>10000 and x[3]<150000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 1 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 2 - ok - if x[0]>50000 and x[0]<220000 and x[1]>60000 and x[1]<130000 and x[2]>45000 and x[2]<100000 and x[3]>10000 and x[3]<130000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 2 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 3 - ok - if x[0]>70000 and x[0]<200000 and x[1]>50000 and x[1]<150000 and x[2]>45000 and x[2]<100000 and x[3]>10000 and x[3]<130000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 3 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 4 - ok - if x[0]>70000 and x[0]<200000 and x[1]>40000 and x[1]<130000 and x[2]>50000 and x[2]<100000 and x[3]>10000 and x[3]<170000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 4 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 5 - ok - if x[0]>90000 and x[0]<200000 and x[1]>50000 and x[1]<130000 and x[2]>45000 and x[2]<100000 and x[3]>10000 and x[3]<160000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 5 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 7 - if x[0]>100000 and x[0]<200000 and x[1]>70000 and x[1]<120000 and x[2]>50000 and x[2]<90000 and x[3]>20000 and x[3]<150000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 7 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte \n","classe 8 - ok - if x[0]>100000 and x[0]<200000 and x[1]>80000 and x[1]<140000 and x[2]>40000 and x[2]<85000 and x[3]>20000 and x[3]<120000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 8 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","classe 9 - ok - if x[0]>110000 and x[0]<210000 and x[1]>70000 and x[1]<120000 and x[2]>40000 and x[2]<85000 and x[3]>20000 and x[3]<100000:     #ANALIZZA GLI ISTOGRAMMI SOPRA OTTENUTI DALLE IMMAGINI REALI E VEDI COME CAMBIARE QUA - classe 9 - AUMENTARE RANGE DI ISTOGRAMMA di 20 ogni parte\n","\n","'''\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXvQynrkw3lZ","executionInfo":{"status":"aborted","timestamp":1656671142715,"user_tz":-120,"elapsed":227,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["\"\"\"\n","'''#-----------------Generate images out of training--------------------'''\n","\n","print(#da cancellare quando vuoi usarlo\n","generator_off = Generator()\n","generator_off.load_state_dict(torch.load(\"/content/drive/MyDrive/CALCIO_CROP_BASE/models/model_generator_7_3200.pth\"))\n","\n","for i in range(20):\n","  z = Variable(Tensor(np.random.normal(0, i, (data.shape[0], opt[1][6]))))\n","  imgs_off = generator_off(z)\n","  save_image(imgs_off.data[:30], \"images_batch_7/{}_{}.png\".format(i,i), nrow=6, normalize=True)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"XnUgaUmoFykZ"},"source":["##### GAN with OPTUNA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16wFitvBFGga","executionInfo":{"status":"aborted","timestamp":1656671142717,"user_tz":-120,"elapsed":228,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["!pip install optuna -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tv8ztDxtb6Hq","executionInfo":{"status":"aborted","timestamp":1656671142718,"user_tz":-120,"elapsed":227,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["\"\"\"\n","import os\n","import optuna\n","from optuna.trial import TrialState\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data\n","from torchvision import datasets\n","from torchvision import transforms\n","\n","os.makedirs(\"images\", exist_ok=True)\n","os.makedirs(\"images_to_check\", exist_ok=True)\n","\n","\n","\n","\n","KERNEL_TYPE = \"rbf\"\n","\n","gan_history = {}\n","gan_history['DLOSS'] = []\n","gan_history['GLOSS'] = []\n","gan_history['MMD'] = []\n","gan_history['SSIM'] = []\n","gan_history['EPOCH'] = []\n","\n","opt = [[\"n_epochs - 0\", \"batch_size - 1\", \"lr - 2\", \"b1 - 3\", \"b2 - 4\", \"n_cpu - 5\", \"latent_dim - 6\", \"img_size - 7\", \"channels - 8\", \"n_critic - 9\", \"clip_value - 10\", \"sample_interval - 11\"],\n","      [400,              64,                0.0002,   0.5,      0.999,    8,           100,                128*2,             3,              4,              0.01,              20]]\n","\n","img_shape = (opt[1][8], 270, 470)\n","cuda = True if torch.cuda.is_available() else False\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","def objective2(trial):\n","    #--------------------------------------------------------------------------------------------------------------------------------------------\n","    # Loss function\n","    adversarial_loss = torch.nn.BCELoss()\n","\n","\n","    class GeneratorXXX(nn.Module):\n","        def __init__(self, trial_):\n","          super(GeneratorXXX, self).__init__()\n","          self.xxx=trial_#\n","          \n","          def block(in_feat, out_feat, normalize=True):     #forse aggiungi self come parametro\n","            layers = [nn.Linear(in_feat, out_feat)]\n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","          \n","          self.model = nn.Sequential(\n","            *block(self.xxx, 128, normalize=False), #.suggest_int(\"lat_dim_\", 1, 100)\n","            #*block(100, 128, normalize=False),\n","            *block(128, 256),\n","            *block(256, 512),\n","            *block(512, 1024),\n","            nn.Linear(1024, int(np.prod([img_shape]))),\n","            nn.Tanh())\n","        \n","        def forward(self, zy):\n","          img = self.model(zy)\n","          img = img.view(img.size(0), *img_shape)\n","          return img\n","\n","    '''----------------------------------------------'''\n","\n","\n","    class Discriminator(nn.Module):\n","        def __init__(self):\n","            super(Discriminator, self).__init__()\n","\n","            self.model = nn.Sequential(\n","                nn.Linear(int(np.prod(img_shape)), 512),\n","                nn.LeakyReLU(0.2, inplace=True),\n","                nn.Linear(512, 256),\n","                nn.LeakyReLU(0.2, inplace=True),\n","                nn.Linear(256, 1),\n","                nn.Sigmoid(),\n","            )\n","\n","        def forward(self, img):\n","            img_flat = img.view(img.size(0), -1)\n","            validity = self.model(img_flat)\n","\n","            return validity\n","\n","    '''----------------------------------------------'''\n","    # Initialize generator and discriminator  \n","    #generator2 = define_model_Gen1(trial) \n","    trial_lat_space = trial.suggest_int(\"lat_dim_\", 50, 500) #di solito viene impostato tra 1 e 100\n","    #print(trial_lat_space)\n","    #print('trial_lat_space {}'.format(trial_lat_space))\n","    #print('trial_lat_space {}'.format(type(trial_lat_space)))\n","    generator2 =  GeneratorXXX(trial_lat_space)\n","    #generator2 = Generator()\n","    discriminator2 = Discriminator()\n","\n","    if cuda:\n","        generator2.cuda()\n","        discriminator2.cuda()\n","        adversarial_loss.cuda()\n","\n","    # Configure data loader\n","    dataloader = trainloader\n","\n","    # Optimizers\n","\n","    # Generate the optimizers.\n","    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n","    lr_trial = trial.suggest_float(\"lr_\", 1e-6, 1e-4, log=True)                                     #di solito andare sopra a lr=0.0001 non porta buoni addestramenti\n","    optimizer_gg = getattr(optim, optimizer_name)(generator2.parameters(), lr=lr_trial)\n","    optimizer_dd = getattr(optim, optimizer_name)(discriminator2.parameters(), lr=lr_trial)\n","\n","\n","    if optimizer_name == \"Adam\":\n","        optimizer_G2 = torch.optim.Adam(generator2.parameters(), lr=lr_trial, betas=(trial.suggest_float(\"b1_\", 0.4, 0.6, log=True),  trial.suggest_float(\"b2_\", 0.8, 0.999, log=True)))\n","        optimizer_D2 = torch.optim.Adam(discriminator2.parameters(), lr=lr_trial, betas=(trial.suggest_float(\"b1_\", 0.4, 0.6, log=True),  trial.suggest_float(\"b2_\", 0.8, 0.999, log=True)))\n","    else:\n","        optimizer_G2=optimizer_gg\n","        optimizer_D2=optimizer_dd\n","\n","    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","    # ----------\n","    #  Training\n","    # ----------\n","\n","\n","    '''--------------------parametri per calcolare NDB score-----------------'''\n","    batches_done = 0\n","    real_batches = [] #to save real images to be used with NDB METRIC\n","    generated_batches = [] #to save real images to be used with NDB METRIC\n","    iters = 0\n","    img_list = []\n","    metrics = []\n","    ndb_scores = []\n","    jj = 0 #contatore per salvare tutte le immagini del batch da usare poi con metrica FID\n","\n","    gen_loss_tensor = torch.tensor([])\n","\n","\n","    '''estrazione batch di immagini reali per NDB'''\n","    \n","    real_combined = real_samples(numTrainBatches, dataloader, nc, img_shape[1],img_shape[2])      # bs x 3 x width x height\n","    '''----------------------------------------------------------------------'''  \n","\n","    #il KERNEL_TYPE = \"rbf\" serve per il MMD \n","    KERNEL_TYPE = \"rbf\"\n","\n","    for epoch in range(opt[1][0]):\n","        for i, (data, targets, targets2) in tqdm(enumerate(dataloader)):\n","\n","            data = data.to(device=device)\n","            targets = targets.to(device = device) #classes\n","            targets2 = targets2.to(device = device) #series\n","\n","\n","            # Adversarial ground truths\n","            valid = Variable(Tensor(data.size(0), 1).fill_(1.0), requires_grad=False)\n","            fake = Variable(Tensor(data.size(0), 1).fill_(0.0), requires_grad=False)\n","\n","            # Configure input\n","            real_imgs = Variable(data.type(Tensor))\n","\n","            # -----------------\n","            #  Train Generator\n","            # -----------------\n","\n","            optimizer_G2.zero_grad()\n","\n","            # Sample noise as generator input\n","            zy = Variable(Tensor(np.random.normal(0, 1, (data.shape[0],  trial_lat_space)))) \n","           \n","\n","            # Generate a batch of images\n","            gen_imgs = generator2(zy)\n","\n","\n","            # Loss measures generator's ability to fool the discriminator\n","            g_loss = adversarial_loss(discriminator2(gen_imgs), valid)\n","\n","            g_loss.backward()\n","            optimizer_G2.step()\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            optimizer_D2.zero_grad()\n","\n","            # Measure discriminator's ability to classify real from generated samples\n","            real_loss = adversarial_loss(discriminator2(real_imgs), valid)\n","            fake_loss = adversarial_loss(discriminator2(gen_imgs.detach()), fake)\n","            d_loss = (real_loss + fake_loss) / 2\n","\n","            d_loss.backward()\n","            optimizer_D2.step()\n","\n","            \n","\n","            print(\n","                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n","                % (epoch, opt[1][0], i, len(dataloader), d_loss.item(), g_loss.item())\n","            )\n","\n","            batches_done = epoch * len(dataloader) + i\n","            if batches_done % opt[1][11] == 0: \n","              #save_image(gen_imgs.data[:1], \"images/%d.png\" % batches_done, nrow=1, normalize=True)\n","              \n","              '''QUESTO SOTTO PER FARE DEI CONFRONTI SU IMMAGINI SALVATE PIÙ DI UNA INSIEME'''\n","              #save_image(gen_imgs.data[:5], \"images_to_check_tuning_hyperp/%d.png\" % batches_done, nrow=5, normalize=True)   #nrow = numero immagini per ciascuna riga\n","\n","\n","\n","        #--------------------------------------------------------------------------------------------\n","        #--------------------------------------------------------------------------------------------\n","\n","        '''PARTE DI CALCOLO DELL' MMD'''\n","        #---- Maximum Mean Discrepancy (MMD) (non capisco come farlo funzionare anche con le img a colori, se ti da qualche errore, prova prima a rieseguire il blocco subito sopra -----\n","        \n","        \n","        real_imgs2 =  real_imgs.reshape(real_imgs.shape[0], real_imgs.shape[1]*real_imgs.shape[2]*real_imgs.shape[3])\n","        #print(real_imgs2.shape)\n","        #print(type(real_imgs2))\n","\n","        gen_imgs2 =  gen_imgs.reshape(gen_imgs.shape[0], gen_imgs.shape[1]*gen_imgs.shape[2]*gen_imgs.shape[3])\n","        #print(gen_imgs2.shape)\n","        #print(type(gen_imgs2))\n","\n","        mmd_average = MMD(real_imgs2, gen_imgs2, KERNEL_TYPE, device) #---- aggiungere device nella funzione all'interno dell'file \n","        #print('MMD AVERAGE : {}'.format(mmd_average))\n","\n","        #--------------------------------------------------------------------------------------------\n","        #--------------------------------------------------------------------------------------------\n","\n","        '''PARTE DI CALCOLO DELLA FID'''\n","        '''SALVO TUTTE LE IMMAGINI GENERATE DAL BATCH COSÌ DA USARLE CON LA METRICA 'FID' '''\n","        '''\n","        for jj in range(len(gen_imgs)):\n","          save_image(gen_imgs.data[jj], \"images/%d.png\" % jj, nrow=1, normalize=True)   #nrow = numero immagini per ciascuna riga\n","        \n","        images1 = load_images('/content/drive/MyDrive/CALCIO_CROP_BASE/images/')\n","        images2 = load_images('/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0')\n","        BatchSize = 8 \n","        UseMultiprocessing = False \n","        \n","        #con queste 2 sotto non mi va\n","        #reals_ = np.transpose(real_combined, (0,2,3,1))     #la metrica FID vuole le immagini in ordine (bs x width x height x 3)\n","        #fakes_ = np.transpose(gen_combined, (0,2,3,1))     #la metrica FID vuole le immagini in ordine (bs x width x height x 3)\n","\n","        fid_value = calculate_fid(images1, images2, UseMultiprocessing, BatchSize)\n","        #print(\"fid_value :{}\".format(fid_value))\n","        '''\n","\n","\n","        '''AVERAGE LOSS DA MONITORARE'''\n","        average_loss = (d_loss + g_loss)/2\n","        b = torch.tensor([g_loss])\n","\n","        gen_loss_tensor = torch.cat((gen_loss_tensor, b), \n","\n","\n","\n","        '''memorizzo info su addestramento GAN'''\n","        gan_history['DLOSS'].append(d_loss.item())\n","        gan_history['GLOSS'].append(g_loss.item()) \n","        gan_history['EPOCH'].append(epoch)\n","        gan_history['MMD'].append(mmd_average.item())\n","        #gan_history['SSIM'].append(ris_ssim.item())\n","        \n","        \n","        '''queste 4 righe sotto non vanno se si monitorano più metriche insieme'''\n","        #trial.report(mmd_average, epoch)\n","\n","        # Handle pruning based on the intermediate value.\n","        #if trial.should_prune():\n","            #raise optuna.exceptions.TrialPruned()\n","\n","\n","\n","    #print(\"tensore di generator loss\")\n","    #print(gen_loss_tensor)\n","    var_gen_loss = torch.var(gen_loss_tensor, unbiased=False)\n","    diff_loss = abs(g_loss - d_loss)\n","    return var_gen_loss, g_loss, diff_loss        #fid_value (ci mette troppo, per usarlo togliere i load delle immagini se riesci ad adattare i tensori)\n","\n","\n","\n","'''MONITORARE MOLTEPLICI METRICHE PER VALUTARE LA GAN'''\n","#https://stackoverflow.com/questions/69071684/how-to-optimize-for-multiple-metrics-in-optuna\n","\n","\n","study = optuna.create_study(directions=[\"maximize\", \"minimize\", \"minimize\"])   #aggiungere un'altra metrica della loss, e anche altre 2 tipo LPIPS e SSIM/KL-Div\n","'''NOTE'''\n","#forse la varianza da massimizzare perchè altrimenti significa che non si sta allendando per niente\n","#aggiungi al posto della loss da minimizzare perchè prenderebbe l'ulitmo valore (la media della loss G)\n","\n","\n","\n","\n","study.optimize(objective2, n_trials=10)\n","\n","pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","print(\"Study statistics: \")\n","print(\"  Number of finished trials: \", len(study.trials))\n","print(\"  Number of pruned trials: \", len(pruned_trials))\n","print(\"  Number of complete trials: \", len(complete_trials))\n","\n","'''#per singolo parametro monitorato\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(\"  Value: \", trial.value)\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","  print(\"    {}: {}\".format(key, value))\n","'''\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EtlNBL0aIV7","executionInfo":{"status":"aborted","timestamp":1656671142719,"user_tz":-120,"elapsed":227,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["\"\"\"\n","#PARTE DI RICERCA EFFETTIVA DELLA COMBINAZIONE MIGLIORE DEGLI IPERPARAMETRI\n","trial = study.best_trials\n","\n","for count_trial in range(len(trial)):\n","  print(\"G_VARIANCE - G_MIN_LOSS - G-D LOSS\")\n","  print(\"  Values: {}\".format(trial[count_trial].values))\n","\n","  print(\"  Params: \")\n","  for key, value in trial[count_trial].params.items():\n","    print(\"    {}: {}\".format(key, value))\n","    print(\"\")\n","  print(\"-------------------------------------------\")\n","\n","print(trial)\n","print(\"\")\n","print(\"\")\n","print(trial[0].values)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"ig3uCmF7L-Zt"},"source":["#####ESEMPIO OPTUNA "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbjnE8a4EABP","executionInfo":{"status":"aborted","timestamp":1656671142721,"user_tz":-120,"elapsed":228,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["#https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py\n","\n","#https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.GridSampler.html?highlight=grid\n","\n","\"\"\"\n","Optuna example that optimizes multi-layer perceptrons using PyTorch.\n","\n","In this example, we optimize the validation accuracy of fashion product recognition using\n","PyTorch and FashionMNIST. We optimize the neural network architecture as well as the optimizer\n","configuration. As it is too time consuming to use the whole FashionMNIST dataset,\n","we here use a small subset of it.\n","\n","\"\"\"\n","\n","\"\"\"\n","import os\n","\n","import optuna\n","from optuna.trial import TrialState\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data\n","from torchvision import datasets\n","from torchvision import transforms\n","\n","\n","DEVICE = torch.device(\"cpu\")\n","BATCHSIZE = 128\n","CLASSES = 10\n","DIR = os.getcwd()\n","EPOCHS = 10\n","LOG_INTERVAL = 10\n","N_TRAIN_EXAMPLES = BATCHSIZE * 30\n","N_VALID_EXAMPLES = BATCHSIZE * 10\n","\n","\n","def define_model(trial):\n","    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n","    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n","    layers = []\n","\n","    in_features = 28 * 28\n","    for i in range(n_layers):\n","        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n","        layers.append(nn.Linear(in_features, out_features))\n","        layers.append(nn.ReLU())\n","        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n","        layers.append(nn.Dropout(p))\n","\n","        in_features = out_features\n","    layers.append(nn.Linear(in_features, CLASSES))\n","    layers.append(nn.LogSoftmax(dim=1))\n","\n","    return nn.Sequential(*layers)\n","\n","\n","def get_mnist():\n","    # Load FashionMNIST dataset.\n","    train_loader = torch.utils.data.DataLoader(\n","        datasets.FashionMNIST(DIR, train=True, download=True, transform=transforms.ToTensor()),\n","        batch_size=BATCHSIZE,\n","        shuffle=True,\n","    )\n","    valid_loader = torch.utils.data.DataLoader(\n","        datasets.FashionMNIST(DIR, train=False, transform=transforms.ToTensor()),\n","        batch_size=BATCHSIZE,\n","        shuffle=True,\n","    )\n","\n","    return train_loader, valid_loader\n","\n","\n","\n","\n","def objective(trial):\n","\n","    # Generate the model.\n","    model = define_model(trial).to(DEVICE)\n","\n","    # Generate the optimizers.\n","    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n","    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n","    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n","\n","    # Get the FashionMNIST dataset.\n","    train_loader, valid_loader = get_mnist()\n","    # Training of the model.\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            # Limiting training data for faster epochs.\n","            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n","                break\n","\n","            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = F.nll_loss(output, target)\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Validation of the model.\n","        model.eval()\n","        correct = 0\n","        with torch.no_grad():\n","            for batch_idx, (data, target) in enumerate(valid_loader):\n","                # Limiting validation data.\n","                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n","                    break\n","                data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n","                output = model(data)\n","                # Get the index of the max log-probability.\n","                pred = output.argmax(dim=1, keepdim=True)\n","                correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n","\n","        trial.report(accuracy, epoch)\n","\n","        # Handle pruning based on the intermediate value.\n","        if trial.should_prune():\n","            raise optuna.exceptions.TrialPruned()\n","\n","    return accuracy\n","\n","\n","\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=100, timeout=600)\n","\n","pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","print(\"Study statistics: \")\n","print(\"  Number of finished trials: \", len(study.trials))\n","print(\"  Number of pruned trials: \", len(pruned_trials))\n","print(\"  Number of complete trials: \", len(complete_trials))\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(\"  Value: \", trial.value)\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","  print(\"    {}: {}\".format(key, value))\n","\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"LXKsNF5hSloT"},"source":["##### PLOT GAN LOSS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgPY7gnjvdDX","executionInfo":{"status":"aborted","timestamp":1656671142722,"user_tz":-120,"elapsed":228,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["\"\"\"\n","DLOSS = gan_history['DLOSS']\n","GLOSS = gan_history['GLOSS']\n","EPOCH = gan_history['EPOCH']\n","\n","plt.figure(figsize=(50, 4), dpi=100)              #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n","plt.plot(EPOCH, DLOSS, 'b', label='DLOSS')\n","plt.plot(EPOCH, GLOSS, 'r', label='GLOSS ')\n","#plt.plot(EPOCH, ndb_scores, 'g', label='NDB_SCORES ')\n","plt.title('Training and validation accuracy (IMG)')\n","plt.legend()\n","plt.savefig(os.path.join(path_progettoDL+'weights/GANLOSS.pdf')) \n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"D5TSgRgRs96F"},"source":["###GAN METRICS DA CALCOLARE OFFLINE"]},{"cell_type":"markdown","metadata":{"id":"u6nY8B7VcfYj"},"source":["####PLOT FID METRIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rCssMXychf5","executionInfo":{"status":"aborted","timestamp":1656671142723,"user_tz":-120,"elapsed":227,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["'''ALTERNATIVA SCARICANDO LA VERSIONE IN GITHUB DI UN UTENTE'''\n","os.chdir('/content/drive/MyDrive/')\n","#!git clone https://github.com/hukkelas/pytorch-frechet-inception-distance.git fid_metric \n","\n","!python /content/drive/MyDrive/fid_metric/fid.py --path1 /content/drive/MyDrive/CALCIO_CROP_BASE/images/ --path2 /content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0 --batch-size 8\n","\n","\n","\n","\n","'''CALCOLO METRICA ATTRAVERSO CLASSE IMPLEMENTATA'''\n","images1 = load_images('/content/drive/MyDrive/CALCIO_CROP_BASE/images/')\n","images2 = load_images('/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0')\n","BatchSize = 8 \n","UseMultiprocessing = False \n","fid_value = calculate_fid(images1, images2, UseMultiprocessing, BatchSize)\n","print(fid_value)\n"]},{"cell_type":"markdown","metadata":{"id":"A_U_C71VdOAO"},"source":["####PLOT IS METRIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PqQamMndPmv","executionInfo":{"status":"aborted","timestamp":1656671142724,"user_tz":-120,"elapsed":226,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["print (\"Calculating Inception Score...\")\n","\n","\n","'''\n","train_c_dataset_2 = CustomDataset(df_class_new, transform_0=_transform_GAN) \n","trainloader_2 = torch.utils.data.DataLoader(dataset = train_c_dataset, batch_size=bs, shuffle=True)\n","\n","print(trainloader_2.dataset.dataframe)\n","'''\n","inception_score = (inception_score(trainloader, cuda=False, batch_size=32, resize=False, splits=1))\n","\n","print(f'Inception Score : {inception_score[0]:.5f}')"]},{"cell_type":"markdown","metadata":{"id":"ohUoctJUcmoa"},"source":["#### PLOT MMD METRIC - da sistemare il calcolo test post-allenamento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6NBlq-l0IT6","executionInfo":{"status":"aborted","timestamp":1656671142725,"user_tz":-120,"elapsed":226,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["MMD_history = gan_history['MMD']\n","EPOCH = gan_history['EPOCH']\n","\n","\n","#plt.plot(EPOCH, ndb_scores, 'b', label='NDB_SCORES ')\n","plt.plot(EPOCH, MMD_history, 'g', label='MMD')\n","plt.title('Maximum Mean Discrepancy')\n","plt.legend()\n","plt.savefig(os.path.join(path_progettoDL+'weights/GANLOSS_2.pdf')) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MI91GBplnjVQ","executionInfo":{"status":"aborted","timestamp":1656671142726,"user_tz":-120,"elapsed":226,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["\n","'''\n","----------------------------------------------------------------------------------Qui non ho avuto tempo di rivedere ....\n","'''\n","\n","\n","\n","#g1 = generator(z)\n","\n","#g2 =  g1.reshape(g1.shape[0], g1.shape[1]*g1.shape[2]*g1.shape[3])\n","\n","\n","#immagini reali\n","for i2, (data2, targets3, targets4) in tqdm(enumerate(trainloader)):\n","        data2 = data2.to(device=device)\n","        \n","        real1 = Variable(data2.type(Tensor))\n","\n","real2 =  real1.reshape(real1.shape[0], real1.shape[1]*real1.shape[2]*real1.shape[3])\n","\n","\n","#immagini generate - serve fare un csv delle sole immagini generate così da caricare solo quelle e darlo al genloader\n","for i2, (data2, targets3, targets4) in tqdm(enumerate(genloader)):\n","        data2 = data2.to(device=device)\n","        \n","        real1 = Variable(data2.type(Tensor))\n","\n","generated2 =  real1.reshape(real1.shape[0], real1.shape[1]*real1.shape[2]*real1.shape[3])\n","\n","\n","ris = MMD(real2, generated2, KERNEL_TYPE, device)  #COPIA DI SOPRA, CHE NON VA COMUNQUE!!!\n","\n","print(\"\")\n","print(\"MMD - post training\")\n","print(ris)\n"]},{"cell_type":"markdown","metadata":{"id":"RRGftpaXbdNE"},"source":["####PLOT NDB METRIC\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N11_KXvhH1CB","executionInfo":{"status":"aborted","timestamp":1656671142727,"user_tz":-120,"elapsed":226,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["'''RIADATTO TENSORI PER AVERLI IN UNA SPECIFICA FORMA/DIMENSIONE'''\n","\n","bs = 64         #DA MODIFICARE POI CON LUNGHEZZA VARIABILE\n","\n","\n","gen_combined = generated_fakes(numTestBatches, generator, img_shape[1], img_shape[2], bs, opt[1][6])\n","#gen_combined = generated_fakes(numTestBatches, generator2, img_shape[1], img_shape[2], trial_lat_space)\n","print(real_combined.shape)\n","print(gen_combined.shape)\n","train_size = numTrainBatches*bs     #perchè il set ha 62 immagini e non riempe neanche il batch\n","test_size = numTestBatches*bs\n","dim = img_shape[1]*img_shape[2]*nc\n","image_size = opt[1][7]\n","\n","train_samples = real_combined.reshape(train_size, dim)\n","test_samples = gen_combined.reshape(test_size, dim)\n","\n","print(train_samples.shape)\n","print(test_samples.shape)\n","\n","\n","\n","\n","\n","'''CALCOLO EFFETTIVO DELL'NDB E PLOT IN GRAFICO CON BINS'''\n","\n","ndb = NDB(training_data=train_samples, number_of_bins=k, whitening=True)\n","results = ndb.evaluate(test_samples, model_label='Test')\n","ndb.plot_results(models_to_plot=['Test'])\n","\n","\n","def get_generated(bin_id, results, num_imgs):\n","    indices = [i for i, x in enumerate(results[\"Bin-Assignment\"]) if x == bin_id]\n","    for i in range(len(indices[:num_imgs])):\n","        img = test_samples[indices[i]]\n","        img = img.reshape((nc, image_size, image_size))\n","        img = np.transpose(img,(1,2,0)).astype(np.float64)\n","\n","        plt.imshow(img)\n","        plt.show()\n","\n","\n","\n","\n","'''QUA SOTTO FA TUTTI DEI CONTI PER STAMPARE DELLE IMMAGINI DI ESEMPIO, MA NON SERVE PER IL 'NDB', MA SOLO PER VALUTARLO ANCORA MEGLIO'''\n","small_center = ndb.original_bin_centers[ndb.bin_order[-1]]\n","small_center = small_center.reshape((nc, img_shape[1], img_shape[2]))\n","small_center = np.transpose(small_center,(1,2,0)).astype(np.float64)\n","small_center = ((small_center * 0.5) + 0.5)\n","wdb_img = wandb.Image(small_center)\n","#wandb.log({\"smallest_bin_center\": wdb_img})\n","plt.imshow(small_center)\n","plt.show()\n","\n","#print out generated images in smallest bin\n","print(\"Smallest Bin Generated Images:\")\n","get_generated(ndb.bin_order[-1], results, 10)\n","\n","\n","#print out largest bin centroid\n","#center\n","large_center = ndb.original_bin_centers[ndb.bin_order[0]]\n","large_center = large_center.reshape((nc, img_shape[1], img_shape[2]))\n","large_center = np.transpose(large_center,(1,2,0)).astype(np.float64)\n","large_center = ((large_center * 0.5) + 0.5)\n","wdb_img = wandb.Image(large_center)\n","#wandb.log({\"largest_bin_center\": wdb_img})\n","plt.imshow(large_center)\n","plt.show()\n","\n","#print out generated images in largest bin\n","print(\"Largest Bin Generated Images:\")\n","get_generated(ndb.bin_order[0], results, 10)"]},{"cell_type":"markdown","metadata":{"id":"-GViGlheXfJt"},"source":["#### PLOT SSIM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o596X4T3QGBR","executionInfo":{"status":"aborted","timestamp":1656671142729,"user_tz":-120,"elapsed":226,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["'''CALCOLO SSIM'''\n","\n","#gen_combined = generated_fakes(numTestBatches, generator, img_shape[1], img_shape[2], bs, opt[1][6])\n","\n","#preparazione tensori\n","real_combined2 = torch.from_numpy(real_combined)\n","#gen_combined2 = torch.from_numpy(gen_combined)\n","\n","'''\n","#qua sempre con un genloader, vedi di creare il tensore come è stato fatto per real_combined\n","'''\n","\n","#calcolo metrica\n","ris_ssim = ssim(real_combined2,gen_combined2)\n","print(ris_ssim)"]},{"cell_type":"markdown","metadata":{"id":"shGoDK-cd70n"},"source":["####PLOT LPIPS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6bWxL3gd_F-","executionInfo":{"status":"aborted","timestamp":1656671142730,"user_tz":-120,"elapsed":225,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["'''\n","nel blocco di definizione delle metriche ci dovrebbe essere anche questo caso \n","ris_lpips = lpips(real_combined2, gen_combined2)\n","print(ris_lpips)\n","'''\n","print('guarda prima -> definizione metrica')\n","\n","\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/algoritmi_custom/GIT-LPIP')\n","!pip install -r requirements.txt --quiet\n","\n","!python lpips_2dirs.py -d0 /content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0/ -d1 /content/drive/MyDrive/CALCIO_CROP_BASE/images/ -o /content/drive/MyDrive/CALCIO_CROP_BASE/LPIPS_RESULTS.txt "]},{"cell_type":"markdown","metadata":{"id":"k-82Jf4Ndqdd"},"source":["####PLOT GS METRIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-0HBUn8ds5Y","executionInfo":{"status":"aborted","timestamp":1656671142730,"user_tz":-120,"elapsed":224,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["'''\n","ris = gs.geom_score(real_combined, gen_combined)\n","\n","rlts = gs.rlts(real_combined, gamma=1.0/128, n=100)\n","mrlt = np.mean(rlts, axis=0)\n","\n","gs.fancy_plot(mrlt, label='MRLT of 1')\n","plt.xlim([0, 30])\n","plt.legend()\n","'''"]},{"cell_type":"markdown","metadata":{"id":"NdslbUPRlioe"},"source":["####PLOT KL-DIV METRIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2Qp-VhohS3j","executionInfo":{"status":"aborted","timestamp":1656671142731,"user_tz":-120,"elapsed":223,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["m = nn.Softmax(dim=3)\n","output = m(real_combined2)\n","\n","print(output)\n","print(output.shape)\n","\n","print(output[0][0][0][:].sum())\n","ris = torch.sum(output[0][0][0][:])\n","print(ris)\n","\n","output2 = m(gen_combined2)\n","print(output2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ST65CEpfll_U","executionInfo":{"status":"aborted","timestamp":1656671142732,"user_tz":-120,"elapsed":222,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["'''\n","P = torch.Tensor([0.36, 0.48, 0.16])\n","Q = torch.Tensor([0.333, 0.333, 0.333])\n","\n","(P * (P / Q).log()).sum()\n","# tensor(0.0863), 10.2 µs ± 508\n","\n","ris1 = F.kl_div(Q.log(), P, None, None, 'sum')\n","'''\n","'''\n","real_combined3 = torch.Tensor(real_combined2)\n","gen_combined3 = torch.Tensor(gen_combined2)\n","\n","#(real_combined3 * (real_combined3 / gen_combined3.log()).sum()\n","'''\n","\n","ris1 = F.kl_div(output2.log(), output, None, None, 'sum')\n","\n","# tensor(0.0863), 14.1 µs ± 408 ns\n","print(ris1)\n","\n","\n","\n","print('controlla bene la metrica, è stata implementata con diverse alternative sopra')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBFGY72Xq1RP","executionInfo":{"status":"aborted","timestamp":1656671142733,"user_tz":-120,"elapsed":221,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["'''\n","# calculate (P || Q)\n","kl_pq = rel_entr(real_combined3, gen_combined3)\n","print(kl_pq.shape)\n","print('KL(P || Q): %.3f nats' % sum(kl_pq))\n","# calculate (Q || P)\n","kl_qp = rel_entr(gen_combined3, real_combined3)\n","print('KL(Q || P): %.3f nats' % sum(kl_qp))\n","'''\n","\n","\n","from math import log2\n","# calculate the kl divergence\n","def kl_divergence(p, q):\n","\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n","\n","# calculate the js divergence\n","def js_divergence(p, q):\n","\tm = 0.5 * (p + q)\n","\treturn 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n"," \n","print(kl_divergence(real_combined3,gen_combined3))\n","\n","print(kl_divergence(p,q))"]},{"cell_type":"markdown","metadata":{"id":"-zMdWtZif_ui"},"source":["####----------------------------------------------------------------------------------------------------------------\n","##RIEPILOGO RISULTATI DI TUTTE LE METRICHE per GAN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71eBHEdWgHb2","executionInfo":{"status":"aborted","timestamp":1656671142734,"user_tz":-120,"elapsed":220,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["#QUA STAMPIAMO I VALORI CHE MEMORIZZIAMO IN UN VETTORE E GLIELI STAMPIAMO PER RIEPILOGO\n","\n","\"\"\"\n","AGGIUNGERE IL VETTORE DALLA PRIMA METRICA CHE MEMORIZZA I VALORI E POI CREA UNA STAMPA QUA \n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"Hosx74qRSs31"},"source":["## TESTS CONVERT GRAYSCALE TO RGB "]},{"cell_type":"markdown","metadata":{"id":"xildnrRfoxil"},"source":["##### import vari "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZytFp2O7okT-","executionInfo":{"status":"aborted","timestamp":1656671142735,"user_tz":-120,"elapsed":219,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["import torch\n","import torchvision.transforms as T\n","from PIL import Image\n","from PIL import ImageMath"]},{"cell_type":"markdown","metadata":{"id":"tbPUx79ZozKx"},"source":["##### prove varie "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAXoqpRIohNy","executionInfo":{"status":"aborted","timestamp":1656671142736,"user_tz":-120,"elapsed":220,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["#ESPERIMENTI CONVERSIONE COLORI NON FUNZIONANTE\n","#https://machinelearningknowledge.ai/ways-to-convert-image-to-grayscale-in-python-using-skimage-pillow-and-opencv/\n","'''\n","img_path = os.path.join(path_images+\"images/new_652.png\")\n","image = io.imread(img_path)\n","io.imshow(image)\n","plt.show()\n","print(image.shape)\n","\n","imgGray = io.color.gray2rgb(image)\n","'''\n","####################\n","'''\n","img_path = os.path.join('/content/drive/MyDrive/CALCIO_CROP_BASE/20200506124834.png') \n","image = io.imread(img_path)\n","io.imshow(image)\n","plt.show()\n","'''\n","####################\n","'''\n","from PIL import Image\n","with Image.open('/content/drive/MyDrive/CALCIO_CROP_BASE/images/new_600.png') as im:\n","  print(im.getchannel)\n","  im.convert('RGB').save('/content/drive/MyDrive/CALCIO_CROP_BASE/images/den.png')\n","'''\n","####################\n","'''\n","trans = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n","transform = T.ToPILImage()\n","\n","x = torch.randn(3, 224, 224)\n","out = trans(x)\n","print(out.shape)\n","img = transform(out)\n","img.show()\n","img #per stampare l'immagine, img.show() non fa !!!!\n","'''\n","####################\n","''' \n","#COSi FUNZIONA -- fa solo il load e lo stampa \n","from PIL import Image\n","img = Image.open('/content/drive/MyDrive/CALCIO_CROP_BASE/images/new_600.png')\n","img\n","'''\n","####################\n","''' cose utili '''\n","'''\n","# define a transform to convert a tensor to PIL image\n","transform = T.ToPILImage()\n","# convert the tensor to PIL image using above transform\n","img = transform(tensor)\n","'''\n","####################\n","'''\n","#usando OPENCV \n","#import cv2_imshow\n","print('USANDO OPEN CV ')\n","import cv2\n","img = cv2.imread('/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0/patch_20200506123315.png', 0) # il 0 serve per leggerla in grayscale, se non lo metti fa direttamente RGB\n","print(img.shape)\n","cv2_imshow(img)\n","backtorgb = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n","print(backtorgb.shape)\n","cv2_imshow(backtorgb)\n","print('--------------------------------------')\n","\n","#altre prove .... ma nada\n","\n","# Open and ensure in RGB mode - in case image is palettised\n","im = Image.open('/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0/patch_20200506123315.png')\n","print(im.size)\n","print('canali {} '.format(im.getchannel(1)))\n","print('canali R {} '.format(im.getchannel('R')))\n","print('canali G {} '.format(im.getchannel('G')))\n","print('canali B {} '.format(im.getchannel('B')))\n","\n","#matrix = (0.2, 0.5, 0.3, 0.0, 0.2, 0.5, 0.3, 0.0, 0.2, 0.5, 0.3, 0.0)\n","#im = im.convert('RGB',matrix)\n","#print(im.size)\n","\n","\n","img = Image.open('/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0/patch_20200506123315.png').convert('RGB')\n","rgbimg = Image.new(\"RGBA\", img.size)\n","rgbimg.paste(img)\n","rgbimg.save('foo.png')\n","'''"]},{"cell_type":"markdown","metadata":{"id":"eEMosgpdo2Hj"},"source":["##### metodo 1 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SKsuNAN-xHZD","executionInfo":{"status":"aborted","timestamp":1656671142738,"user_tz":-120,"elapsed":220,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["'''\n","il blocco sotto sembra funzionare ... ma le img non sono come quelle reali, la matrix non ho ben capito come funziona, ma cambiano i numeri si aggiungono i colori \n","'''\n","\n","from PIL import Image\n","# Open and ensure in RGB mode - in case image is palettised\n","im = Image.open('/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0/patch_20200506123315.png').convert('RGB')\n","# Crude conversion to black and white using 20% red, 50% green and 30% blue\n","matrix = (1.0, 0.8, 0.5, 0.0,\n","          0.5, 1.0, 0.0, 0.0,\n","          0.0, 0.4, 1.0, 0.3) #cambiando i numeri, escono fuori dei colori ... quindi forse si può fare !!!! \n","result = im.convert('RGB',matrix)\n","result.save('result.png')\n","result"]},{"cell_type":"markdown","metadata":{"id":"d45MkW5no5oD"},"source":["##### metodo 2 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfhEGlDooNb-","executionInfo":{"status":"aborted","timestamp":1656671142739,"user_tz":-120,"elapsed":220,"user":{"displayName":"Denis Bernovschi","userId":"01730545031398802449"}}},"outputs":[],"source":["'''\n","il blocco sotto sembra funzionare ... ma le img non sono come quelle reali, il CMAP le crea in base alla mappa scelta \n","'''\n","\n","#LINK : https://matplotlib.org/stable/tutorials/colors/colormaps.html \n","import numpy as np\n","from PIL import Image\n","from matplotlib import cm\n","\n","# Get 256 entries from \"viridis\" or any other Matplotlib colormap\n","colmap = cm.get_cmap('inferno', 256) \n","\n","# Make a Numpy array of the 256 RGB values\n","# Each line corresponds to an RGB colour for a greyscale level\n","np.savetxt('cmap.csv', (colmap.colors[...,0:3]*255).astype(np.uint8), fmt='%d', delimiter=',')\n","\n","# Load image as greyscale and make into Numpy array\n","grey = np.array(Image.open('/content/drive/MyDrive/CALCIO_CROP_BASE/patches/class9.0/patch_20200506123315.png').convert('L'))\n","Image.fromarray(grey).save('before_result.png')\n","\n","# Load RGB LUT from CSV file\n","lut = np.loadtxt('/content/drive/MyDrive/CALCIO_CROP_BASE/cmap.csv', dtype=np.uint8, delimiter=',')\n","\n","# Make output image, same height and width as grey image, but 3-channel RGB\n","result = np.zeros((*grey.shape,3), dtype=np.uint8)\n","\n","# Take entries from RGB LUT according to greyscale values in image\n","np.take(lut, grey, axis=0, out=result)\n","# Save result\n","Image.fromarray(result).save('result.png')"]},{"cell_type":"markdown","metadata":{"id":"UoMC9ZJLkLm7"},"source":["## TRAIN VGG - CLASSIFICATORE "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMXAiMAvyom5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"12b697b7-3219-4ab2-cf66-7b72d2a640e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["train() called: model=VGG, opt=SGD, epochs=30, device=cpu\n","\n","-----------------------------------------------------------------\n","Inizio Epoch : 1\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 1 train batch: 33it [10:29, 19.07s/it]\n","Epoch : 1 val batch: 7it [00:43,  6.16s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 1 di 30, train loss : 2.238520203284847 , train acc : 0.27621359223300973, val loss : 2.2493576212516895, val acc : 0.3791469194312796, bal acc : 0.40476190476190477 , val bal acc : 0.32407407407407407 \n","Val Balance Accuracy Increase (inf --> 0.324074).  Saving model ...\n","-----------------------------------------------------------------\n","Inizio Epoch : 2\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 2 train batch: 33it [12:43, 23.13s/it]\n","Epoch : 2 val batch: 7it [00:49,  7.05s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 2 di 30, train loss : 2.0482983809072994 , train acc : 0.5029126213592233, val loss : 2.057829460261557, val acc : 0.45734597156398105, bal acc : 0.625 , val bal acc : 0.5082621082621084 \n","Val Balance Accuracy Increase (0.324074 --> 0.508262).  Saving model ...\n","-----------------------------------------------------------------\n","Inizio Epoch : 3\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 3 train batch: 33it [12:51, 23.37s/it]\n","Epoch : 3 val batch: 7it [00:48,  6.89s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 3 di 30, train loss : 1.935387050295339 , train acc : 0.6087378640776699, val loss : 2.0005309491360923, val acc : 0.504739336492891, bal acc : 0.5 , val bal acc : 0.5052380952380953 \n","EarlyStopping counter: 1 out of 10\n","-----------------------------------------------------------------\n","Inizio Epoch : 4\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 4 train batch: 33it [12:46, 23.24s/it]\n","Epoch : 4 val batch: 7it [00:48,  6.88s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 4 di 30, train loss : 1.8799335218170314 , train acc : 0.658252427184466, val loss : 1.9750273510178118, val acc : 0.514218009478673, bal acc : 0.6666666666666666 , val bal acc : 0.3845238095238095 \n","EarlyStopping counter: 2 out of 10\n","-----------------------------------------------------------------\n","Inizio Epoch : 5\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 5 train batch: 33it [12:46, 23.23s/it]\n","Epoch : 5 val batch: 7it [00:47,  6.78s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 5 di 30, train loss : 1.8282855459787313 , train acc : 0.7072815533980582, val loss : 1.957453433371268, val acc : 0.5402843601895735, bal acc : 0.6666666666666666 , val bal acc : 0.6434027777777778 \n","Val Balance Accuracy Increase (0.508262 --> 0.643403).  Saving model ...\n","-----------------------------------------------------------------\n","Inizio Epoch : 6\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 6 train batch: 33it [12:51, 23.37s/it]\n","Epoch : 6 val batch: 7it [00:48,  6.88s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 6 di 30, train loss : 1.78996706587597 , train acc : 0.7432038834951457, val loss : 1.944737381844724, val acc : 0.556872037914692, bal acc : 0.5555555555555556 , val bal acc : 0.5633333333333332 \n","EarlyStopping counter: 1 out of 10\n","-----------------------------------------------------------------\n","Inizio Epoch : 7\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 7 train batch: 33it [12:54, 23.46s/it]\n","Epoch : 7 val batch: 7it [00:48,  6.89s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 7 di 30, train loss : 1.7624403981329169 , train acc : 0.7660194174757281, val loss : 1.932170087692297, val acc : 0.5710900473933649, bal acc : 0.6875 , val bal acc : 0.5688888888888889 \n","EarlyStopping counter: 2 out of 10\n","-----------------------------------------------------------------\n","Inizio Epoch : 8\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 8 train batch: 33it [12:45, 23.20s/it]\n","Epoch : 8 val batch: 7it [00:47,  6.77s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 8 di 30, train loss : 1.7172123152075462 , train acc : 0.8033980582524272, val loss : 1.9256410124177616, val acc : 0.5687203791469194, bal acc : 0.7916666666666666 , val bal acc : 0.6916666666666667 \n","Val Balance Accuracy Increase (0.643403 --> 0.691667).  Saving model ...\n","-----------------------------------------------------------------\n","Inizio Epoch : 9\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 9 train batch: 33it [12:50, 23.35s/it]\n","Epoch : 9 val batch: 7it [00:47,  6.84s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 9 di 30, train loss : 1.7019238867806 , train acc : 0.8140776699029126, val loss : 1.915253049389446, val acc : 0.5853080568720379, bal acc : 0.6666666666666666 , val bal acc : 0.7196428571428571 \n","Val Balance Accuracy Increase (0.691667 --> 0.719643).  Saving model ...\n","-----------------------------------------------------------------\n","Inizio Epoch : 10\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 10 train batch: 33it [12:49, 23.32s/it]\n","Epoch : 10 val batch: 7it [00:48,  6.94s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 10 di 30, train loss : 1.6813470729346414 , train acc : 0.8330097087378641, val loss : 1.9160471380604387, val acc : 0.5829383886255924, bal acc : 1.0 , val bal acc : 0.6933333333333332 \n","EarlyStopping counter: 1 out of 10\n","-----------------------------------------------------------------\n","Inizio Epoch : 11\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 11 train batch: 33it [13:01, 23.68s/it]\n","Epoch : 11 val batch: 7it [00:48,  6.97s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 11 di 30, train loss : 1.6793466336518816 , train acc : 0.829611650485437, val loss : 1.9186677232172817, val acc : 0.5734597156398105, bal acc : 0.7777777777777777 , val bal acc : 0.4854166666666666 \n","EarlyStopping counter: 2 out of 10\n","Saved model to disk\n","-----------------------------------------------------------------\n","Inizio Epoch : 12\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 12 train batch: 33it [12:58, 23.58s/it]\n","Epoch : 12 val batch: 7it [00:48,  6.91s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 12 di 30, train loss : 1.665367603302002 , train acc : 0.8393203883495146, val loss : 1.9110970344588656, val acc : 0.5734597156398105, bal acc : 0.8333333333333334 , val bal acc : 0.5561904761904761 \n","EarlyStopping counter: 3 out of 10\n","-----------------------------------------------------------------\n","Inizio Epoch : 13\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 13 train batch: 33it [12:54, 23.47s/it]\n","Epoch : 13 val batch: 7it [00:47,  6.76s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch : 13 di 30, train loss : 1.6459010445955888 , train acc : 0.8611650485436894, val loss : 1.9068334113930074, val acc : 0.5829383886255924, bal acc : 0.8214285714285714 , val bal acc : 0.6055555555555555 \n","EarlyStopping counter: 4 out of 10\n","-----------------------------------------------------------------\n","Inizio Epoch : 14\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 14 train batch: 33it [12:17, 22.34s/it]\n","Epoch : 14 val batch: 0it [00:00, ?it/s]"]}],"source":["from sklearn.preprocessing import LabelBinarizer\n","import time\n","\n","#per progress bar\n","from tqdm import tqdm\n","\n","#LINK UTILE : https://stackoverflow.com/questions/59584457/pytorch-is-there-a-definitive-training-loop-similar-to-keras-fit\n","'''\n","LINK UTILE PER CREARE UN OGGETO DI CLASSE Trainer, CHE OTTIMIZZA IL PROCESSO DI TRAINING INCORPORANDO ANCHE LE CALLBACKS\n","https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer\n","'''\n","\n","#per ignorare i vari warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss() #sarebbere la nostra loss function \n","optimizer = optimizer_ #opt definito all'interno del blocco Hyper-Parameters \n","\n","\n","history = {} # Collects per-epoch loss and acc like Keras' fit().\n","history['loss'] = []\n","history['val_loss'] = []\n","history['acc'] = []\n","history['val_acc'] = []\n","history['bal_acc'] = []\n","history['val_bal_acc'] = []\n","\n","start_time_sec = time.time()\n","losses = [] \n","\n","early_stopping = _EarlyStopping(patience=10, verbose=True, path = path_drive+'ProgettoDL/pytorch_model')\n","\n","print('train() called: model={}, opt={}, epochs={}, device={}\\n'.\n","      format(type(model).__name__,type(optimizer).__name__, epochs, device))\n","\n","for epoch in range(0, epochs):\n","    print('-----------------------------------------------------------------')\n","    print('Inizio Epoch : {}'.format(epoch+1))\n","    #alleno il modello \n","    model.train()\n","    train_loss         = 0.0\n","    num_train_correct  = 0\n","    num_train_examples = 0\n","    \n","    for batch_idx, (data, targets, targets2) in tqdm(enumerate(trainloader), desc = 'Epoch : {} train batch'.format(epoch+1)):\n","      data = data.to(device=device)\n","      targets = targets.to(device = device) #classes\n","      targets2 = targets2.to(device = device) #series\n","\n","      # Clear the gradients\n","      optimizer.zero_grad()\n","      # Forward Pass\n","      scores = model(data)\n","      # Find the Loss\n","      loss = criterion(scores,targets)\n","      # Calculate gradients \n","      loss.backward()\n","      # Update Weights\n","      optimizer.step()\n","      # Calculate Loss\n","      train_loss += loss.item()  * data.size(0)\n","      num_train_correct  += (torch.max(scores, 1)[1] == targets).sum().item()\n","      num_train_examples += data.shape[0]   \n","   \n","\n","    #print('num_train_correct {}'.format(num_train_correct))\n","    #print('num_train_examples {}'.format(num_train_examples))\n","\n","    train_acc   = num_train_correct / num_train_examples\n","    train_loss  = train_loss / len(trainloader.dataset.dataframe)\n","    #train_loss  = train_loss / sampler_.number_of_samples\n","\n","    bal_acc = _bal_acc_(targets,scores)\n","\n","    model.eval()\n","    val_loss       = 0.0\n","    num_val_correct  = 0\n","    num_val_examples = 0\n","\n","    for batch_idx, (data, targets, targets2) in tqdm(enumerate(valloader), desc = 'Epoch : {} val batch'.format(epoch+1)):\n","      data = data.to(device=device)\n","      targets = targets.to(device = device) #classes\n","      targets2 = targets2.to(device = device) #series\n","\n","      scores = model(data)\n","      loss = criterion(scores,targets)\n","\n","      val_loss         += loss.data.item()  * data.size(0)\n","      num_val_correct  += (torch.max(scores, 1)[1] == targets).sum().item()\n","      num_val_examples += scores.shape[0]\n","    \n","    #print('num_val_correct {}'.format(num_val_correct))\n","    #print('num_val_examples {}'.format(num_val_examples))\n","    val_acc  = num_val_correct / num_val_examples\n","    val_loss = val_loss / len(valloader.dataset.dataframe)\n","    val_bal_acc = _bal_acc_(targets,scores)\n","    \n","\n","    history['loss'].append(train_loss)\n","    history['val_loss'].append(val_loss)\n","    history['acc'].append(train_acc)\n","    history['val_acc'].append(val_acc)\n","    history['bal_acc'].append(bal_acc)\n","    history['val_bal_acc'].append(val_bal_acc)\n","\n","\n","    print('Epoch : {} di {}, train loss : {} , train acc : {}, val loss : {}, val acc : {}, bal acc : {} , val bal acc : {} ' \n","          . format(epoch+1,epochs, train_loss, train_acc, val_loss, val_acc, bal_acc, val_bal_acc))\n","   \n","    \n","    #early_stopping(val_loss, model)\n","    early_stopping(val_bal_acc, model)\n","        \n","    if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        break\n","\n","    if epoch%10 == 0 and epoch>5:\n","      #salvataggio modello pesi finali\n","      path = path_progettoDL+'pytorch_model_GAN_3_{}'.format(epoch)\n","      torch.save(model.state_dict(), os.path.join(path))\n","      print(\"Saved model to disk\")\n","\n","\n","end_time_sec = time.time()\n","total_time_sec = end_time_sec - start_time_sec\n","time_per_epoch_sec = total_time_sec / epochs\n","print('Time total:     %5.2f sec' % (total_time_sec))\n","print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n"]},{"cell_type":"markdown","metadata":{"id":"xBGTOuc1VSqq"},"source":["## PLOT "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQzh_8drJ56m"},"outputs":[],"source":["'''PLOT CURVES'''\n","import datetime\n","path = path_progettoDL\n","\n","\n","data_ora = datetime.datetime.now()\n","\n","acc = history['acc']\n","val_acc = history['val_acc']\n","loss = history['loss']\n","val_loss = history['val_loss']\n","bal_acc = history['bal_acc']\n","val_bal_acc = history['val_bal_acc']\n","lista = [acc,val_acc,loss,val_loss, bal_acc, val_bal_acc]\n","\n","import csv\n","os.chdir(path_progettoDL+'weights/')\n","with open(\"VGG16.csv\", \"w\", newline=\"\") as f:\n","    writer = csv.writer(f)\n","    writer.writerows(lista)\n","     \n","len_epochs = range(len(acc))\n","\n","plt.plot(len_epochs, acc, 'b', label='Training acc')\n","plt.plot(len_epochs, val_acc, 'r', label='Validation acc')\n","plt.title('Training and validation accuracy (IMG)')\n","plt.legend()\n","plt.savefig(os.path.join(path_progettoDL+'weights/PlotAcc_{}_{}.pdf'.format(type_img,data_ora))) \n","\n","plt.figure()\n"," \n","plt.plot(len_epochs, loss, 'b', label='Training loss')\n","plt.plot(len_epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and validation loss (IMG)')\n","plt.legend()\n","plt.savefig(os.path.join(path_progettoDL+'weights/PlotLoss_{}_{}.pdf'.format(type_img,data_ora)))\n","\n","#plt.figure(figsize=(20, 6), dpi=80)\n","plt.figure()\n","\n","plt.plot(len_epochs, bal_acc, 'b', label='Training Balance Accuracy')\n","plt.plot(len_epochs, val_bal_acc, 'r', label='Validation Balance Accuracy')\n","plt.title('Training and validation balance accuracy (IMG)')\n","plt.legend()\n","plt.savefig(os.path.join(path_progettoDL+'weights/PlotBalAcc_{}_{}.pdf'.format(type_img,data_ora)))"]},{"cell_type":"markdown","metadata":{"id":"QjI6Y_nefFTj"},"source":["## SAVE MODEL \n","\n","https://pytorch.org/tutorials/beginner/saving_loading_models.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApFaEaa6oPu0"},"outputs":[],"source":["#salvataggio modello pesi finali\n","path = path_progettoDL+'pytorch_model_GAN_IMB'\n","torch.save(model.state_dict(), os.path.join(path))\n","print(\"Saved model to disk\")"]},{"cell_type":"markdown","metadata":{"id":"XefnmV_tsbOw"},"source":["## LOAD MODEL "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3VDiBcm-XEOb"},"outputs":[],"source":["path = path_progettoDL+'pytorch_model_GAN_3_20'\n","model.load_state_dict(torch.load(path))\n","model.eval()\n","print('Model IMG Loaded')"]},{"cell_type":"markdown","metadata":{"id":"HQMtb_JBD5Vp"},"source":["## PREDICTION "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4H0bpu6kjiui"},"outputs":[],"source":["num_test_correct  = 0\n","num_test_examples = 0\n","#da rimuovere dopo \n","#criterion = nn.CrossEntropyLoss()\n","\n","scores_, targets_ = list(), list()\n","\n","for (data, targets, targets2) in (testloader):\n","  data = data.to(device=device)\n","  targets = targets.to(device = device) #classes\n","  targets2 = targets2.to(device = device) #series\n","\n","  scores = model(data)\n","  \n","  num_test_correct  += (torch.max(scores, 1)[1] == targets).sum().item()\n","  num_test_examples += scores.shape[0]\n","\n","  #scores_.append(torch.max(scores, 1)[1])\n","  scores_ = np.append(scores_, torch.max(scores, 1)[1].detach().numpy())\n","  targets_ = np.append(targets_, targets.detach().numpy())\n","\n","#print('num_val_correct {}'.format(num_val_correct))\n","#print('num_val_examples {}'.format(num_val_examples))\n","\n","test_acc  = num_test_correct / num_test_examples\n","test_bal_acc = balanced_accuracy_score(targets_, scores_)\n","\n","print('Accuracy : {:.4f}, balance accuracy : {:.4f}'. format(test_acc, test_bal_acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srwp3LLlMqXW"},"outputs":[],"source":["# qui ho cercato di capire quanti scores_ e targets_ sono uguali tra loro\n","K=0\n","K += (scores_ == targets_).sum().item()\n","print(f'predizione corrette : {K}')"]},{"cell_type":"markdown","metadata":{"id":"lnkHiR9qSc9t"},"source":["## IDEA ALEX "]},{"cell_type":"markdown","metadata":{"id":"SbTAkQREZudC"},"source":["**IDEA**:\n","PER MIGLIORARE L'ALGORITMO POTREMMO METTERE UNA LOGICA AGGIUNTIVA:\n","\n","***IPOTESI*** SE VETTORE DELLE PROBABILITÀ DELLE PREDICTION IN FASE DI TRAINING HA COME VALORE MAGGIORE (DOVE VALE argmax) UN VALORE INFERIORE A 0.25-0.3 ALLORA PROBABILMENTE LE PROBABILITÀ SONO MOLTE DISTRIBUITE PERCHÈ LA RETE NEURALE LA STA PREDICENDO MALE, QUINDI POSSIAMO FORSE SUPPORRE CHE QUELL'IMMAGINE PUÒ TROVARSI FORSE (COME CLASSE DI QUALITÀ IN ZONA INTERMEDIA), QUANDO INVECE POTREBBE AVER DECISO MALE DI TROVARSI NEGLI ESTREMI (I.E. 1,2-, 3+,4).\n","**AZIONE** IO PROVEREI QUINDI A STAMPARE TUTTI I VETTORI OGNI QUAL VOLTA IL VALORE DI ARGMAX SIA INFERIRORE A 0.25-0.3 E AIUTARLO IN MODO SUPERVISIONATO, METTENDO MAGARI DELLE CONDIZIONI PER FARGLI CAMBIARE IDEA POSTO DECISIONE DELLA RETE NEURALE, O ADDIRITTURA CERCANDO UN MODO PER AGGIORNARE I PESI.\n","ALTRIMENTI SEMPRE LATO UTENTE SI PUÒ PROVARE A MODIFICARE L'OUTPUT DELLA PREDIZIONE\n","**NOTA:** OVVIAMENTE PRIMA DI PROCEDERE SU QUESTA STRADA STAMPARE TUTTI I POSSIBILI VETTORI CON LA RELATIVA LABEL REALE E LABEL PREDETTA, COSÌ DA VEDERE SE QUESTA COSA FOSSE MOLTO RICORRENTE\n"]},{"cell_type":"markdown","metadata":{"id":"VkaCQp4Ndzbe"},"source":["##SEARCH UNIVOQUE SERIES TO BALANCE SETS (DA CONTROLLARE SERVE PER LE CM) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkviN4oh2M6l"},"outputs":[],"source":["#SEARCHING UNIVOQUE SERIES\n","test_array_s0, test_array_s1, test_array_s2, test_array_s3, test_array_s4, test_array_s5, test_array_s6, test_array_s7, test_array_s8, test_array_s9, test_array_s10, test_array_s11, test_array_s12 = [], [], [], [], [], [], [], [], [], [], [], [], []\n","pred_array_s0, pred_array_s1, pred_array_s2, pred_array_s3, pred_array_s4, pred_array_s5, pred_array_s6, pred_array_s7, pred_array_s8, pred_array_s9, pred_array_s10, pred_array_s11, pred_array_s12 = [], [], [], [], [], [], [], [], [], [], [], [], []\n","i=0\n","for index, row in test_balance_df.iterrows():\n","    \n","    series_ = int(row['series'])\n","    if series_ == 0:\n","      test_array_s0.append(targets_[i])\n","      pred_array_s0.append(scores_[i])\n","    if series_ == 1:\n","      test_array_s1.append(targets_[i])\n","      pred_array_s1.append(scores_[i])\n","    if series_ == 2:\n","      test_array_s2.append(targets_[i])\n","      pred_array_s2.append(scores_[i])\n","    if series_ == 3:\n","      test_array_s3.append(targets_[i])\n","      pred_array_s3.append(scores_[i])\n","    if series_ == 4:\n","      test_array_s4.append(targets_[i])\n","      pred_array_s4.append(scores_[i])\n","    if series_ == 5:\n","      test_array_s5.append(targets_[i])\n","      pred_array_s5.append(scores_[i])\n","    if series_ == 6:\n","      test_array_s6.append(targets_[i])\n","      pred_array_s6.append(scores_[i])\n","    if series_ == 7:\n","      test_array_s7.append(targets_[i])\n","      pred_array_s7.append(scores_[i])\n","    if series_ == 8:\n","      test_array_s8.append(targets_[i])\n","      pred_array_s8.append(scores_[i])\n","    if series_ == 9:\n","      test_array_s9.append(targets_[i])\n","      pred_array_s9.append(scores_[i])\n","    if series_ == 10:\n","      test_array_s10.append(targets_[i])\n","      pred_array_s10.append(scores_[i])\n","    if series_ == 11:\n","      test_array_s11.append(targets_[i])\n","      pred_array_s11.append(scores_[i])\n","    if series_ == 12:\n","      test_array_s12.append(targets_[i])\n","      pred_array_s12.append(scores_[i])\n","\n","    i=i+1\n","\n","print(test_array_s0)\n","print(pred_array_s0)\n","\n","from functools import reduce\n","reduced = reduce(np.union1d, (pred_array_s0, test_array_s0))\n","print(reduced)"]},{"cell_type":"markdown","metadata":{"id":"D8WKlbpqKgAP"},"source":["## METRICHE MASK & IMG "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHSjdjJcgc1Y"},"outputs":[],"source":["'''METRICHE'''\n","print('--------------Metrice IMG----------------')\n","#print(y_test)\n","#print(y_pred)\n","# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html                                  \n","print(\"test accuracy  : {:.4f}\".format(accuracy_score(targets_, scores_) ))\n","# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html?highlight=precision_score#sklearn.metrics.precision_score\n","print(\"precision  : {:.4f}\".format(precision_score(targets_, scores_, average=\"macro\")))\n","# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall_score#sklearn.metrics.recall_score         \n","print(\"recall : {:.4f}\".format(recall_score(targets_, scores_ , average=\"macro\")))\n","# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1_score#sklearn.metrics.f1_score             \n","print(\"f1_score : {:.4f}\".format(f1_score(targets_, scores_, average=\"macro\")))        \n","print('classification report')\n","print(classification_report(targets_, scores_))  \n"]},{"cell_type":"markdown","metadata":{"id":"tJjt39sGAfQF"},"source":["Per quanto riguarda la funzione np_quadratic_weighted_kappa abbiamo avuto alcune difficoltà implementative e quindi abbiamo cercato un codice online che ci calcolasse la stessa metrica \n","\n","[Link Utilizzato](https://www.kaggle.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps)"]},{"cell_type":"markdown","metadata":{"id":"MHdYyyAffspk"},"source":["## METRICHE SECONDARIE QWK, MS, MAE \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"in2Vpij3AHgB"},"outputs":[],"source":["\n","# The following 3 functions have been taken from Ben Hamner's github repository\n","# https://github.com/benhamner/Metrics\n","def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n","    \"\"\"\n","    Returns the confusion matrix between rater's ratings\n","    \"\"\"\n","    assert(len(rater_a) == len(rater_b))\n","    if min_rating is None:\n","        min_rating = min(rater_a + rater_b)\n","    if max_rating is None:\n","        max_rating = max(rater_a + rater_b)\n","    num_ratings = int(max_rating - min_rating + 1)\n","    conf_mat = [[0 for i in range(num_ratings)]\n","                for j in range(num_ratings)]\n","    for a, b in zip(rater_a, rater_b):\n","        conf_mat[a - min_rating][b - min_rating] += 1\n","    return conf_mat\n","\n","\n","def histogram(ratings, min_rating=None, max_rating=None):\n","    \"\"\"\n","    Returns the counts of each type of rating that a rater made\n","    \"\"\"\n","    if min_rating is None:\n","        min_rating = min(ratings)\n","    if max_rating is None:\n","        max_rating = max(ratings)\n","    num_ratings = int(max_rating - min_rating + 1)\n","    hist_ratings = [0 for x in range(num_ratings)]\n","    for r in ratings:\n","        hist_ratings[r - min_rating] += 1\n","    return hist_ratings\n","\n","def quadratic_weighted_kappa(y, y_pred):\n","    \"\"\"\n","    Calculates the quadratic weighted kappa\n","    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n","    value, which is a measure of inter-rater agreement between two raters\n","    that provide discrete numeric ratings.  Potential values range from -1\n","    (representing complete disagreement) to 1 (representing complete\n","    agreement).  A kappa value of 0 is expected if all agreement is due to\n","    chance.\n","    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n","    each correspond to a list of integer ratings.  These lists must have the\n","    same length.\n","    The ratings should be integers, and it is assumed that they contain\n","    the complete range of possible ratings.\n","    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n","    is the minimum possible rating, and max_rating is the maximum possible\n","    rating\n","    \"\"\"\n","    rater_a = y\n","    rater_b = y_pred\n","    min_rating=1 # era None abbiamo messo 0\n","    max_rating=9 # era None abbiamo messo 9\n","    rater_a = np.array(rater_a, dtype=int)\n","    rater_b = np.array(rater_b, dtype=int)\n","    assert(len(rater_a) == len(rater_b))\n","    if min_rating is None:\n","        min_rating = min(min(rater_a), min(rater_b))\n","    if max_rating is None:\n","        max_rating = max(max(rater_a), max(rater_b))\n","    conf_mat = Cmatrix(rater_a, rater_b,\n","                                min_rating, max_rating)\n","    num_ratings = len(conf_mat)\n","    num_scored_items = float(len(rater_a))\n","\n","    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n","    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n","\n","    numerator = 0.0\n","    denominator = 0.0\n","\n","    for i in range(num_ratings):\n","        for j in range(num_ratings):\n","            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n","                              / num_scored_items)\n","            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n","            numerator += d * conf_mat[i][j] / num_scored_items\n","            denominator += d * expected_count / num_scored_items\n","\n","    return (1.0 - numerator / denominator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jydD-lsObD4e"},"outputs":[],"source":["path_drive = '/content/drive/My Drive/'\n","path = path_drive+'ProgettoDL/'\n","\n","os.chdir(path)\n","\n","from metrics import np_quadratic_weighted_kappa, minimum_sensitivity\n","from sklearn.metrics import mean_absolute_error\n","\n","#Alternativa al MS di metrics \n","from imblearn.metrics import sensitivity_score\n","\n","def compute_metrics(y_true, y_pred, num_classes):\n","  # Calculate metric\n","  qwk = quadratic_weighted_kappa(y_true, y_pred)\n","  mae = mean_absolute_error(y_true, y_pred)\n","  #ms = minimum_sensitivity(y_true, y_pred) #---> DA RIVEDERE PERCHE' NON C'é PIU' y_pred_no_argmax\n","  \n","  ms = sensitivity_score(y_true, y_pred, average='macro')\n","\n","\n","  metrics = {\n","\t\t'QWK': qwk,\n","\t\t'MS': ms,\n","\t\t'MAE': mae\n","  }\n","  \n","  return metrics\n","\n","def print_metrics(metrics):\n","\tprint('QWK: {:.4f}'.format(metrics['QWK']))\n","\tprint('MS: {:.4f}'.format(metrics['MS']))\n","\tprint('MAE: {:.4f}'.format(metrics['MAE']))    \n","\n","\n","#-----codice------\n","\n","num_classi = 10\n","\n","print('Metrics')\n","metrics = compute_metrics(targets_, scores_,num_classi)\n","print_metrics(metrics)\n","\n","\n","with open(\"metrics.txt\", \"w\") as text_file:\n","    print(print_metrics, file=text_file)\n"]},{"cell_type":"markdown","metadata":{"id":"Tnlbf22bBpIt"},"source":["***Metrice Ottenute***\n","\n","**K Cohen**   https://it.vvikipedla.com/wiki/Cohen%27s_kappa\n","Il Kappa di Cohen è un coefficiente statistico che rappresenta il grado di accuratezza e affidabilità in una classificazione statistica; è un indice di concordanza che tiene conto della probabilità di concordanza casuale; l'indice calcolato in base al rapporto tra l'accordo in eccesso rispetto alla probabilità di concordanza casuale e l'eccesso massimo ottenibile. Attraverso la matrice di confusione è possibile valutare questo parametro. In particolare ... Esistono diversi \"gradi di concordanza\", in base ai quali possiamo definire se Kappa di Cohen è scarso o ottimo:\n","\n","- se k assume valori inferiori a 0, allora non c'è concordanza;\n","- se k assume valori compresi tra 0-0,4, allora la concordanza è scarsa;\n","- se k assume valori compresi tra 0,4-0,6, allora la concordanza è discreta;\n","- se k assume valori compresi tra 0,6-0,8, la concordanza è buona;\n","- se k assume valori compresi tra 0,8-1, la concordanza è ottima.\n","\n","**QWK**: 0.7849\n","\n","BLA BLA BLA \n","\n","**MS**: 1.0000\n","\n","\n","In statistics, **mean absolute error (MAE)** is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. \n","\n","**MAE**: 0.0000"]},{"cell_type":"markdown","metadata":{"id":"z25yiN0zKZ5Y"},"source":["## CONFUSION MATRIX FUNCTION - DA RIMUOVERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LsYoP-3hgfY5"},"outputs":[],"source":["#Confusion Matrix - CROP\n","import sklearn.metrics as metrics\n","\n","def plot_confusion_matrix(cm,\n","                          target_names,\n","                          title='Confusion matrix',\n","                          cmap=None,\n","                          normalize=True):\n","\n","    accuracy = np.trace(cm) / float(np.sum(cm))\n","    misclass = 1 - accuracy\n","\n","    if cmap is None:\n","        cmap = plt.get_cmap('Blues')\n","\n","    fig = plt.figure(figsize=(8, 6))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","\n","    if target_names is not None:\n","        tick_marks = np.arange(len(target_names))\n","        plt.xticks(tick_marks, target_names, rotation=45)\n","        plt.yticks(tick_marks, target_names)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","\n","    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","    for i in range (cm.shape[0]):\n","      for j in range (cm.shape[1]):\n","        if normalize:\n","            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","        else:\n","            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","    axes = plt.gca()\n","    bottom, top = axes.get_ylim()\n","    axes.set_ylim(bottom + 0.5, top - 0.5)\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n","    plt.show()\n","    \n","    return fig\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8VdVD6ChMIOs"},"source":["## PLOT CONFUSION MATRIX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2OZL1txL-5V"},"outputs":[],"source":["import sklearn.metrics as metrics\n","data_ora = datetime.datetime.now()\n","\n","\n","fig, axs = plt.subplots(1)\n","fig0 = ConfusionMatrixDisplay.from_predictions(y_true=targets_, y_pred=scores_, cmap='Blues', ax = axs)\n","plt.suptitle('Confusion Matrix IMG', y=1.0, fontsize=12)\n","plt.title('Accuracy {:.4f} , Prediction {:.4f}'.format(accuracy_score(targets_, scores_),precision_score(targets_, scores_, average=\"macro\") ), fontsize=10)\n","plt.show()\n","fig.savefig(os.path.join(path+'weights/CM_{}_{}.pdf'.format(type_img,data_ora))) \n"]},{"cell_type":"markdown","metadata":{"id":"yy4ylbat-r51"},"source":["##PLOT CONFUSION MATRIX PER CIASCUNA SERIE DEL CALCIO "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6QFmQV0IBLp"},"outputs":[],"source":["path = path_progettoDL\n","data_ora = datetime.datetime.now()\n","\n","test_array_series_complete = [\n","        test_array_s0, test_array_s1, test_array_s2, test_array_s3, test_array_s4,\n","        test_array_s5, test_array_s6, test_array_s7, test_array_s8, test_array_s9,\n","        test_array_s10, test_array_s11, test_array_s12             \n","]\n","\n","pred_array_series_complete = [\n","        pred_array_s0, pred_array_s1, pred_array_s2, pred_array_s3, pred_array_s4,\n","        pred_array_s5, pred_array_s6, pred_array_s7, pred_array_s8, pred_array_s9,\n","        pred_array_s10, pred_array_s11, pred_array_s12             \n","]\n","\n","for series in range(13):\n","  fig, axs = plt.subplots(1)\n","  fig0 = ConfusionMatrixDisplay.from_predictions(y_true=test_array_series_complete[series], y_pred=pred_array_series_complete[series], cmap='Blues', ax = axs)\n","  plt.suptitle('Confusion Matrix Series {}'.format(series), y=1.0, fontsize=12)\n","  plt.title('Accuracy {:.4f} , Prediction {:.4f}'.format(accuracy_score(test_array_series_complete[series], pred_array_series_complete[series]),precision_score(test_array_series_complete[series], pred_array_series_complete[series], average=\"macro\") ), fontsize=10)\n","  plt.show()\n","  fig.savefig(os.path.join(path+'weights/CM_serie{}_{}_{}.pdf'.format(series, type_img,data_ora))) "]},{"cell_type":"markdown","metadata":{"id":"TUnAYcXVclfD"},"source":["## CRAMER V CORRELATION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqvgi3VKck3o"},"outputs":[],"source":["\n","#PRIMA VERSIONE\n","import pandas as pd\n","import numpy as np\n","import scipy.stats as ss\n","import seaborn as sns\n","\n","def cramers_v(confusion_matrix):\n","    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n","        uses correction from Bergsma and Wicher,\n","        Journal of the Korean Statistical Society 42 (2013): 323-328\n","    \"\"\"\n","    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n","    n = confusion_matrix.sum()\n","    phi2 = chi2 / n\n","    r, k = confusion_matrix.shape\n","    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n","    rcorr = r - ((r-1)**2)/(n-1)\n","    kcorr = k - ((k-1)**2)/(n-1)\n","    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n","\n","confusion_matrix = pd.crosstab(scores_, targets_)\n","print(\"cramer correlation tra predizioni delle classi, e le classi effettive\")\n","cramer1 = cramers_v(confusion_matrix.values)\n","print('CRAMER : {:.5f} '.format(cramer1))\n","\n","\n","'''\n","#------ prove denis ----------\n","test_array_series = np.array(test_balance_df['series']) \n","y_test_series = test_array_series #custom_to_categorical(np.unique(test_array_series, return_inverse=True)[1], num_classes=13)  \n","#print(y_test_series)\n","#------ fine prove denis ----------\n","\n","confusion_matrix2 = pd.crosstab(y_test_series, scores_)\n","print(\"cramer correlation tra predizioni delle classi e le ground thruth di shotgun series\")\n","cramer2 = cramers_v(confusion_matrix2.values)\n","print(cramer2)\n","'''\n","print('-------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5sZfKVKTmEdX"},"outputs":[],"source":["print('Seconda versione Cramer')\n","'''\n","#SECONDA VERSIONE.        https://www.youtube.com/watch?v=eTnLTJer_Oo\n","contTable = pd.crosstab(y_test_series, scores_)\n","print(contTable)\n","\n","!pip install researchpy\n","\n","import researchpy\n","\n","crosstab, res = researchpy.crosstab(pd.Series(y_test_series), pd.Series(scores_), test='chi-square')\n","print(\"\\n{}\".format(res))\n","\n","df = min(contTable.shape[0], contTable.shape[1]) - 1\n","print(\"\\ndf = {}\".format(df))\n","\n","V = res.iloc[2,1]\n","print(\"V = {}\".format(V))\n","\n","if df == 1:\n","    if V < 0.10:\n","        qual = 'negligible'\n","    elif V < 0.30:\n","        qual = 'small'\n","    elif V < 0.50:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","elif df == 2:\n","    if V < 0.07:\n","        qual = 'negligible'\n","    elif V < 0.21:\n","        qual = 'small'\n","    elif V < 0.35:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","elif df == 3:\n","    if V < 0.06:\n","        qual = 'negligible'\n","    elif V < 0.17:\n","        qual = 'small'\n","    elif V < 0.29:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","elif df == 4:\n","    if V < 0.05:\n","        qual = 'negligible'\n","    elif V < 0.15:\n","        qual = 'small'\n","    elif V < 0.25:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","else:\n","    if V < 0.05:\n","        qual = 'negligible'\n","    elif V < 0.13:\n","        qual = 'small'\n","    elif V < 0.22:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","\n","print(\"\\nquality classification of the correlation is:   {}\".format(qual))\n","'''\n","print('------')"]},{"cell_type":"markdown","metadata":{"id":"UHab1JLzqkKx"},"source":["To indicate the strength of the association between two nominal variables, Cramér's V (Cramér, 1946) is often used.\n","\n","As for the interpretation for Cramér's V various rules of thumb exist but one of them is from Cohen (1988, pp. 222, 224, 225) who let's the interpretation depend on the degrees of freedom, shown in the table below.\n","\n","|df*|negligible|small|medium|large|\n","|-------|---|---|---|---|\n","|1|0 < .10|.10 < .30|.30 < .50|.50 or more|\n","|2|0 < .07|.07 < .21|.21 < .35|.35 or more|\n","|3|0 < .06|.06 < .17|.17 < .29|.29 or more|\n","|4|0 < .05|.05 < .15|.15 < .25|.25 or more|\n","|5|0 < .05|.05 < .13|.13 < .22|.22 or more|\n","\n","The degrees of freedom (df*) is for Cramér's V the minimum of the number of rows, or number of columns, then minus one.\n","\n","Lets see how to obtain Cramér's V with Python, using an example.\n","\n","\n","\n","\n","**A SECONDA DEI RISULTATI E CONFRONTANDOLI CON LA TABELLA RIUSCIAMO A CAPIRE L'INTENSITA' DEL BIAS TRA DIVERSE VARIABILI**"]},{"cell_type":"markdown","metadata":{"id":"NoegV_027yUO"},"source":["## **T-SNE  & PCA**\n"]},{"cell_type":"markdown","metadata":{"id":"X7-u3Qf0YsFe"},"source":["### Spiegazioni, Link Utili e Implementazione "]},{"cell_type":"markdown","metadata":{"id":"c964uCqMUpS4"},"source":["***(t-SNE)*** t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions suitable for human observation. With help of the t-SNE algorithms, you may have to plot fewer exploratory data analysis plots next time you work with high dimensional data.\n","\n","[Link utile ](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n","\n","***(PCA) Principal Component Analysis***\n","Lʹanalisi delle componenti principali (detta pure PCA oppure CPA) è una tecnica utilizzata nell’ambito della statistica multivariata per la semplificazione dei dati d’origine.\n","Lo scopo primario di questa tecnica è la riduzione di un numero più o meno elevato di variabili (rappresentanti altrettante caratteristiche del fenomeno analizzato) in alcune variabili latenti. Ciò avviene tramite una trasformazione lineare delle variabili che proietta quelle originarie in un nuovo sistema cartesiano nel quale le variabili vengono ordinate in ordine decrescente di varianza: pertanto, la variabile con maggiore varianza viene proiettata sul primo asse, la seconda sul secondo asse e così via. La riduzione della complessità avviene limitandosi ad analizzare le principali (per varianza) tra le nuove variabili.\n","Diversamente da altre trasformazioni (lineari) di variabili praticate nellʹambito della statistica, in questa tecnica sono gli stessi dati che determinano i vettori di trasformazione.\n","[Step By Step](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n","\n","[Link Utile](https://www.analyticsvidhya.com/blog/2020/12/an-end-to-end-comprehensive-guide-for-pca/) "]},{"cell_type":"markdown","metadata":{"id":"1o8fXQZePP25"},"source":["***Parametri del TSNE***\n","1. **n_components** int, default=2 - Dimension of the embedded space.\n","\n","2. **perplexityfloat, default=30.0** - The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results.\n","\n","3. **early_exaggeration float, default=12.0**\n","Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high.\n","\n","4. **learning_ratefloat, default=200.0** The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a ‘ball’ with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.\n","\n","5. **n_iterint, default=1000**\n","Maximum number of iterations for the optimization. Should be at least 250.\n","\n","6. **n_iter_without_progressint, default=300**\n","Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.\n","\n","7. **metricstr or callable, default=’euclidean’**\n","The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is “precomputed”, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is “euclidean” which is interpreted as squared euclidean distance.\n","\n","8. **init{‘random’, ‘pca’} or ndarray of shape(n_samples, n_components), default=’random’**\n","Initialization of embedding. Possible options are ‘random’, ‘pca’, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization.\n","\n","9. **verboseint, default=0** Verbosity level.\n","\n","10. **random_stateint, RandomState instance or None, default=None** Determines the random number generator. Pass an int for reproducible results across multiple function calls. Note that different initializations might result in different local minima of the cost function. See :term: Glossary <random_state>.\n","\n","11. **methodstr, default=’barnes_hut’**\n","By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=’exact’ will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples.\n","\n","12. **n_jobsint, default=None**\n","The number of parallel jobs to run for neighbors search. This parameter has no impact when metric=\"precomputed\" or (metric=\"euclidean\" and method=\"exact\"). None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n","\n","\n","[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n","\n","[misread-tsne](https://distill.pub/2016/misread-tsne/)\n","\n","[altro modo spiegato anche meglio](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n"]},{"cell_type":"markdown","metadata":{"id":"umabolL61nFw"},"source":["#### Dataset "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yp2BSeGZ0vij"},"outputs":[],"source":["'''\n","import numpy as np\n","from keras.models import Sequential\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import os\n","import pandas as pd\n","\n","os.chdir('/content/drive/MyDrive/ProgettoDL')\n","path = os.getcwd()\n","\n","col_list_sx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATOSX\", \"CLASSE_CALCIOSX\"]\n","dataframe_sx_complessivo = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_sx, sep=\";\")\n","\n","\n","col_list_dx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATODX\", \"CLASSE_CALCIODX\"]\n","dataframe_dx_complessivo = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_dx, sep=\";\")\n","\n","\n","dataframe_sx_complessivo.columns = ['ID','series', 'filename', 'class']\n","dataframe_dx_complessivo.columns = ['ID','series', 'filename', 'class']\n","\n","#print(dataframe_sx.columns)                 #stampo i due elementi con stesso ID (lato dx e sx di stesso CALCIO)\n","frames = [dataframe_sx_complessivo, dataframe_dx_complessivo]\n","result_complessivo = pd.concat(frames)\n","#print(result_complessivo)\n","#print(result_complessivo.loc[[1]])\n","#print(type(result_complessivo.loc[[1]]))\n","\n","result_complessivo[\"class\"] = result_complessivo[\"class\"].map({'1': int(0), '2-': int(1), '2': int(2), '2+': int(3), '3-': int(4), '3': int(5), '3+': int(6), '4-': int(7), '4': int(8), '4+': int(9)})\n","result_complessivo[\"series\"] = result_complessivo[\"series\"].map({2: int(0), 4: int(1), 8: int(2), 10: int(3), 6: int(4), 9: int(5), 3: int(6), 11: int(7), 12: int(8), 13: int(9), 14: int(10), 15: int(11), 7: int(12)})\n","\n","#IDENTIFICAZIONE VALORI NULL \n","print(\"Null VALUE di class : \"+format(result_complessivo['class'].isnull().sum()))\n","print(result_complessivo.loc[result_complessivo['class'] == '0'])\n","print(result_complessivo[result_complessivo['class'].isnull()])\n","result_complessivo['class'] = pd.to_numeric(result_complessivo['class'], errors='coerce')\n","print(result_complessivo[result_complessivo['class'].isnull()])\n","result_complessivo = result_complessivo.dropna(subset=['class'])    #rimuovo le righe con elementi nulli\n","print(result_complessivo[result_complessivo['class'].isnull()])\n","\n","print(\"Null VALUE di class : \"+format(result_complessivo['class'].isnull().sum()))\n","\n","#IMMG EXIST ?  (cerco se qualche path non esiste e lo elimino dal dataframe) e se esiste ne faccio la MASCHERA\n","import os.path\n","from os import path\n","os.chdir('/content/drive/MyDrive/CALCIO_NOPRE')\n","for index, row in result_complessivo.iterrows():\n","    filename = row['filename']\n","    if(os.path.exists(filename) == False):\n","      result_complessivo = result_complessivo.drop(result_complessivo[(result_complessivo['filename'] == filename)].index)\n","      print('File : {} eliminato'.format(filename))\n","\n","print('------------------- DATASET BASE ---------------')\n","print(type(result_complessivo))  \n","print(len(result_complessivo))\n","print(result_complessivo)\n","\n","result_complessivo_totale = pd.DataFrame()\n","\n","for index, row in result_complessivo.iterrows():\n","  filename_mask = 'mask_{}'.format(row['filename'])\n","  #filename_gray = 'gray_{}'.format(row['filename'])\n","  class_ = row['class']\n","  series_ = row['series']\n","  #print('{}_{}_{}_{}'.format(filename_gray,filename_mask, class_, series_)) \"ID\": row['ID']\n","  row_df_1 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename_mask, \"class\" : class_},index=[0])\n","  #row_df_2 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename_gray, \"class\" : class_},index=[0])\n","  #row_df_3 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename, \"class\" : class_},index=[0])\n","  #print(row_df_1)\n","  #print(row_df_2)\n","  result_complessivo_totale = result_complessivo_totale.append(row_df_1)\n","  #result_complessivo_totale = result_complessivo_totale.append(row_df_2)\n","  #result_complessivo_totale = result_complessivo_totale.append(row_df_3)\n","\n","\n","print('------------------- DATASET COMPLESSIVO ---------------') \n","print(type(result_complessivo_totale))  \n","print(len(result_complessivo_totale))\n","#print(result_complessivo_totale)\n","\n","from sklearn.utils import shuffle\n","result_complessivo_totale = shuffle(result_complessivo_totale)\n","print(type(result_complessivo_totale))  \n","print(len(result_complessivo_totale))\n","print(result_complessivo_totale)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"_U2UEFjs1iXp"},"source":["#### import utili per il TSNE e PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvdYrmbf1aYd"},"outputs":[],"source":["'''\n","%matplotlib inline\n","from __future__ import print_function\n","import time\n","import numpy as np\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import seaborn as sns\n","from sklearn.manifold import TSNE\n","import pandas as pd    \n","from sklearn.preprocessing import StandardScaler\n","'''"]},{"cell_type":"markdown","metadata":{"id":"z56gEZfX14mX"},"source":["#### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini"]},{"cell_type":"markdown","metadata":{"id":"7qiUkpP0_LXM"},"source":["##### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini - QUALITY CLASS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7t1a9DA0wnE"},"outputs":[],"source":["'''\n","# https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n","from tqdm import tqdm\n","immg_rows = 270 \n","immg_cols = 470\n","X = [] \n","imgs_array_tot = []\n","\n","data_X = result_complessivo_totale['filename'][:1000] #---versione originale \n","result_complessivo_totale_min = result_complessivo_totale[:1000] #--deve essere uguale a y_dim ---versione originale \n","y = result_complessivo_totale['class'][:1000] #--- deve essere uguale ... ---versione originale \n","\n","for index, row in tqdm(result_complessivo_totale_min.iterrows()):\n","    filename = row['filename']\n","    if(filename[0] == 'm'):\n","      image = load_img('/content/drive/My Drive/MASK_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    elif(filename[0] == 'g'): \n","      image = load_img('/content/drive/My Drive/GRAY_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    else:\n","      image = load_img('/content/drive/My Drive/CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    \n","    #print('Originale : {} x {} x {}'.format(image.size[0], image.size[1], len(image.size)-1))\n","    #plt.imshow(image)\n","    scale_percent = 90 # percent of original size\n","    width, height = image.size\n","    #print('channel : {}'.format(len(image.size)))\n","    width = int(width * scale_percent / 100)\n","    height = int(height * scale_percent / 100)\n","    dim = (width, height)\n","    # resize image\n","    x = img_to_array(image)\n","    resized = cv2.resize(x, dim, interpolation = cv2.INTER_AREA)\n","    #print('Ridimensionata : {}'.format((resized.shape)))\n","    #print('Resized Dimensions : ',resized.shape)\n","    imgs_array_tot.append(resized)\n","    X = np.asarray(imgs_array_tot)\n","print(X.shape)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"7IsDgmRB_QxY"},"source":["##### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini - SHOTGUN SERIES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1WGaPWme_Ca8"},"outputs":[],"source":["'''\n","# https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n","from tqdm import tqdm\n","\n","immg_rows = 270 \n","immg_cols = 470\n","X = [] \n","imgs_array_tot = []\n","data_X = result_complessivo_totale['filename'][:1000]\n","\n","result_complessivo_totale_min = result_complessivo_totale[:1000] #--deve essere uguale a y_dim\n","\n","y = result_complessivo_totale['class'][:1000] #--- deve essere uguale ... \n","\n","y_series = result_complessivo_totale['series'][:1000] #--- deve essere uguale ...\n","\n","for index, row in tqdm(result_complessivo_totale_min.iterrows()):\n","    filename = row['filename']\n","    if(filename[0] == 'm'):\n","      image = load_img('/content/drive/My Drive/MASK_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    elif(filename[0] == 'g'): \n","      image = load_img('/content/drive/My Drive/GRAY_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    else:\n","      image = load_img('/content/drive/My Drive/CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","\n","    scale_percent = 90 # percent of original size\n","    width, height = image.size\n","    width = int(width * scale_percent / 100)\n","    height = int(height * scale_percent / 100)\n","    dim = (width, height)\n","    # resize image\n","    x = img_to_array(image)\n","    resized = cv2.resize(x, dim, interpolation = cv2.INTER_AREA)\n","    #print('Ridimensionata : {}'.format((resized.shape)))\n","    #print('Resized Dimensions : ',resized.shape)\n","    imgs_array_tot.append(resized)\n","    X2 = np.asarray(imgs_array_tot)\n","print(X2.shape) \n","'''"]},{"cell_type":"markdown","metadata":{"id":"ihY4u-CL2JKI"},"source":["#### Check & Create Dataframe for PCA (Principal Analysis Component) & T-SNE (t-distributed stochastic neighbor embedding)"]},{"cell_type":"markdown","metadata":{"id":"2OaloopJ5gCK"},"source":["##### classi di qualità "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niWaWydy2H9P"},"outputs":[],"source":["'''\n","print('X SHAPE : {}'.format(X.shape))\n","\n","nsamples = X.shape[0]\n","rows = X.shape[1]\n","cols = X.shape[2]\n","channel = 1\n","\n","print('n_samples : {} , rows : {} , cols : {} , channel : {} '.format(nsamples, rows, cols, channel))\n","print(type(X))\n","X_1 = np.reshape(X, (X.shape[0],rows*cols*channel)) #-- serve per modificare la dimensione, per il fit_transform          FORSE QUI BISOGNA SOLO USARE I PRIMI 2 VALORI E IL 3 DEI CANALI NO!\n","\n","print('X MODIFICATO : {}'.format(X_1.shape)) #--- controllo se ho fatto tutto correttamente \n","\n","feat_cols = [ 'pixel'+str(i) for i in range(X_1.shape[1]) ]\n","print('Feat Cols : {} '.format(len(feat_cols)))\n","#print(feat_cols)\n","df = pd.DataFrame(X_1,columns=feat_cols)\n","#df = pd.DataFrame(X_1)\n","df['y'] = pd.DataFrame({ 'y': np.array(y) })\n","df['label'] = df['y'].apply(lambda i: str(i))\n","#X, y = None, None\n","print('Size of the dataframe: {}'.format(df.shape))\n","\n","# For reproducability of the results\n","np.random.seed(42)\n","rndperm = np.random.permutation(df.shape[0])\n","'''"]},{"cell_type":"markdown","metadata":{"id":"qrtXkNtk5luP"},"source":["##### shotgun series "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nRob1Q0F5buA"},"outputs":[],"source":["'''\n","print('X2 SHAPE : {}'.format(X2.shape))\n","\n","nsamples = X2.shape[0]\n","rows = X2.shape[1]\n","cols = X2.shape[2]\n","channel = 1\n","\n","print('n_samples : {} , rows : {} , cols : {} , channel : {} '.format(nsamples, rows, cols, channel))\n","print(type(X2))\n","X_11 = np.reshape(X2, (X2.shape[0],rows*cols*channel)) #-- serve per modificare la dimensione, per il fit_transform          FORSE QUI BISOGNA SOLO USARE I PRIMI 2 VALORI E IL 3 DEI CANALI NO!\n","\n","print('X MODIFICATO : {}'.format(X_11.shape)) #--- controllo se ho fatto tutto correttamente \n","#print(X_1)\n","\n","feat_cols = [ 'pixel'+str(i) for i in range(X_11.shape[1]) ]\n","print('Feat Cols : {} '.format(len(feat_cols)))\n","#print(feat_cols)\n","df_2 = pd.DataFrame(X_11,columns=feat_cols)\n","#df = pd.DataFrame(X_1)\n","df_2['y'] = pd.DataFrame({ 'y': np.array(y_series) })\n","df_2['label'] = df_2['y'].apply(lambda i: str(i))\n","#X, y = None, None\n","print('Size of the dataframe: {}'.format(df_2.shape))\n","\n","\n","\n","# For reproducability of the results\n","np.random.seed(42)\n","rndperm = np.random.permutation(df_2.shape[0])\n","'''"]},{"cell_type":"markdown","metadata":{"id":"hbq6tUyY2bxr"},"source":["#### Calcolo TSNE & PLOT TSNE"]},{"cell_type":"markdown","metadata":{"id":"PUPCoiGQ6j-D"},"source":["##### TSNE QUALITY CLASS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bg-1zy_g2bDc"},"outputs":[],"source":["'''\n","time_start = time.time()\n","N = 1000 \n","df_subset = df.loc[rndperm[:N],:].copy()\n","data_subset = df_subset[feat_cols].values\n","#data_subset = df_subset\n","#tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=3000, init='random', n_jobs = 10) #-- non so se serve init ... originale \n","tsne = TSNE(n_components=2, verbose=1, perplexity=200, n_iter=6000, init='random', n_jobs = 10) #-- nuova versione \n","tsne_results = tsne.fit_transform(data_subset)\n","print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n","'''"]},{"cell_type":"markdown","metadata":{"id":"J9nnpLbE6sfN"},"source":["##### TSNE SHOTGUN SERIES "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5Ht1Qoc6rtg"},"outputs":[],"source":["'''\n","time_start = time.time()\n","N = 1000\n","df_subset_series = df_2.loc[rndperm[:N],:].copy()\n","#data_subset_series = df_subset_series\n","data_subset_series = df_subset_series[feat_cols].values\n","#tsne_series = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=3000, init='random', n_jobs = 10) #-- non so se serve init ... \n","tsne_series = TSNE(n_components=2, verbose=1, perplexity=5, n_iter=6000, init='random', n_jobs = 10) #-- nuova versione \n","tsne_results_series = tsne_series.fit_transform(data_subset_series)\n","print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n","'''"]},{"cell_type":"markdown","metadata":{"id":"RdicUGi28Sr_"},"source":["##### PLOT TSNE QUALITY CLASSES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXkTZdoA3Thu"},"outputs":[],"source":["'''\n","df_subset['tsne-2d-one'] = tsne_results[:,0]\n","df_subset['tsne-2d-two'] = tsne_results[:,1]\n","plt.figure(figsize=(16,10))\n","sns.scatterplot(\n","    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n","    hue=\"y\",\n","    palette=sns.color_palette('Paired', as_cmap = True),\n","    data=df_subset,\n","    legend=\"full\",\n","    alpha=0.3\n",")\n","'''"]},{"cell_type":"markdown","metadata":{"id":"1WOsA8la8b6O"},"source":["##### TSNE PLOT SHOTGUN SERIES "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOdq6FA08f4b"},"outputs":[],"source":["'''\n","df_subset_series['tsne-2d-one'] = tsne_results_series[:,0]\n","df_subset_series['tsne-2d-two'] = tsne_results_series[:,1]\n","plt.figure(figsize=(16,10))\n","sns.scatterplot(\n","    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n","    hue=\"y\",\n","    palette=sns.color_palette('Paired', as_cmap = True),\n","    data=df_subset_series,\n","    legend=\"full\",\n","    alpha=0.3\n",")\n","'''"]},{"cell_type":"markdown","metadata":{"id":"5aSi_lI_xuvO"},"source":["## **Metriche Nuove** + **B.A. across series**"]},{"cell_type":"markdown","metadata":{"id":"u7H7AQn_u3pW"},"source":["#### Alcune Definizioni \n"]},{"cell_type":"markdown","metadata":{"id":"dxwu_-ZMOLMs"},"source":["*  **True Positives** (TP): Items where the true label is positive and whose class is correctly predicted to be positive.\n","*  **False Positives** (FP): Items where the true label is negative and whose class is incorrectly predicted to be positive\n","*  **True Negatives** (N): Items where the true label is negative and whose class is correctly predicted to be negative.\n","*  **False Negatives** (FN): Items where the true label is positive and whose class is incorrectly predicted to be negative.\n","\n","* **False Positive Rate**, or *Type I Error*: Number of items wrongly identified as positive out of the total actual negatives — FP/(FP+TN) - This error means that an image not containing a particular parasite egg is incorrectly labeled as having it\n","* **False Negative Rate**, or *Type II Error*: Number of items wrongly identified as negative out of the total actual positives — FN/(FN+TP). This metric is especially important to us, as it tells us the frequency with which a particular parasite egg is not classified correctly\n","\n","-------------\n","\n","* **Statistical Parity Difference**\n","This measure is based on the following formula :\n","𝑃𝑟(𝑌=1|𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑)−𝑃𝑟(𝑌=1|𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑) Here the bias or statistical imparity is the difference between the probability that a random individual drawn from unprivileged is labeled 1 (so here that he has more than 50K for income) and the probability that a random individual from privileged is labeled 1. So it has to be close to 0 so it will be fair.\n","\n","*  **Equal Opportunity Difference** This metric is just a difference between the true positive rate of unprivileged group and the true positive rate of privileged group so it follows this formula - 𝑇𝑃𝑅𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑−𝑇𝑃𝑅𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑 Same as the previous metric we need it to be close to 0.\n","\n","* **demographic parity** A fairness metric that is satisfied if the results of a model's classification are not dependent on a given sensitive attribute.\n","\n","* **equality of opportunity** A fairness metric that checks whether, for a preferred label (one that confers an advantage or benefit to a person) and a given attribute, a classifier predicts that preferred label equally well for all values of that attribute. In other words, equality of opportunity measures whether the people who should qualify for an opportunity are equally likely to do so regardless of their group membership."]},{"cell_type":"markdown","metadata":{"id":"XRI4vgyNd-1T"},"source":["#### Implementazione Metriche Nuove e B.A. across series"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7dJJO7ixuJ7"},"outputs":[],"source":["!pip install fairlearn \n","from fairlearn.metrics import selection_rate\n","from fairlearn.metrics import true_positive_rate, false_positive_rate, true_negative_rate, false_negative_rate\n","from fairlearn.metrics import equalized_odds_difference\n","\n","import sklearn as sk\n","\n","\n","#---- metriche lisa ----#\n","#_true = test_balance_df['class'].to_numpy()\n","#VERIFICA SE SERVE RIFARE STA RIGA SOPRA, MA BASTA PRENDERE:\n","y_true = targets_\n","y_pred = scores_\n","\n","\n","SR = selection_rate(y_true, y_pred, pos_label=1, sample_weight=None)\n","print('selection_rate : {}' . format(SR))\n","\n","\n","#Per quanto riguarda AO come metrica, potremo utilizzare i risultati della confusion matrix ?\n","#LINK : https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n","#LINK : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix\n","#print('Unique Element Y_test : {}'.format(np.unique(y_test)))\n","#print('Unique Element Y_pred : {}'.format(np.unique(y_pred)))\n","#print('True_Positive_Rate : {}'.format(true_positive_rate(y_true, y_pred)))\n","\n","from sklearn.metrics import confusion_matrix \n","cm = confusion_matrix (y_true, y_pred)\n","FP = cm.sum(axis=0) - np.diag(cm)  \n","FN = cm.sum(axis=1) - np.diag(cm)\n","TP = np.diag(cm)\n","TN = cm.sum() - (FP + FN + TP)\n","\n","# Sensitivity, hit rate, recall, or true positive rate\n","TPR = TP/(TP+FN)\n","print('TPR : {}'.format(TPR))\n","# Specificity or true negative rate\n","TNR = TN/(TN+FP) \n","print('TNR : {}'.format(TNR))\n","# Precision or positive predictive value\n","PPV = TP/(TP+FP)\n","print('PPV : {}'.format(PPV))\n","# Negative predictive value\n","NPV = TN/(TN+FN)\n","print('NPV : {}'.format(NPV))\n","# Fall out or false positive rate\n","FPR = FP/(FP+TN)\n","print('FPR : {}'.format(FPR))\n","# False negative rate\n","FNR = FN/(TP+FN)\n","print('FNR : {}'.format(FNR))\n","# False discovery rate\n","FDR = FP/(TP+FP)\n","print('FDR : {}'.format(FDR))\n","\n","# Overall accuracy\n","ACC = (TP+TN)/(TP+FP+FN+TN)\n","print('Accuracy : {}'.format(ACC))\n","\n","\n","AO = 0.5*(\n","    (TPR[0] + FPR[0]) - \n","    (TPR[1] + FPR[1]) + \n","    (TPR[2] + FPR[2]) - \n","    (TPR[3] + FPR[3]) +\n","    (TPR[4] + FPR[4]) -\n","    (TPR[5] + FPR[5]) +\n","    (TPR[6] + FPR[6]) -\n","    (TPR[7] + FPR[7]) +\n","    (TPR[8] + FPR[8]) -\n","    (TPR[9] + FPR[9]))\n","\n","print('AO : {}'.format(AO))\n","#y_true= y_true.reshape(1,-1)\n","#y_pred= y_pred.reshape(-1,1)\n","#print(y_true.shape)\n","#print(y_pred.shape)\n","\n","\n","'''FORSE QUA RIUSCIAMO A TROVARE UN ESEMPIO DI APPLICAZIONE DEL METODO'''\n","'''https://deepnote.com/@Machine-Learning-2/Miniproject-z523fGqWSSu7QV34n_u7OA'''\n","'''https://fairlearn.org/main/user_guide/assessment.html'''\n","\n","\n","EO =(TPR[0] - TPR[1] + TPR[2] - TPR[3] + TPR[4] - TPR[5] + TPR[6] - TPR[7] + TPR[8] - FPR[9]) \n","print('EO : {}' . format(EO))\n","\n","\n","#Demographic parity\n","'''\n","Demographic parity is one of the most popular fairness indicators in the literature. \n","Demographic parity is achieved if the absolute number of positive predictions \n","in the subgroups are close to each other. This measure does not take true class into\n","consideration and only depends on the model predictions. In some literature, \n","demographic parity is also referred to as statsictal parity or independence.\n","'''\n","DP = (TP + FP)\n","print('Demographic parity : {}' . format(DP))\n","\n","#Equalized odds\n","'''\n","Equalized odds, also known as separation, are achieved if the sensitivities in the \n","subgroups are close to each other. The group-specific sensitivities \n","indicate the number of the true positives divided by the total \n","number of positives in that group.\n","'''\n","Equalized_Odds = TP / (TP + FN)\n","print('Equalized Odds : {}' . format(Equalized_Odds))\n","\n","\n","##---- Link Riccardo ----##\n","#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n","\n","print('----------------')\n","Balanced_Accuracy = sk.metrics.balanced_accuracy_score(y_true, y_pred, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Generale : {:.5f}' . format(Balanced_Accuracy))\n","\n","\n","#####----------- PER CIASCUNA SERIE BALANCED ACCURACY -----------####\n","test_array_series_complete = [\n","        test_array_s0, test_array_s1, test_array_s2, test_array_s3, test_array_s4,\n","        test_array_s5, test_array_s6, test_array_s7, test_array_s8, test_array_s9,\n","        test_array_s10, test_array_s11, test_array_s12             \n","]\n","\n","pred_array_series_complete = [\n","        pred_array_s0, pred_array_s1, pred_array_s2, pred_array_s3, pred_array_s4,\n","        pred_array_s5, pred_array_s6, pred_array_s7, pred_array_s8, pred_array_s9,\n","        pred_array_s10, pred_array_s11, pred_array_s12             \n","]\n","sum_BA = 0\n","print('----------------')\n","for series in range(13):\n","  BA = sk.metrics.balanced_accuracy_score(test_array_series_complete[series],pred_array_series_complete[series], sample_weight=None, adjusted=False)\n","  print('Balanced Accuracy Series {} : {:.5f}' . format(series,BA ))\n","  print('----------------')\n","  sum_BA = sum_BA + BA\n","\n","\n","#----------- MEDIA DELLE BALANCED ACCURACY ---------------\n","Average = sum_BA/13\n","print('Average Balanced Accuracy : {:.5f}' . format(Average))\n"," \n","\n","##---- Wodsworth et Al ----# \n","#HIGH_RISK_GAP = SP #modulo o cardinalità \n","\n","#FN_GAP = false_negative (s1) - false negative (s2) \n","#FN_GAP = (false_negative_rate(y_true, y_pred) - false_negative_rate(y_true, y_pred))  #modulo o cardinalità\n","  \n","#FN_GAP = false_negative (s1) - false negative (s2) \n","#FP_GAP = (false_positive_rate(y_true, y_pred) - false_positive_rate(y_true, y_pred))  #modulo o cardinalità\n","\n","\n","\n","### LINK UTILE ####\n","#https://www.kaggle.com/nathanlauga/ethics-and-ai-how-to-prevent-bias-on-ml"]},{"cell_type":"markdown","metadata":{"id":"palNIXpFq6SV"},"source":["##PROVA MDSS (ATTUALMENTE NON FUNZIONA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9nh6reyAxcm3"},"outputs":[],"source":["'''imports'''\n","'''Importing packages & libraries for Bias metrics from AIF360 (end page analysis)'''\n","\n","\"\"\"\n","import sys\n","import itertools\n","import datetime\n","\n","### altri package necessari \n","!pip install tempeh -q --force-reinstall\n","!pip install fairlearn -q --force-reinstall\n","!pip install GitPython -q --force-reinstall\n","\n","#download aif360 \n","#!pip install -r requirement.txt \n","#!python setup.py\n","###test  errore nella foto \n","\n","#!pip uninstall scikit-learn -q -y \n","# errore cercato in rete : https://github.com/pycaret/pycaret/issues/704\n","# INSTALL CONDA ON GOOGLE COLAB\n","#! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n","#! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh\n","#! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local\n","#sys.path.append('/usr/local/lib/python3.7/site-packages/')\n","# INSTALL PACKAGE CON CONDA \n","#!conda install -c conda-forge scikit-learn -y \n","\n","## package necessario utile per il gitclone\n","from git import Repo\n","\n","date_ = datetime.datetime.now()\n","#Repo.clone_from(\"link ... .git\", \"path di salvataggio\")\n","#Repo.clone_from(\"https://github.com/Trusted-AI/AIF360.git\", \"/content/aif360_repo_{}\".format(date_))\n","Repo.clone_from(\"https://github.com/Trusted-AI/AIF360.git\", \"/content/aif360\") #controllare sempre, se è già salvato ... se salvato basta commentarlo\n","\n","#---serve per aggiungere un nuovo package alla lista dei package installati\n","sys.path.append('/content/aif360/')\n","\n","\n","from aif360.metrics.mdss.ScoringFunctions import Bernoulli, ScoringFunction \n","from aif360.metrics.mdss import MDSS\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import OrdinalEncoder\n","\n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score,classification_report, f1_score, precision_score, recall_score, confusion_matrix\n","from sklearn.metrics import confusion_matrix\n","\n","from IPython.display import Markdown, display\n","import numpy as np\n","import pandas as pd\n","\n","from collections import defaultdict\n","from aif360.datasets import BinaryLabelDataset\n","from aif360.metrics import ClassificationMetric\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J462wngdkoit"},"outputs":[],"source":["\"\"\"from aif360.datasets import StandardDataset\n","dataset = StandardDataset(df, label_name='two_year_recid', favorable_classes=[0],\n","                 protected_attribute_names=['sex', 'race'],\n","                 privileged_classes=[[1], [1]],\n","                 instance_weights_name=None)\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MMYiWDkIRbO"},"outputs":[],"source":["\"\"\"\n","# !python3 -m pip install <pkg> -q il -q è per il quiet (per non avere quel macello in output) --force-reinstall\n","\n","#https://github.com/Trusted-AI/AIF360\n","'''REPOSITORY PRINCIPALE DOVE SI TROVANO TUTTE LE RESTANTI CARTELLE E IMPLEMENTAZIONI'''\n","#https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_mdss_classifier_metric.ipynb\n","'''file dove il tipo fa un esempio di uso di questa metrica!'''\n","#https://aif360.readthedocs.io/en/latest/index.html\n","'''link di aif360 dove ci sono i metodi CHE IMPORTA SOTTO'''\n","\n","\n","class MDSSClassificationMetric(ClassificationMetric):\n","    '''\n","        Bias subset scanning is proposed as a technique to identify bias in predictive models using subset scanning [1].\n","        This class is a wrapper for the bias scan scoring and scanning methods that uses the ClassificationMetric abstraction.\n","    References:\n","        .. [1] Zhang, Z., & Neill, D. B. (2016). Identifying significant predictive bias in classifiers. arXiv preprint arXiv:1611.08292.\n","    '''\n","    def __init__(self, dataset: BinaryLabelDataset, classified_dataset: BinaryLabelDataset, \n","                scoring_function: ScoringFunction = Bernoulli(direction='positive'), unprivileged_groups: dict = None, privileged_groups:dict = None):\n","    \n","        super(MDSSClassificationMetric, self).__init__(dataset, classified_dataset,\n","                                                       unprivileged_groups=unprivileged_groups,\n","                                                       privileged_groups=privileged_groups)\n","        \n","        self.scanner = MDSS(scoring_function)\n","    \n","    def score_groups(self, privileged=True, penalty = 1e-17):\n","        '''\n","        compute the bias score for a prespecified group of records.\n","        \n","        :param privileged: flag for group to score - privileged group (True) or unprivileged group (False).\n","        This abstract the need to explicitly specify the direction of bias to scan for which depends on what the favourable label is.\n","        :param penalty: penalty term. Should be positive. The penalty term as with any regularization parameter may need to be \n","        tuned for ones use case. The higher the penalty, the less complex (number of features and feature values) the highest scoring\n","        subset that gets returned is.\n","        \n","        :returns: the score for the group\n","        '''\n","        groups = self.privileged_groups if privileged else self.unprivileged_groups\n","        subset = dict()\n","        \n","        xor_op = privileged ^ bool(self.classified_dataset.favorable_label)\n","        direction = 'positive' if xor_op else 'negative'\n","\n","        for g in groups:\n","            for k, v in g.items():\n","                if k in subset.keys():\n","                    subset[k].append(v)\n","                else:\n","                    subset[k] = [v]\n","        \n","        coordinates = pd.DataFrame(self.dataset.features, columns=self.dataset.feature_names)\n","        expected = pd.Series(self.classified_dataset.scores.flatten())\n","        outcomes = pd.Series(self.dataset.labels.flatten())\n","        \n","        self.scanner.scoring_function.kwargs['direction'] = direction\n","        return self.scanner.score_current_subset(coordinates, expected, outcomes, dict(subset), penalty)\n","    \n","    def bias_scan(self, privileged=True, num_iters = 10, penalty = 1e-17):\n","        '''\n","        scan to find the highest scoring subset of records\n","        \n","        :param privileged: flag for group to scan for - privileged group (True) or unprivileged group (False). \n","        This abstract the need to explicitly specify the direction of bias to scan for which depends on what the favourable label is.\n","        :param num_iters: number of iterations (random restarts)\n","        :param penalty: penalty term. Should be positive. The penalty term as with any regularization parameter may need to be \n","        tuned for ones use case. The higher the penalty, the less complex (number of features and feature values) the highest scoring\n","        subset that gets returned is.\n","        \n","        :returns: the highest scoring subset and the score\n","        '''\n","\n","        xor_op = privileged ^ bool(self.classified_dataset.favorable_label)\n","        direction = 'positive' if xor_op else 'negative'\n","        self.scanner.scoring_function.kwargs['direction'] = direction\n","\n","        coordinates = pd.DataFrame(self.classified_dataset.features, columns=self.classified_dataset.feature_names)\n","        \n","        expected = pd.Series(self.classified_dataset.scores.flatten())\n","        outcomes = pd.Series(self.dataset.labels.flatten())\n","        \n","        return self.scanner.scan(coordinates, expected, outcomes, penalty, num_iters)\n","\n","from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_compas\n","\n","#import requests\n","#url = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'\n","#r = requests.get(url, allow_redirects=True)\n","#open('dataset.csv', 'wb').write(r.content)\n","#dataset_orig = pd.read_csv('/content/dataset.csv', sep=\",\")  \n","\n","\n","series_group = [{'series': 0, 'series': 1, 'series': 2, 'series': 3, 'series': 4, 'series': 5, 'series': 6, 'series': 7, 'series': 8, 'series': 9, 'series': 10, 'series': 11, 'series': 12}]\n","classes_group = [{'class': 0, 'class': 1, 'class': 2, 'class': 3, 'class': 4, 'class': 5, 'class': 6, 'class': 7, 'class': 8, 'class': 9}]\n","\n","\"\"\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xiBNnjm2v-Y7"},"outputs":[],"source":["\"\"\"\n","\n","mdss_classified = MDSSClassificationMetric(dataset_orig_test, dataset_bias_test,\n","                         unprivileged_groups=male_group,\n","                         privileged_groups=female_group)\n","\n","\n","mdss_classified = MDSSClassificationMetric(test_balance_df, dataset_bias_test,\n","                         unprivileged_groups=classes_group,\n","                         privileged_groups=series_group)\n","\n","series_privileged_score = mdss_classified.score_groups(privileged=True)\n","print(series_privileged_score)\n","\"\"\""]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":["ox6IFmrBfpP-","50n4oVKemd9n","VgAYZM1NDIRj","zUvw3mwvfCMA","xuS19wSXbdE8","28i0TbSpzt_Z","Ti_N5HuUFggs","o3iWx_QMIVQp","VX2SB5uYzzyB","26Za2K_yFljo","DBHMPMTPX5V5","lGEjbyRmMQNm","9rWuwr7bXGLI","oow7OM0obIF4","pMvexlvYe5xD","MG_zlz7PfzZC","zghXE8FCfueE","hUVliMM4zT4F","JCHj6IT9fnoi","H2p8tTB2fv7b","FZz9rT_PYs3V","EAZMofCHONfK","bfJ3LnbAZhQx","QB6Ks6vqMLmS","GeSLye5CMa4Y","lY-1rkp2bUi6","hA59LYtlDgUr","XnUgaUmoFykZ","ig3uCmF7L-Zt","u6nY8B7VcfYj","A_U_C71VdOAO","ohUoctJUcmoa","RRGftpaXbdNE","-GViGlheXfJt","shGoDK-cd70n","k-82Jf4Ndqdd","NdslbUPRlioe","-zMdWtZif_ui","Hosx74qRSs31","xildnrRfoxil","tbPUx79ZozKx","eEMosgpdo2Hj","d45MkW5no5oD","lnkHiR9qSc9t","VkaCQp4Ndzbe","z25yiN0zKZ5Y","yy4ylbat-r51","NoegV_027yUO","palNIXpFq6SV"],"machine_shape":"hm","name":"Copia di Pytorch - architettura base_&_TEST_DENIS.ipynb","provenance":[{"file_id":"1e4Y9JykKQU4LTHcLHy5sAzpwI-SPmTl6","timestamp":1623766889995},{"file_id":"1d0RkIbwRSJuanJjnXiLFbQSY2wZpHWXW","timestamp":1623496078537},{"file_id":"1eCj3WI6GCKlwC6VldMXH9RiHM1W8W5kj","timestamp":1623321103095},{"file_id":"1tcILVHrFk_-CCLnWgai88ENO5EJx1w_0","timestamp":1623062215958},{"file_id":"1GtF5i2sJMGQwC1uEVhcJJwKD7Zm_MjqW","timestamp":1622106936931},{"file_id":"1QblOtm62z9o1JIL5BVVghxIfnaxJ-Xtw","timestamp":1622099766973},{"file_id":"1gcmGZCqZ_dyjzG5wG6b5xgRBEGH0BKdK","timestamp":1621966104442},{"file_id":"10dyVnpAJogKpKJo-l5KYpqNYLxR8v0v8","timestamp":1621951584071},{"file_id":"1i_LnVOfxnPfx3KpC89VtMAf6M6eKIip1","timestamp":1621802010244},{"file_id":"1E4TbDqseXZ6wxtPDbKKuKUpB7F076PP4","timestamp":1621784336493}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}